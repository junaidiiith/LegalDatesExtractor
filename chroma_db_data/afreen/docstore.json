{"docstore/metadata": {"e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8": {"doc_hash": "22cf53ef4e6d3e04c8cc198e8d6faee490e5860d64080a2daf33cdc69d3696e5"}, "9434786b-aa69-40b3-b9cc-3ecf847eb7a1": {"doc_hash": "1dff8fe0ae0cc0e2572b7ffeda2c2e1b4d320a1c196f6c6c0aad29c68a7a9615"}, "b038a19c-c71f-4121-bd5a-82e96b947813": {"doc_hash": "174e05d397138babb9acc5327d18692ed92b972ad946563d09fcbab1d1ebda98"}, "a885c675-2205-4e32-af60-be54d05f8a5c": {"doc_hash": "0171ff74fb4dc58b586a7b264c4187648fa1c366a1a65cdf538056c6a75bc3d9"}, "99b6828c-1bde-47f2-8d06-18aa053bc100": {"doc_hash": "b83badc1b39ddc55aaa2a40efea5af6ef91478a7d649a4ebdabf5d9dfe68133d"}, "2d8bdb5b-9c77-46f5-af13-5aa0e0147749": {"doc_hash": "7ffd10070c7fb159b5feed98a9cefa13b023314fd9e24efb135599e4f9d41ca7"}, "6a3fabe9-d151-4818-9fc2-cd3511fbf510": {"doc_hash": "1f976bf51c039fb41eaa8ccc16adeb8ab9a439bf03b50b0e096f105f376cd900"}, "75f91c95-e67a-4a4f-836f-fe6f8a4e8f61": {"doc_hash": "8334c081061f122a58feab1bd82a81a210d358bf9fd63059061958bf53ad5a6e"}, "e5b5d091-a362-4b06-9ab4-a6257983cb23": {"doc_hash": "9b0f3e50f2970049434ba82c72ca2ff06d635661a04042112d6711676cbaf4bf"}, "3fa68b9a-1700-4b2f-9ff7-f97b0b162055": {"doc_hash": "8dca31c19dd3c8d7a5c13381cb9afbab7de23b828f684bf4fae46b0e28ba42b6"}, "b571e5f2-5732-41c9-ab76-1583a8b95ea5": {"doc_hash": "cb020070bc4c0e7a47c8294fe5217a0f7ca06fb67022bf2f1c99278f252e6167"}, "773594f0-5d4a-49ab-9516-d152f7991587": {"doc_hash": "a8b463b4a6524da51231a2f3fe1208b92efc60ddded73fd9ea9f27d1c715ebf6"}, "37589ad1-7249-4c95-919f-c59b67b41ace": {"doc_hash": "0012c781a1e0ca3bd3b19552bc3a6d6bca56431b3bdc9eb8037a669082f0493d"}, "532ba3a2-4f81-4db0-98b1-d600f70164e1": {"doc_hash": "1c2f4308a905c93454415114531d856c2d0928aa7b93c63c7c846f7ee3d0438d"}, "509156ca-8903-422c-8d55-04d31f7a28c3": {"doc_hash": "9b3c2665492b5def7b084b4d03a92c673434b0768081fd6f8e57b5e220fc4ffe", "ref_doc_id": "e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8"}, "39e8e174-2c07-43d1-840e-046161b01ea9": {"doc_hash": "76317590e84d96ecebb69eb1272dec20942a96920493efdcffecc25ff977a147", "ref_doc_id": "e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8"}, "2e3d8929-0c52-4cdc-9a13-6657d08243b2": {"doc_hash": "1ad998aa92dff8ac0928ef899f77c7d19dda354f2a5936ad8d0d26b77273ce46", "ref_doc_id": "e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8"}, "670cb8a2-96b6-44f3-a01b-2be22d814996": {"doc_hash": "a814cc110e1d7d39b1952b2ce0f1c2fbc933dd2be19728fcc464f280c69558d4", "ref_doc_id": "e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8"}, "2c90203a-1040-443c-b8eb-dcee414e230d": {"doc_hash": "c3fbff81d5ec002c088fa8d5c7cf94a85ec4bed97e4e6213830f78f0bb63a5b7", "ref_doc_id": "e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8"}, "9f04337a-d2b9-4eb9-b530-25e4c7aecdbd": {"doc_hash": "91fab7d3c987a8b1694e2022e36b5fcedd8694e6e8fe2ada14d68d5d0b95caa3", "ref_doc_id": "9434786b-aa69-40b3-b9cc-3ecf847eb7a1"}, "f63766c7-afd7-45b9-a0b4-554f2cb3ec2b": {"doc_hash": "0dffb4c85c1051c2aecd5b0088df3c702764bb24a6e820f2e8aafa47efa9cc7b", "ref_doc_id": "9434786b-aa69-40b3-b9cc-3ecf847eb7a1"}, "8407880d-4a1d-49b4-b361-f96b2da6c972": {"doc_hash": "34a48ee320dd284a49e7e42c7533d32d803b6c7773af19eeff1372c34d3c236d", "ref_doc_id": "9434786b-aa69-40b3-b9cc-3ecf847eb7a1"}, "6d2c42d6-8d07-4514-8e33-6741baffeeba": {"doc_hash": "ffdff9c08ed1fecac21878b793434720770363f11426194ed35c0c8a4c59c2ef", "ref_doc_id": "9434786b-aa69-40b3-b9cc-3ecf847eb7a1"}, "5a6d0d3a-bc83-4e90-bf9b-4dc8b6ec4a5d": {"doc_hash": "f838aa453879dd4a4e548da5a5f307c5695f6f65c8368b375389df3d6baa9990", "ref_doc_id": "9434786b-aa69-40b3-b9cc-3ecf847eb7a1"}, "69dc2a78-d7e0-4d04-af6f-70a28d6fca29": {"doc_hash": "d8ab9a6198b1feb9ca02ba2dbdef3309326bc3ad6d5c6f73ccc524f38a3f2a49", "ref_doc_id": "b038a19c-c71f-4121-bd5a-82e96b947813"}, "c7e4035d-e71d-444b-bacf-08a674fad8fd": {"doc_hash": "7ab75c6c0d7f7f4d5c016b7b11d7bc079f16db32efb53671103d4e9d351f4068", "ref_doc_id": "b038a19c-c71f-4121-bd5a-82e96b947813"}, "bb5b8920-a3bf-4919-a56b-50da41b12714": {"doc_hash": "8444063ebb461611323fd4efd5707e2fb807a711dd818d744cf1c3abaf6c2cc0", "ref_doc_id": "b038a19c-c71f-4121-bd5a-82e96b947813"}, "bb8ad3af-5a02-45d2-b4a8-949d3c9842a6": {"doc_hash": "6431f29c167f4dc00068615fdd4bb5a6fd23065f86b44c9686b3acc5285bba45", "ref_doc_id": "b038a19c-c71f-4121-bd5a-82e96b947813"}, "25103751-f41d-4b4f-8652-8b0c942c054c": {"doc_hash": "f17a6dcd3fd52f8819ad7d16855324c599295c6020b9a86bd2c1f2a73ae17887", "ref_doc_id": "b038a19c-c71f-4121-bd5a-82e96b947813"}, "921588a1-14f3-4360-97d8-c0e69e040e90": {"doc_hash": "f36749948accac7d9556364d598a1812239628c5c4d6647e55401d3375938107", "ref_doc_id": "a885c675-2205-4e32-af60-be54d05f8a5c"}, "93fd2242-059f-4a53-9399-7bbc25608b6c": {"doc_hash": "aef303af2b6e950de07adec1651580a036855c68d058fe50a352db2eb2f87ef6", "ref_doc_id": "a885c675-2205-4e32-af60-be54d05f8a5c"}, "dcea69f3-ff89-4127-a2e8-f21e4e12b472": {"doc_hash": "b9ec4f7cacc4ab4dee6c6c0b5429e0b88dcc47f0ce62ac3e94c3dbf3ee52b6e6", "ref_doc_id": "a885c675-2205-4e32-af60-be54d05f8a5c"}, "be25a61e-f0ae-4f95-bfa8-a6e35c77247d": {"doc_hash": "007eca2326b0071de076d75145b0e1ff9064b217d9799a9491cd0c5423a22fac", "ref_doc_id": "a885c675-2205-4e32-af60-be54d05f8a5c"}, "253fdafe-73fa-44f7-b65b-68ed46fce9ab": {"doc_hash": "d009c881a8ac3d24cea975d3996c3eec9d17ff67633229f25e3288cda7b893ef", "ref_doc_id": "a885c675-2205-4e32-af60-be54d05f8a5c"}, "0e03c852-bbb7-4f8c-a6b6-f982e30aefa3": {"doc_hash": "ccfa5c4e0e55bd5b993f1c621e216ff2003a70c64ce8541c8bbc0262efb2ba22", "ref_doc_id": "99b6828c-1bde-47f2-8d06-18aa053bc100"}, "a18f3843-a381-48ce-8c5a-1fb2d3577082": {"doc_hash": "c8cda78ebb581e7d4b618c62e4135791b2b6e75125347495b8b82e392477fcdd", "ref_doc_id": "99b6828c-1bde-47f2-8d06-18aa053bc100"}, "11958e16-6d14-4071-a88b-8ce34e958501": {"doc_hash": "7812005deda0f1a3ae9e2b101928cc5bd5639a16b15c0fb211afe6c233913912", "ref_doc_id": "99b6828c-1bde-47f2-8d06-18aa053bc100"}, "680b3d2c-9116-44c0-9ae9-d61c58297611": {"doc_hash": "4e2276e949c0819d827d4faf0622b349ee8fde43ac2483c14132df0310c52888", "ref_doc_id": "99b6828c-1bde-47f2-8d06-18aa053bc100"}, "35df42b8-120a-4ff1-a1af-a8d31cb51229": {"doc_hash": "843a60aa7123b66f6182ce008435f6e23830a8a3e811121ec12dd899d9b0e008", "ref_doc_id": "2d8bdb5b-9c77-46f5-af13-5aa0e0147749"}, "33538484-e919-4af5-927e-ff392b3cbc46": {"doc_hash": "de67167b67623324950f5b197bb1130c13a5b0c6a239718a9d5b8b8fed09b336", "ref_doc_id": "2d8bdb5b-9c77-46f5-af13-5aa0e0147749"}, "e7a78683-ba39-495a-ad59-c6954f9200f1": {"doc_hash": "46b432681fc415ae50e7758026495b177d0d186db8ef1707e357870235604f4a", "ref_doc_id": "2d8bdb5b-9c77-46f5-af13-5aa0e0147749"}, "035ffeab-be1d-4d46-8d91-e280da11eec4": {"doc_hash": "d4724643e83f2512afe1546d2bb64c7456042282162e711d9e841437d5c71288", "ref_doc_id": "2d8bdb5b-9c77-46f5-af13-5aa0e0147749"}, "34e2e113-2421-4f1c-ae2a-cf5cd5773d08": {"doc_hash": "b95199c196b028de36e8984013f1fb7098127c2dc5bb6e7620e582bf5bf5e9d2", "ref_doc_id": "2d8bdb5b-9c77-46f5-af13-5aa0e0147749"}, "9b488bbe-e17e-4cb6-b6ad-fa33b2c20916": {"doc_hash": "e20f196516f3ea3287b6110e4ecb7b980c0b7a18c9e0585d42ae0a53cf63b249", "ref_doc_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510"}, "c8952b2b-4672-4cdb-931d-ab5c4bce6da3": {"doc_hash": "01bc81cd6a528e788a60236088829cb9ac0c6f5c28c6d77b08648341cd0ec8be", "ref_doc_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510"}, "599dddd6-22be-497e-b7e4-dc5cccb4ab9a": {"doc_hash": "14175f56078f348ff289524dffb4569dad264fb723abd171c804e800530f7460", "ref_doc_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510"}, "7b3bbd18-3411-4a78-b092-bd5ec3a6b762": {"doc_hash": "7199c0716960b71681428ca2702de1c8b80343c502ac3d10616d5c39d431bc3c", "ref_doc_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510"}, "002f2372-cf27-49ca-8e77-3a02d34c7da0": {"doc_hash": "8a90ed123879155be9c4935fa01756f28533015f37fc08b4e7cea3f71b1ad9d3", "ref_doc_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510"}, "e5ef7850-936f-4cd0-af9a-b59bc03aeb4e": {"doc_hash": "6080c6eb8bc9601f430c23e601adfe7e1c2528ce97b20212ed2f4210a6ae6251", "ref_doc_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510"}, "83be867f-1739-4404-949a-62f15c183419": {"doc_hash": "a74859b60d9bca3321e6426f6e95a1892a6118e58d23c116b217dd44c27257a6", "ref_doc_id": "75f91c95-e67a-4a4f-836f-fe6f8a4e8f61"}, "8aff54e3-1eb4-438f-be7a-f35813354d28": {"doc_hash": "d28aafbd02fa4efc955dca17f4d0cbbff56810a4bf4800fa924bb38857bc4e5e", "ref_doc_id": "75f91c95-e67a-4a4f-836f-fe6f8a4e8f61"}, "e692e3aa-bbc4-4c68-9f46-7e77c8320470": {"doc_hash": "a67159ea104f3ff6678d58be2bc56014984ab61cefabbf3bc1827139180b0017", "ref_doc_id": "75f91c95-e67a-4a4f-836f-fe6f8a4e8f61"}, "8524edff-44a4-470f-be8a-0dab06c57d5d": {"doc_hash": "70b0a8f56da70981b7790613ebdc6f6a0219442ef436605635ed76173713cfbb", "ref_doc_id": "75f91c95-e67a-4a4f-836f-fe6f8a4e8f61"}, "b0629401-432c-44a1-8bdb-51a55cef775e": {"doc_hash": "ba19999c2ab237102e7c63ebe9f70632e0f9aa3cd21dfbd7ba0bf7c6ffee28d8", "ref_doc_id": "e5b5d091-a362-4b06-9ab4-a6257983cb23"}, "728752c2-7f9b-41e7-87a2-55523d10065c": {"doc_hash": "1499d3b172c77eb31d1673d384a61ae1e713e42f8b4f9dc001472350cefb1243", "ref_doc_id": "e5b5d091-a362-4b06-9ab4-a6257983cb23"}, "aafde6a4-5e10-47a0-a722-16f71fcef598": {"doc_hash": "35eed1007a8a1208f2083b7ef191d306c8dfe8d593823bbf5c11b4149e56423b", "ref_doc_id": "e5b5d091-a362-4b06-9ab4-a6257983cb23"}, "36c66177-7ef8-48ac-a4c6-8bd2a1699c3f": {"doc_hash": "0a0d45597fafe998bfb6629082da1e6f713ddb9153040de4ac8d52bce2073ffc", "ref_doc_id": "e5b5d091-a362-4b06-9ab4-a6257983cb23"}, "8f37fb67-79e6-45dc-b4b1-fd29556f2d38": {"doc_hash": "8673f0a8cf416d57e572a63c178fc957005887a5f0a69c9229bfc27c57993c2c", "ref_doc_id": "e5b5d091-a362-4b06-9ab4-a6257983cb23"}, "c71ddbaa-324e-447f-809e-b283a23f2f81": {"doc_hash": "3e9573fa0386ad96ed7f81d642c62086ff75114f81a2a46905f00e438be7c432", "ref_doc_id": "3fa68b9a-1700-4b2f-9ff7-f97b0b162055"}, "ecf7a71b-9b4b-42ee-a83b-043585fedfdd": {"doc_hash": "4b81c30d5524524de4cd27a92f1a5518503ce2e17f4d8ac08ced4f0c77cb6d48", "ref_doc_id": "3fa68b9a-1700-4b2f-9ff7-f97b0b162055"}, "366953ba-e3e6-4f02-8e23-356f687fa945": {"doc_hash": "222b37d402280ba4358fab5625332a6a8fa225b6847f05ec8fbcaf27c7b68996", "ref_doc_id": "3fa68b9a-1700-4b2f-9ff7-f97b0b162055"}, "6dbcc888-9064-42f3-89c5-11485572f074": {"doc_hash": "daeae81fa2b2f2362ca434c9cf69d7860c86aa7065199bd8c82f726091f6d394", "ref_doc_id": "3fa68b9a-1700-4b2f-9ff7-f97b0b162055"}, "7b5dc589-333e-4822-8d64-e44356622efe": {"doc_hash": "87a40564b35e612b0056e47b8e02d60e7364a490560a74011563f812c83ca144", "ref_doc_id": "b571e5f2-5732-41c9-ab76-1583a8b95ea5"}, "3a3890db-eb00-484c-81cb-00c4753ccd73": {"doc_hash": "77d6260c9275be877168be8683de376ae61dfdecfb78a913e2622cb9f8ab6c32", "ref_doc_id": "b571e5f2-5732-41c9-ab76-1583a8b95ea5"}, "7472a06c-aa43-4420-8521-8e601d3ab4fc": {"doc_hash": "c34f1697aca29852aac059aeff0a7976a5169b4a5217b6550f0c9ec9c7ea9213", "ref_doc_id": "b571e5f2-5732-41c9-ab76-1583a8b95ea5"}, "93199c91-5086-488e-91f4-f208a2030ee8": {"doc_hash": "f1736cbdcaee1195ddae59b7558a46d270e4fd89ecfc93337d65694fe0461349", "ref_doc_id": "b571e5f2-5732-41c9-ab76-1583a8b95ea5"}, "291d4b2a-f2da-4d90-b1e8-6f43d4814703": {"doc_hash": "cec8b04bd1564653d2e340b579aab9aa300fcd2cc5d5a13a52d8b7dee62eff49", "ref_doc_id": "b571e5f2-5732-41c9-ab76-1583a8b95ea5"}, "d89490a8-9cce-49fe-a837-a721281cca2e": {"doc_hash": "d07f5a968c820fa66277c8d53e93939d8ba5c45334973a356a9a9067683cb0b7", "ref_doc_id": "773594f0-5d4a-49ab-9516-d152f7991587"}, "75668d34-9b3b-49a2-a6a2-2afe99aecacd": {"doc_hash": "ca1f7ee8b9f6c70219d0748e6865988aeef63ec3e1776136c7af3ac1e3855809", "ref_doc_id": "773594f0-5d4a-49ab-9516-d152f7991587"}, "c6022b8c-d37a-4c3f-82e7-1d1710f6f149": {"doc_hash": "d7b0ad5224c831dc4e8bb05e565f994a468f9d10fb77cd22649a278c966aa9b0", "ref_doc_id": "773594f0-5d4a-49ab-9516-d152f7991587"}, "0add8097-61e2-4b6a-9211-8efa93a89164": {"doc_hash": "8c5429a48198039ce19c7a4877f616cb90da8b9d806479b68da94c14d06e8ff0", "ref_doc_id": "773594f0-5d4a-49ab-9516-d152f7991587"}, "3716fe1d-b48d-4ac3-9b5f-3fa4cebeae3c": {"doc_hash": "4b618bc00a9390919c7ec464f0050c0af835f1623f1c87afbdb5e36a979a0318", "ref_doc_id": "773594f0-5d4a-49ab-9516-d152f7991587"}, "bbfcbde5-0704-4928-93c6-e0f9dc9e7700": {"doc_hash": "0a0987beea26ccb20bc2de47b512d4088f9b0b2ed5b2e586529378da658bc766", "ref_doc_id": "773594f0-5d4a-49ab-9516-d152f7991587"}, "02c28097-932c-433a-9fd4-f4ffc87a4941": {"doc_hash": "0e404aa9a046ca58c37069766f4186afe514ca7feccb321bf69a673756f475dc", "ref_doc_id": "37589ad1-7249-4c95-919f-c59b67b41ace"}, "4a3de1ee-dde0-473b-b211-a95aedda4abd": {"doc_hash": "11606d782fb94972326bc709488dabcdb0941e2df699b2ee4ff7049c3e5a5c1e", "ref_doc_id": "37589ad1-7249-4c95-919f-c59b67b41ace"}, "895c1b36-0277-45e4-a7d4-8d16e9d06503": {"doc_hash": "2a458f3e1cab848ee12251c2079ee3afc3551ea316583b826db4f1f8f8798731", "ref_doc_id": "37589ad1-7249-4c95-919f-c59b67b41ace"}, "c993343f-0b07-4e8f-bc46-1c1d8066c7c1": {"doc_hash": "269037bd1738e03c66430608102b6e8657110cbf2ed136cc3c721009c60311e6", "ref_doc_id": "37589ad1-7249-4c95-919f-c59b67b41ace"}, "dbce4d86-a647-4d02-89c4-5bd4b2727b95": {"doc_hash": "350445f0b48b7d38e2da3361358da450ce7587f61aaf44dc59419f538d06834a", "ref_doc_id": "37589ad1-7249-4c95-919f-c59b67b41ace"}, "6cbbc5e1-0a41-4aef-908d-23a514a7e57a": {"doc_hash": "d75eb78419566c15921ee90d416f847956aef69f005d6f803e3d1b05ef656d00", "ref_doc_id": "532ba3a2-4f81-4db0-98b1-d600f70164e1"}, "d3061cab-32b1-4119-b4a7-ded32c90aaa6": {"doc_hash": "cb33542a56d044bf5c1df0512062de3f3eff87f94d11754f086035359c9c492f", "ref_doc_id": "e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8"}, "8d0bb36f-55bb-407c-8dcf-f3d27f01373c": {"doc_hash": "99e867d6ae5752a55eae3ac6915558f7eeb54849b776d87ea1aca15b5b291d5a", "ref_doc_id": "9434786b-aa69-40b3-b9cc-3ecf847eb7a1"}, "d5d4f5f7-3cad-4af9-8f3e-63a6369f48cc": {"doc_hash": "e16b4ac38cba374919d404849a4fc92fe81086bb6d74886b32cbd55af429aafe", "ref_doc_id": "b038a19c-c71f-4121-bd5a-82e96b947813"}, "a800d462-98fe-41c7-b4dd-51b151000173": {"doc_hash": "1c0fc3bcb7b3a90097d8ce3b051995d3340f66d778f4263c6f5536e8c66b6a27", "ref_doc_id": "a885c675-2205-4e32-af60-be54d05f8a5c"}, "b0fcac38-35b2-469f-a995-3af2c379b04b": {"doc_hash": "938649ef35bb83d7bce5997bb497405668d7bb995456b136bb96d6cdbc8e4652", "ref_doc_id": "99b6828c-1bde-47f2-8d06-18aa053bc100"}, "2e034a6e-3446-45ed-b977-45153ee631c5": {"doc_hash": "d386a289af167558f3d6a157a76ba700720e2d1deb4b06b354e29f8b4b9363da", "ref_doc_id": "2d8bdb5b-9c77-46f5-af13-5aa0e0147749"}, "ce9dd654-ca74-4398-b86f-b4b11368a82a": {"doc_hash": "3890668fa0a22c2391c8fce0cb0676488688f71232a1f3b17d65480ac686cfcf", "ref_doc_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510"}, "a75e04ee-de57-4192-8e52-39e206664801": {"doc_hash": "ba974afc9e56d8c5d796a0666ff54abe052bb0f3ad4db8e2bcd75c9ab6dc7293", "ref_doc_id": "75f91c95-e67a-4a4f-836f-fe6f8a4e8f61"}, "ed47203f-b3de-4937-98e2-93d3db63322c": {"doc_hash": "af36b98c2c7027517cdec0c357a897d9a0933e9adf0bcf0ee4f8439b924398f3", "ref_doc_id": "e5b5d091-a362-4b06-9ab4-a6257983cb23"}, "d80958f3-49b3-4626-bc3d-b0ad54aa697a": {"doc_hash": "38054cc34d0576e1fb21534ccd9d1ca92003aea4ed88c384fec4386cac8fc122", "ref_doc_id": "3fa68b9a-1700-4b2f-9ff7-f97b0b162055"}, "1f11f2f7-e4ff-46d4-a696-7ab96391c3f5": {"doc_hash": "b2aeb68feaeb3f45b6f4636415ec8478c203c020b1882775f55fb76a7b899ccd", "ref_doc_id": "b571e5f2-5732-41c9-ab76-1583a8b95ea5"}, "368f84f3-f914-42e7-a10c-f82df1b89ccc": {"doc_hash": "7dc694369c657b557c768cf8cd3553ce08873c7e6246a4016367ec683a7d39f7", "ref_doc_id": "773594f0-5d4a-49ab-9516-d152f7991587"}, "87807c64-df88-46c8-886e-943e227afaea": {"doc_hash": "501fd1b309dfc6ee5e9f695a26010093c9b56fb4f4e2dba329b085aa6d7dfe88", "ref_doc_id": "37589ad1-7249-4c95-919f-c59b67b41ace"}, "0d7d5364-ed68-4b6d-9101-285a47c92db9": {"doc_hash": "9dbbd000fa01a516121092f241ca254cc0b18fa58625c5bac90532541adba0d4", "ref_doc_id": "532ba3a2-4f81-4db0-98b1-d600f70164e1"}}, "docstore/data": {"509156ca-8903-422c-8d55-04d31f7a28c3": {"__data__": {"id_": "509156ca-8903-422c-8d55-04d31f7a28c3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8", "node_type": "4", "metadata": {}, "hash": "22cf53ef4e6d3e04c8cc198e8d6faee490e5860d64080a2daf33cdc69d3696e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39e8e174-2c07-43d1-840e-046161b01ea9", "node_type": "1", "metadata": {}, "hash": "76317590e84d96ecebb69eb1272dec20942a96920493efdcffecc25ff977a147", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# HOW TO PROBE: SIMPLE YET EFFECTIVE TECHNIQUES FOR IMPROVING POST-HOC EXPLANATIONS\n\nAnonymous authors\n\nPaper under double-blind review\n\n(a) Trailer Truck\nCE Loss\nBCE Loss\nBinoculars\nLinear Probe\n3-layer MLP\n\nFigure 1: (a) Using Binary Cross-Entropy (BCE, col. 3) instead of Cross-Entropy (CE, col. 2) to train linear probes on identical backbones significantly increases the class-specificity of explanations. (b) Non-linear probes better extract class-specific features from pretrained representations: moving from a linear to a 3-layer BCE probe noticeably improves class localization. These critical findings are observed across a wide range of pre-training and attribution methods. Here, we show B-cos (B\u00f6hle et al., 2022) explanations for a model pre-trained using DINO (Caron et al., 2021).\n\n# ABSTRACT\n\nPost-hoc importance attribution methods are a popular tool for \u201cexplaining\u201d Deep Neural Networks (DNNs) and are inherently based on the assumption that the explanations can be applied independently of how the models were trained. Contrarily, in this work we bring forward empirical evidence that challenges this very notion. Surprisingly, we discover a strong dependency on and demonstrate that the training details of a pre-trained model\u2019s classification layer (&lt;10% of model parameters) play a crucial role, much more than the pre-training scheme itself. This is of high practical relevance: (1) as techniques for pre-training models are becoming increasingly diverse, understanding the interplay between these techniques and attribution methods is critical; (2) it sheds light on an important yet overlooked assumption of post-hoc attribution methods which can drastically impact model explanations and how they are interpreted eventually. With this finding we also present simple yet effective adjustments to the classification layers, that can significantly enhance the quality of model explanations. We validate our findings across several visual pre-training frameworks (fully-supervised, self-supervised, contrastive vision-language training) and analyze how they impact explanations for a wide range of attribution methods on a diverse set of evaluation metrics. Code to reproduce all experiments: https://anonymous.4open.science/r/how-to-probe-iclr/\n\n# 1 INTRODUCTION\n\nMost prominently in image classification models, importance attribution methods have emerged as a popular approach to mitigate the \u2018black box\u2019 problem of modern Deep Neural Networks (DNNs) (Samek et al., 2021). These methods assign importance values to input features, such as image pixels, to help humans understand why a DNN arrived at a particular classification decision.\n\nWhile most attribution methods do not take the model training into account (are applied \u201cpost-hoc\u201d), given the recent emergence of increasingly diverse pre-training paradigms for representation learning (Chen et al., 2020a;b; He et al., 2019; Chen et al., 2020c; Grill et al., 2020; Caron et al., 2021; Chen & He, 2020; Caron et al., 2020; Bachman et al., 2019; Bardes et al., 2022; Radford et al., 2021) and the popularity of importance attribution methods (Bach et al., 2015; Selvaraju et al., 2017; B\u00f6hle et al., 2022; Shrikumar et al., 2017) to examine model decisions, a better understanding of the interplay between model explanations and training details is of great practical importance.\n\nIn this work, we present an important finding that raises questions about the underlying assumptions of post-hoc attribution methods and their utility on downstream tasks. In particular, we find that\nUnder review as a conference paper at ICLR 2025\n\n| | |DINO: Bcos: W(x) x Input| | |DINO: Layerwise Relevance Propagation| | | | |\n|---|---|---|---|---|---|---|---|---|---|\n| |1.0|1.0|[ 0.8 DINO: Bcos: W(x) x Input DINO: Layerwise Relevance Propagation| | | | | | |\n| | |BCE|BCE| |+4%| | | | |\n| |0.8|CE|+2%| | | | | | |\n| |0.6|0.6| |+6%| | | | | |\n| |0.4|0.4| | | |BCE| | | |\n| |0.2|0.2| | | |CF| | | |\n| |0.0|0.0| | | | | | | |\n\nFigure 2: Impact of Loss (BCE vs. CE).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4035, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39e8e174-2c07-43d1-840e-046161b01ea9": {"__data__": {"id_": "39e8e174-2c07-43d1-840e-046161b01ea9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8", "node_type": "4", "metadata": {}, "hash": "22cf53ef4e6d3e04c8cc198e8d6faee490e5860d64080a2daf33cdc69d3696e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "509156ca-8903-422c-8d55-04d31f7a28c3", "node_type": "1", "metadata": {}, "hash": "9b3c2665492b5def7b084b4d03a92c673434b0768081fd6f8e57b5e220fc4ffe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e3d8929-0c52-4cdc-9a13-6657d08243b2", "node_type": "1", "metadata": {}, "hash": "1ad998aa92dff8ac0928ef899f77c7d19dda354f2a5936ad8d0d26b77273ce46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CE). (a) EPG Scores, and (b) Pixel Deletion scores for Bcos and LRP attributions for a linear probe when trained on frozen pre-trained features (DINO in this case). We find that BCE probes lead to more localized and stable attributions, thus highlighting the significant impact of the loss function on well-established attribution methods. Interestingly, despite the fact that the models differ only by a single classification layer, the attributions show stark differences in commonly used metrics for evaluating their quality.\n\nThe quality of attributions for pre-trained models can be highly dependent on how the classification head (i.e. the \u2018probe\u2019) is trained, even if the model backbone remains frozen. For example, in figure Figure 2a we show that the localization scores of B-cos (B\u00f6hle et al., 2022) and LRP (Bach et al., 2015) attributions significantly differ depending on which loss (BCE / CE) is used, with consistent improvements under the BCE loss; this can also be observed qualitatively, see Figure 1a. Similar improvements can also be seen under the commonly used pixel deletion paradigm for evaluating attribution methods (Figure 2b), despite the fact that the two models only differ by a single linear layer and the linear probes themselves have limited modeling capacity (see discussion Appendix B).\n\nImportantly, we find these effects to be robust across several pre-training paradigms and attribution methods on downstream classification tasks. Specifically, we conduct a thorough evaluation of the resulting explanations on a wide range of common interpretability metrics, that include object localization (Zhang et al., 2018; Samek et al., 2016; Wang et al., 2020), pixel deletion (Samek et al., 2017; Hedstr\u00f6m et al., 2024), compactness (Chalasani et al., 2018) and complexity (Tseng et al., 2020) measures. By this we demonstrate empirically that training the probes via the binary cross-entropy (BCE) loss, as opposed to the conventionally used cross-entropy (CE) loss, leads to consistent and significant gains across several interpretability metrics, which thus might have important implications for many DNN-based applications.\n\nTo further study the impact of the probes, we investigate the interplay between the probe\u2019s complexity and the properties of the resulting explanations. Interestingly, we find that more complex (i.e. multi-layer) probes can better distill class-specific features from the pre-trained representations and thus significantly increase the localization performance (cf. Figure 1b), in particular when using interpretable B-cos Multi-Layer Perceptrons (MLPs) (B\u00f6hle et al., 2023).\n\nIn short, we make the following contributions: (1) We identify and analyze a critical yet overlooked problem of importance attribution methods, namely that how models are trained can significantly impact the resulting attributions. (2) We show both quantitatively and qualitatively that even when models differ only in their linear probe, explanations can dramatically differ based on the objective used to train the probe on downstream tasks; in particular, we find that using BCE instead of CE leads to significantly improved explanations when evaluated on a diverse set of interpretability metrics. (3) We demonstrate that our findings are independent of how the visual encoder is trained by conducting a detailed study across supervised, self-supervised (MoCov2 (He et al., 2019), DINO (Caron et al., 2021), BYOL (Grill et al., 2020)), and vision-language-based learning (CLIP, (Radford et al., 2021)). For this, we use diverse explanation methods (LRP (Bach et al., 2015), IntGrad (Sundararajan et al., 2017), B-cos (B\u00f6hle et al., 2022), Input\u00d7Gradients (Shrikumar et al., 2017), GradCAM (Selvaraju et al., 2017), LIME (Ribeiro et al., 2016)) and assess the quality of the resulting explanations on multiple datasets (ImageNet (Russakovsky et al., 2015), VOC (Everingham et al.), and COCO (Lin et al., 2014)). (4) Furthermore, we find that non-linear B-cos MLP probes further boost downstream performance and \u2018class-specific\u2019 localization ability of attribution methods across pre-trained backbones. (5) We also show for the first time that the inherently interpretable B-cos (B\u00f6hle et al., 2022) models are compatible with SSL approaches, preserving both their performance and interpretability.\n\nOur findings uncover a new crucial aspect of explainable artificial intelligence (XAI) that should be considered when using existing attribution methods or developing new ones in the future.", "mimetype": "text/plain", "start_char_idx": 4031, "end_char_idx": 8561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e3d8929-0c52-4cdc-9a13-6657d08243b2": {"__data__": {"id_": "2e3d8929-0c52-4cdc-9a13-6657d08243b2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8", "node_type": "4", "metadata": {}, "hash": "22cf53ef4e6d3e04c8cc198e8d6faee490e5860d64080a2daf33cdc69d3696e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39e8e174-2c07-43d1-840e-046161b01ea9", "node_type": "1", "metadata": {}, "hash": "76317590e84d96ecebb69eb1272dec20942a96920493efdcffecc25ff977a147", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "670cb8a2-96b6-44f3-a01b-2be22d814996", "node_type": "1", "metadata": {}, "hash": "a814cc110e1d7d39b1952b2ce0f1c2fbc933dd2be19728fcc464f280c69558d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "), and COCO (Lin et al., 2014)). (4) Furthermore, we find that non-linear B-cos MLP probes further boost downstream performance and \u2018class-specific\u2019 localization ability of attribution methods across pre-trained backbones. (5) We also show for the first time that the inherently interpretable B-cos (B\u00f6hle et al., 2022) models are compatible with SSL approaches, preserving both their performance and interpretability.\n\nOur findings uncover a new crucial aspect of explainable artificial intelligence (XAI) that should be considered when using existing attribution methods or developing new ones in the future.\nUnder review as a conference paper at ICLR 2025\n\n# 2 RELATED WORK\n\nImportance Attributions. To understand deep neural networks (DNNs), several post-hoc attribution methods (Selvaraju et al., 2017; Chattopadhay et al., 2018; Bach et al., 2015; Jiang et al., 2021; Wang et al., 2020; Desai & Ramaswamy, 2020; Petsiuk et al., 2018; Shrikumar et al., 2017; Sundararajan et al., 2017; Rao et al., 2022; Simonyan et al., 2013; Teney et al., 2020; Fong & Vedaldi, 2017; Zeiler & Fergus, 2014; Ribeiro et al., 2016; Springenberg et al., 2015; Dabkowski & Gal, 2017), as well as inherently interpretable models (Chen et al., 2019; Brendel & Bethge, 2019; B\u00f6hle et al., 2021; 2022) have been developed. The attribution (or explanation) map summarizes the DNN computation and assigns importance values to the pixels that the model has used to make a decision. Such attribution methods can be broadly classified into the following categories: (1) backpropagation-based (Bach et al., 2015; Sundararajan et al., 2017; Shrikumar et al., 2017; Springenberg et al., 2015) that rely on gradients computed either with respect to the input or intermediate layers, (2) perturbation-based (Ribeiro et al., 2016; Petsiuk et al., 2018) that assign importance by noting the change in output on perturbing the input while treating the network as a \u2018black-box\u2019, (3) activation-based (Selvaraju et al., 2017; Jiang et al., 2021; Wang et al., 2020; Desai & Ramaswamy, 2020) that leverage the weights of activation maps across different layers to assign importance values to the input, and (4) inherently interpretable models (Chen et al., 2019; Brendel & Bethge, 2019; B\u00f6hle et al., 2021; 2022) that have been architecturally designed to be more interpretable. In this work, we present surprising empirical evidence that shows that both \u2018post-hoc\u2019 and inherently interpretable explanation methods highly depend on the weights of the last classification layer, such as the linear probe for self-supervised methods.\n\nEvaluating Importance Attributions. Recent work has studied important properties of such explanation methods, like faithfulness (Adebayo et al., 2018; Hooker et al., 2019; Srinivas & Fleuret, 2018; Rao et al., 2022), robustness to adversarial attacks (Ghorbani et al., 2019; Slack et al., 2020; Dombrowski et al., 2019) and fairness (Dai et al., 2022). In contrast to this, we focus on another important dimension\u2014the sensitivity of explanations to the model training, which has thus far not been systematically studied. And although a dependence on the training has been reported in a few instances (Caron et al., 2021; Tsipras et al., 2019; B\u00f6hle et al., 2023), these are mostly limited to single explanation methods and pre-training paradigms. Moreover, while explanation methods are often developed and evaluated in the context of supervised models, the pre-training paradigms even for classification models have become increasingly diverse. Given the fact that many explanations are applied \u2018post hoc\u2019, it is important to understand whether and to what degree they yield consistent results independent of the pre-training paradigm. To address this, we systematically study a wide range of explanation methods across a variety of pre-trained backbones and find that the results are consistent across explanation methods, suggesting that our conclusions are not an artifact of a particular explanation method or backbone combination.\n\nNon-linear Probes.", "mimetype": "text/plain", "start_char_idx": 7951, "end_char_idx": 12004, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "670cb8a2-96b6-44f3-a01b-2be22d814996": {"__data__": {"id_": "670cb8a2-96b6-44f3-a01b-2be22d814996", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8", "node_type": "4", "metadata": {}, "hash": "22cf53ef4e6d3e04c8cc198e8d6faee490e5860d64080a2daf33cdc69d3696e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e3d8929-0c52-4cdc-9a13-6657d08243b2", "node_type": "1", "metadata": {}, "hash": "1ad998aa92dff8ac0928ef899f77c7d19dda354f2a5936ad8d0d26b77273ce46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c90203a-1040-443c-b8eb-dcee414e230d", "node_type": "1", "metadata": {}, "hash": "c3fbff81d5ec002c088fa8d5c7cf94a85ec4bed97e4e6213830f78f0bb63a5b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And although a dependence on the training has been reported in a few instances (Caron et al., 2021; Tsipras et al., 2019; B\u00f6hle et al., 2023), these are mostly limited to single explanation methods and pre-training paradigms. Moreover, while explanation methods are often developed and evaluated in the context of supervised models, the pre-training paradigms even for classification models have become increasingly diverse. Given the fact that many explanations are applied \u2018post hoc\u2019, it is important to understand whether and to what degree they yield consistent results independent of the pre-training paradigm. To address this, we systematically study a wide range of explanation methods across a variety of pre-trained backbones and find that the results are consistent across explanation methods, suggesting that our conclusions are not an artifact of a particular explanation method or backbone combination.\n\nNon-linear Probes. He et al. (2022) argue that linear probes cannot disentangle non-linear representations, and show improved accuracy by fine-tuning multiple-layers of pre-trained models. Li et al. (2023) propose to dynamically choose the complexity of a \u2018readout\u2019 (i.e. probing) module to increase performance. Similarly, we also find non-linear probes to consistently improve classification accuracy. In contrast to these works, we use interpretable MLPs that lead to both improved accuracy and quality of explanations.\n\n# 3 INTERPRETABLE PROBING OF PRE-TRAINED REPRESENTATIONS\n\nTo test the generality of our findings and isolate the impact of pre-training, we evaluate model explanations across a broad range of pre-training paradigms, with a specific focus on commonly used self-supervised representation learning methods. Linear probing of pre-trained models is a widely adopted approach to evaluate the learned representations on downstream tasks, making it very relevant to understand how to obtain effective explanations for combined models (backbone + probe).\n\n# 3.1 SETUP\n\nTo holistically evaluate explanations methods, we utilize a diverse suite of interpretability metrics described below. Note, however, that in contrast to fully supervised models, where output neurons are optimized to represent specific classes, this is not the case for self-supervised models.\nUnder review as a conference paper at ICLR 2025\n\n# Step 1: Train Linear Probe\n\nBCE or CE\n\n# Step 2: Evaluate Explanation Quality\n\nLoss\n\nModel Prediction: Butterfly\n\n|Image|Pre-trained Encoder|Probe b()|Explanation|\n|---|---|---|---|\n|2x2 Input|Pre-trained Encoder|Probe b()|2x2 (GridPG)|\n\nFigure 3: Setup: Step 1. Linear or MLP probes h are trained on frozen pre-trained models f. Step 2. Explanation methods are applied to the classification predictions of the trained probes, and evaluated across a wide array of interpretability metrics to assess explanation quality (e.g. localization).\n\nTo nonetheless compare the explanations obtained from various pre-trained model backbones, we propose the following experimental setting: (1) we pre-train the models based on various pre-training paradigms and freeze the model parameters; (2) we train linear and non-linear classifiers (probes) on those frozen features for downstream image classification; (3) we apply attribution methods to the classification predictions of the trained probes and evaluate the quality of the generated explanations (see Figure 3).\n\nThis allows us to compare all pre-trained backbones in a standardized setting and to leverage existing evaluation metrics that were developed in the context of explaining classification models (see below).\n\nFurther, by freezing the backbone features, we are able to isolate the impact of the pre-training paradigm. Finally, note that with a linear probe, the classification output is the result of simply a linear combination of the backbone representations. For most attribution methods (e.g. LRP, B-cos, LIME, IxG, and IntGrad, see Section 4.2), the resulting importance attributions, in turn, are also just a linear superposition of the attributions that would be obtained for individual neurons in the backbones\u2019 representations, and therefore a direct reflection of the backbones\u2019 interpretability itself.\n\n# Evaluation metrics\n\nTo assess the class-specificity of model explanations, we follow prior work and measure the fraction of positive contributions Ai that fall within a pre-specified region R of a given image vs. the total amount of positive contributions \u03a3Ai. The score si for each image i is thus given by si = Ai R/\u03a3A. We discuss the motivation for our metric selection in Appendix A.\n\nFor single-label classification (ImageNet (Russakovsky et al., 2015)), we employ the grid pointing game (GridPG) (B\u00f6hle et al., 2021; 2022; Zhang et al., 2018; Samek et al., 2016).", "mimetype": "text/plain", "start_char_idx": 11069, "end_char_idx": 15857, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c90203a-1040-443c-b8eb-dcee414e230d": {"__data__": {"id_": "2c90203a-1040-443c-b8eb-dcee414e230d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8", "node_type": "4", "metadata": {}, "hash": "22cf53ef4e6d3e04c8cc198e8d6faee490e5860d64080a2daf33cdc69d3696e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "670cb8a2-96b6-44f3-a01b-2be22d814996", "node_type": "1", "metadata": {}, "hash": "a814cc110e1d7d39b1952b2ce0f1c2fbc933dd2be19728fcc464f280c69558d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Evaluation metrics\n\nTo assess the class-specificity of model explanations, we follow prior work and measure the fraction of positive contributions Ai that fall within a pre-specified region R of a given image vs. the total amount of positive contributions \u03a3Ai. The score si for each image i is thus given by si = Ai R/\u03a3A. We discuss the motivation for our metric selection in Appendix A.\n\nFor single-label classification (ImageNet (Russakovsky et al., 2015)), we employ the grid pointing game (GridPG) (B\u00f6hle et al., 2021; 2022; Zhang et al., 2018; Samek et al., 2016). Here, the trained models are evaluated on a synthetic grid of images of distinct classes and for each of the class logits the region R is given by its respective position in the synthetic image grid, see also Figure 4.\n\nFor multi-label classification (VOC (Everingham et al.), COCO (Lin et al., 2014)), we rely on the bounding box annotations provided in the datasets and use the energy pointing game (EPG) (Wang et al., 2020). I.e., the region R corresponds to the bounding boxes of the class for which the explanations are computed. The ImageNet validation set also includes bounding box annotations, which we use to additionally report the EPG score on this dataset. We report both the GridPG and EPG scores in percentages, with higher scores indicating better localization.\n\nWe further analyze the explanations using the pixel deletion method (Samek et al., 2017; Hedstr\u00f6m et al., 2024), and also evaluate their compactness (Gini index p.p. as in (Chalasani et al., 2018)) and complexity (entropy as in (Tseng et al., 2020)), thus ensuring a comprehensive evaluation setting.\n\n# 3.2 THE IMPACT OF THE PROBES\u2019 TRAINING OBJECTIVE\n\nAs discussed in the previous section, we train linear probes on the frozen, pre-trained representations, to apply common explanation methods and metrics.", "mimetype": "text/plain", "start_char_idx": 15286, "end_char_idx": 17144, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9f04337a-d2b9-4eb9-b530-25e4c7aecdbd": {"__data__": {"id_": "9f04337a-d2b9-4eb9-b530-25e4c7aecdbd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9434786b-aa69-40b3-b9cc-3ecf847eb7a1", "node_type": "4", "metadata": {}, "hash": "1dff8fe0ae0cc0e2572b7ffeda2c2e1b4d320a1c196f6c6c0aad29c68a7a9615", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f63766c7-afd7-45b9-a0b4-554f2cb3ec2b", "node_type": "1", "metadata": {}, "hash": "0dffb4c85c1051c2aecd5b0088df3c702764bb24a6e820f2e8aafa47efa9cc7b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We further analyze the explanations using the pixel deletion method (Samek et al., 2017; Hedstr\u00f6m et al., 2024), and also evaluate their compactness (Gini index p.p. as in (Chalasani et al., 2018)) and complexity (entropy as in (Tseng et al., 2020)), thus ensuring a comprehensive evaluation setting.\n\n# 3.2 THE IMPACT OF THE PROBES\u2019 TRAINING OBJECTIVE\n\nAs discussed in the previous section, we train linear probes on the frozen, pre-trained representations, to apply common explanation methods and metrics.\n\nFollowing prior work (Caron et al., 2021; Grill et al., 2020; Chen et al., 2020c;a;b; He et al., 2022), we optimize these probes via the cross-entropy (CE) loss LCE,i for image i, which is given by\n\nLCE,i = \u2212 XlogPexp (\u02c6c,i) kexp (\u02c6k,i) \u00d7 tc,i .\n\nHere, \u02c6c,i denotes the probe\u2019s output logit for class c and input i, and tc,i the respective one-hot encoded label. Interestingly, in our experiments we noticed that the explanations of the predicted class\nUnder review as a conference paper at ICLR 2025\n\n# Cross Entropy\n\n|Input|Probe 1|Probe 2|\n|---|---|---|\n| |+1.0| |\n| |[|7|\n| |-1.0| |\n\nExplained Class: Bighorn\n\nLoss = 4.08\n\nLoss = 4.08\n\nGridPG Score 65.7%/ GridPG Score = 11.9%\n\nFigure 4: Due to the shift-invariance of softmax, one cannot expect positive and negative attribu-\n\ntions to be well calibrated, which can lead to unintuitive model explanations, see also Equation (3).\n\nSpecifically, one can easily define equivalent linear probes (Probe 1,2) that achieve the same CE\n\nloss, but visually dissimilar explanations and GridPG scores (65.7% vs. 11.9%). Col. 2+3 show\n\nLRP Bach et al. (2015) attributions for two equivalent probes explaining the same class (bighorn).\n\nTo obtain more interpretable post-hoc explanations, we therefore propose to train linear probes with\n\nthe BCE loss, cf. Section 3.2.\n\nfor CE-trained linear probes were highly distributed and failed to localize the class objects effec-\n\ntively (see Figure 1a). We hypothesize that this could be due to the shift-invariance of the CE loss.\n\n# Softmax Shift-Invariance Issue\n\nTo understand this, note that the CE loss is invariant to adding a shift \u03b4 to all output logits, as long as this shift is the same for all classes (Srinivas & Fleuret, 2021); in fact, this shift can even be specific to image i (see Figure 4):\n\nPexp(\u02c6c,i + \u03b4i) = Pexp(\u02c6c,i) exp(\u03b4i)\n\nkexp(\u02c6k,i + \u03b4i) = kexp(\u02c6k,i) exp(\u03b4i)\n\n(2)\n\nImportantly, note that such an image-specific shift can be obtained by shifting the probes\u2019 weight\n\nvectors wk for all classes k by a fixed vector wk\u2032 = wk + \u2206w:\n\nPkexp(wk + \u2206wTai) = Pexp(kexp\u2032c ai)\n\n(3)\n\nHere, ai denotes the backbones\u2019 frozen input representation. As can be seen from Equations (2)\n\nand (3), there are an infinite number of linear probes which achieve the same loss and are thus\n\nindistinguishable as far as the optimization is concerned.\n\nAs most attribution methods, in some form or another, rely on the models\u2019 weights to compute\n\nthe importance attributions, this reliance can have a crucial impact on the resulting explanations,\n\nas we show in Figure 4. Specifically, we show the attributions derived via layer-wise relevance\n\npropagation (Bach et al., 2015) for two functionally equivalent probes. While the probes give the\n\nsame predictions and achieve the same loss for every input x by design, they yield vastly different\n\nimportance attribution maps (cf. Sundararajan et al. (2017)). This, in turn, results in very different\n\nGridPG scores (65.7% vs. 11.9%).\n\nSince both probes are equivalent under CE-based optimization, it cannot be expected that for\n\nCE-trained probes the attributions are calibrated such that \u2018positive\u2019 attributions will be class-\n\nspecific. Therefore, we additionally evaluate BCE-based probes, which do not exhibit the same\n\nshift invariance as CE models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f63766c7-afd7-45b9-a0b4-554f2cb3ec2b": {"__data__": {"id_": "f63766c7-afd7-45b9-a0b4-554f2cb3ec2b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9434786b-aa69-40b3-b9cc-3ecf847eb7a1", "node_type": "4", "metadata": {}, "hash": "1dff8fe0ae0cc0e2572b7ffeda2c2e1b4d320a1c196f6c6c0aad29c68a7a9615", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f04337a-d2b9-4eb9-b530-25e4c7aecdbd", "node_type": "1", "metadata": {}, "hash": "91fab7d3c987a8b1694e2022e36b5fcedd8694e6e8fe2ada14d68d5d0b95caa3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8407880d-4a1d-49b4-b361-f96b2da6c972", "node_type": "1", "metadata": {}, "hash": "34a48ee320dd284a49e7e42c7533d32d803b6c7773af19eeff1372c34d3c236d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Specifically, we show the attributions derived via layer-wise relevance\n\npropagation (Bach et al., 2015) for two functionally equivalent probes. While the probes give the\n\nsame predictions and achieve the same loss for every input x by design, they yield vastly different\n\nimportance attribution maps (cf. Sundararajan et al. (2017)). This, in turn, results in very different\n\nGridPG scores (65.7% vs. 11.9%).\n\nSince both probes are equivalent under CE-based optimization, it cannot be expected that for\n\nCE-trained probes the attributions are calibrated such that \u2018positive\u2019 attributions will be class-\n\nspecific. Therefore, we additionally evaluate BCE-based probes, which do not exhibit the same\n\nshift invariance as CE models.\n\n# BCE Probing\n\nThe BCE objective has recently been shown to perform well for image classification\n\n(Wightman et al., 2021; B\u00f6hle et al., 2023); in detail, the BCE loss is given by\n\nLBCE,i = \u2212 \u03a3c tc,i log(\u03c3(\u02c6c,i)) + (1 \u2212 tc,i) log(1 \u2212 \u03c3(\u02c6c,i))\n\n(4)\n\nwith \u03c3 denoting the sigmoid function. Importantly, in contrast to the CE loss, the BCE loss is not\n\nshift-invariant. Specifically, note that for BCE, the linear probe is penalized for adding a con-\n\nstant positive shift to non-target classes and thus biased towards focusing on class-specific features.\nUnder review as a conference paper at ICLR 2025\n\nWe therefore expect it to result in better calibrated explanations, which is indeed what we observe: specifically, as we show in Section 5.1, BCE probes exhibit a significantly higher degree of class-specificity and lend themselves better for localizing class objects. They also lead to more stable predictions under the pixel deletion evaluation, as well as more compact and less complex explanations.\n\n# 3.3 NON-LINEAR PROBING WITH INTERPRETABLE MLPS\n\nIn addition to the impact of the probes\u2019 loss function on the explanations (Section 3.2), here we discuss the interplay between classifier\u2019s complexity and the resulting explanations. In particular, we note that features computed by self-supervised (Chen et al., 2020c; Grill et al., 2020; Caron et al., 2021) or vision-language backbones (Radford et al., 2021) are not necessarily optimized to be linearly separable with respect to the classes of any arbitrary downstream task (He et al., 2022). We posit that this lack of linear separability might further diminish the localization ability of explanation methods, as different classes may share certain features in the frozen feature representations of pre-trained backbones that are not trained with full supervision. To address this, prior work (Li et al., 2023; Hewitt & Liang, 2019) investigates using non-linear probes, which have been shown to result in improved downstream performance. However, this might come at a cost to model interpretability, as it has been shown (Rao et al., 2022) that explanation methods like GradCAM (Selvaraju et al., 2017), perform significantly worse at earlier layers than at the last layer of a DNN.\n\nMLP Probes. To mitigate this, we propose to use an interpretable Multi-Layer Perceptron (MLP) probing technique to improve model accuracy and explanation quality. Specifically, we also train more complex probes on the frozen features, namely two-layer and three-layer conventional and B-cos (B\u00f6hle et al., 2022) MLPs and evaluate how this impacts the explanations\u2019 class-specificity. A conventional fully connected MLP f (x; \u03b8) with L layers is given by:\n\nf (x; \u03b8) = lL \u25e6 lL\u22121 \u25e6 . . . \u25e6 l2 \u25e6 l1(x) (5)\n\nwhere lj denotes a linear layer j with parameters Wj, and \u03b8 is the set of all parameters within the MLP. For a given input al to layer l, the output is computed as: l(al; Wl) = \u03d5(Wl al), with \u03d5 a non-linear activation function (e.g. ReLU).\n\nInstead of computing their outputs as a ReLU-activated linear transformation, the B-cos layers ll\u2217 employ the B-cos transformation, which is given by:\n\nB-cos layer ll\u2217(al; Wl) = |c(al; W)|B\u22121 \u2299 Wl al (6)\n\nHere, \u2299 is row-wise multiplication, i.e.", "mimetype": "text/plain", "start_char_idx": 3063, "end_char_idx": 7030, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8407880d-4a1d-49b4-b361-f96b2da6c972": {"__data__": {"id_": "8407880d-4a1d-49b4-b361-f96b2da6c972", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9434786b-aa69-40b3-b9cc-3ecf847eb7a1", "node_type": "4", "metadata": {}, "hash": "1dff8fe0ae0cc0e2572b7ffeda2c2e1b4d320a1c196f6c6c0aad29c68a7a9615", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f63766c7-afd7-45b9-a0b4-554f2cb3ec2b", "node_type": "1", "metadata": {}, "hash": "0dffb4c85c1051c2aecd5b0088df3c702764bb24a6e820f2e8aafa47efa9cc7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d2c42d6-8d07-4514-8e33-6741baffeeba", "node_type": "1", "metadata": {}, "hash": "ffdff9c08ed1fecac21878b793434720770363f11426194ed35c0c8a4c59c2ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". . \u25e6 l2 \u25e6 l1(x) (5)\n\nwhere lj denotes a linear layer j with parameters Wj, and \u03b8 is the set of all parameters within the MLP. For a given input al to layer l, the output is computed as: l(al; Wl) = \u03d5(Wl al), with \u03d5 a non-linear activation function (e.g. ReLU).\n\nInstead of computing their outputs as a ReLU-activated linear transformation, the B-cos layers ll\u2217 employ the B-cos transformation, which is given by:\n\nB-cos layer ll\u2217(al; Wl) = |c(al; W)|B\u22121 \u2299 Wl al (6)\n\nHere, \u2299 is row-wise multiplication, i.e. all rows of Wl are scaled by the scalar entries of the vector to its left and c computes the cosine similarity between the input vector al and each row of Wl. B\u00f6hle et al. (2022) showed that the additional cosine factor in Equation (6) induces increased weight-input alignment during optimization, which significantly increases the localization performance of a linear summary of the B-cos models. Interestingly, we show in Section 5.2, using B-cos MLP probes on conventional backbones can also improve the localization of post-hoc explanations.\n\n# 4 EXPERIMENTAL SCOPE\n\nIn the following section, we outline our experimental scope, and discuss the pre-training (Section 4.1) and explanation methods (Section 4.2) that we evaluate.\n\n# 4.1 PRE-TRAINING FRAMEWORKS\n\nWe aim to have a broad enough representative set that highlights how our evaluation generalizes across differently trained feature extractors, particularly (1) fully-supervised, (2) self-supervised, and (3) contrastive vision-language learning.\n\n(1) Fully-Supervised Learning First, we evaluate the explanation methods on fully supervised backbones. On the one hand, backbones pre-trained in a supervised manner are still often used for transfer learning (Cheng et al., 2022; Xie et al., 2021; Chen et al., 2017). On the other hand, an\nUnder review as a conference paper at ICLR 2025\n\n324 evaluation of fully supervised models also provides a useful reference value, as most explanation methods have been developed in this context.\n\n325 In addition to evaluating the end-to-end trained classifiers, we also evaluate linear probes on the frozen representations of these models, in order to increase the comparability to the self-supervised approaches we present in the following.\n\n326 (2) Self-Supervised Learning. We consider three popular self-supervised pre-training frameworks: MoCov2 (Chen et al., 2020c), BYOL (Grill et al., 2020), and DINO (Caron et al., 2021).\n\n327 (3) Vision-Language Learning. For this we use the multi-modal CLIP (Radford et al., 2021), that is pre-trained on a large-scale dataset comprising of image-text pairs.\n\n328 To summarize, we evaluate across a broad spectrum of pre-training mechanisms: a contrastive, two self-distillation-based, and a multi-modal pre-training paradigm, which cover some of the most popular approaches to self-supervised learning. We contrast our evaluations with fully-supervised trained models (for more details refer to Appendix F.3).\n\n# 4.2 EXPLANATION METHODS\n\n341 To evaluate model interpretability, we apply explanation methods to the classification predictions of the probes trained on the frozen features (cf. Figure 3 and Section 3.1). In the following, we provide a short overview on the explanation methods.\n\n342 Input\u00d7Gradient (Shrikumar et al., 2017) is a backpropagation-based attribution method that involves taking the element-wise product of the input and the gradient. For a given model f (x; \u03b8) and input x, it is denoted by x \u2299 \u2202f (x;\u03b8). \u2202x\n\n343 Integrated Gradients (Sundararajan et al., 2017) follows an axiomatic method and is formulated as the integral of gradients over a straight line path from a baseline input x\u2032 to the given input x. IntGrad for an input x is equal to (x \u2212 x\u2032) \u00d7 R 1\u2202f (x\u2032+\u03b1(x\u2212x\u2032))\u2202\u03b1.\n\n344 Layer-wise Relevance Propagation (Bach et al., 2015) generates attribution maps by propagating relevance scores backwards through the network, thus decomposing the model prediction into contributions from individual input features.", "mimetype": "text/plain", "start_char_idx": 6522, "end_char_idx": 10517, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d2c42d6-8d07-4514-8e33-6741baffeeba": {"__data__": {"id_": "6d2c42d6-8d07-4514-8e33-6741baffeeba", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9434786b-aa69-40b3-b9cc-3ecf847eb7a1", "node_type": "4", "metadata": {}, "hash": "1dff8fe0ae0cc0e2572b7ffeda2c2e1b4d320a1c196f6c6c0aad29c68a7a9615", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8407880d-4a1d-49b4-b361-f96b2da6c972", "node_type": "1", "metadata": {}, "hash": "34a48ee320dd284a49e7e42c7533d32d803b6c7773af19eeff1372c34d3c236d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a6d0d3a-bc83-4e90-bf9b-4dc8b6ec4a5d", "node_type": "1", "metadata": {}, "hash": "f838aa453879dd4a4e548da5a5f307c5695f6f65c8368b375389df3d6baa9990", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For a given model f (x; \u03b8) and input x, it is denoted by x \u2299 \u2202f (x;\u03b8). \u2202x\n\n343 Integrated Gradients (Sundararajan et al., 2017) follows an axiomatic method and is formulated as the integral of gradients over a straight line path from a baseline input x\u2032 to the given input x. IntGrad for an input x is equal to (x \u2212 x\u2032) \u00d7 R 1\u2202f (x\u2032+\u03b1(x\u2212x\u2032))\u2202\u03b1.\n\n344 Layer-wise Relevance Propagation (Bach et al., 2015) generates attribution maps by propagating relevance scores backwards through the network, thus decomposing the model prediction into contributions from individual input features. Many relevance-propagation rules exist, in this work we focus on the EpsilonGammaBox composite because it has been shown to work particularly well in (Montavon et al., 2019; Rao et al., 2023a).\n\n345 GradCAM (Selvaraju et al., 2017) is an activation based explanation method, that generates attributions corresponding to the gradient of the class logit with respect to the feature map of the last convolutional layer of a DNN.\n\n346 LIME (Ribeiro et al., 2016) samples perturbed versions of the input of interest and observes the changes in predictions. A linear model is fit to these perturbed instances to provide local explanations for a model\u2019s decision.\n\n347 B-cos (B\u00f6hle et al., 2022) are attributions generated by the inherently interpretable B-cos networks. Essentially, the attribution map is computed by an element-wise product of the dynamic weights with the input that faithfully encapsulates the contribution of each pixel to a given class c: (Wc T(x) \u2299 x).\n\n348 In short, we evaluate a wide range of attribution methods, including gradient-based, activation-based, perturbation-based post-hoc explanations, as well as the inherent model explanations of the recently proposed B-cos models.\n\n349 For exact implementation details on datasets, models, pre-training please see Appendix F.\n\n# 5 RESULTS\n\n374 We now discuss our experimental findings. Specifically, we analyze the impact of the probes\u2019 optimization objective (Section 5.1) and the probe complexity (Section 5.2) on the model explanations.\nUnder review as a conference paper at ICLR 2025\n\n| | | | |Bcos: W(x) x Input| |Integrated Gradients| |LIME| | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |80| | | |100| | | | | | | | | | | | | | | |\n| |1| | | | | | |6| | | | | | | | | | | | |\n| | |60| | | | | | | | | | | | | | | | | | |\n| | |40| | | | | | | | | | | | | | | | | | |\n| | |20|BCE|BCE| | | | | | | | | | | | | | | | |\n| | | |CE|CE| | | | | | | | | | | | | | | | |\n| | | |64|66|68|70|72|74|76|64|66|68|Accuracy|72|70|74|76| | | |\n| | | | | | |Accuracy| | | | | | | | | | | | | | |\n\nFigure 5: BCE vs. CE \u2014 Accuracy and GridPG scores on ImageNet. GridPG scores improve significantly when the linear probe is trained with binary cross entropy (BCE) loss as compared to cross entropy (CE) loss, and this is consistent across pre-training paradigms, as well as for B-cos models (top row) and conventional models (bottom row). Additional XAI methods in Appendix.\n\n# 5.1 IMPACT OF BCE VS. CE\n\nIn the following, we evaluate the impact of the optimization objective of the linear probes on their accuracy and on the explanations. To assess the quality of the explanations, we report results on the metrics discussed previously in Section 3.1.\n\n# Localization\n\nIn Figure 5, we plot the linear probe accuracy versus the GridPG scores for MoCov2 (red), BYOL (green), DINO (blue), CLIP (yellow) and supervised (black) models for B-cos/LRP (col. 1), IntGrad (col. 2) and LIME (col.", "mimetype": "text/plain", "start_char_idx": 9937, "end_char_idx": 13524, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a6d0d3a-bc83-4e90-bf9b-4dc8b6ec4a5d": {"__data__": {"id_": "5a6d0d3a-bc83-4e90-bf9b-4dc8b6ec4a5d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9434786b-aa69-40b3-b9cc-3ecf847eb7a1", "node_type": "4", "metadata": {}, "hash": "1dff8fe0ae0cc0e2572b7ffeda2c2e1b4d320a1c196f6c6c0aad29c68a7a9615", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d2c42d6-8d07-4514-8e33-6741baffeeba", "node_type": "1", "metadata": {}, "hash": "ffdff9c08ed1fecac21878b793434720770363f11426194ed35c0c8a4c59c2ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Additional XAI methods in Appendix.\n\n# 5.1 IMPACT OF BCE VS. CE\n\nIn the following, we evaluate the impact of the optimization objective of the linear probes on their accuracy and on the explanations. To assess the quality of the explanations, we report results on the metrics discussed previously in Section 3.1.\n\n# Localization\n\nIn Figure 5, we plot the linear probe accuracy versus the GridPG scores for MoCov2 (red), BYOL (green), DINO (blue), CLIP (yellow) and supervised (black) models for B-cos/LRP (col. 1), IntGrad (col. 2) and LIME (col. 3) on ImageNet. We do so for linear probes trained via BCE (filled markers) and CE (hollow markers) for conventional (row 2) and B-cos (row 1) models.\n\nWe find significant gains in the explanations\u2019 localization for all approaches, models, and explanations when using the BCE loss instead of the commonly used CE loss. E.g., for conventional models explained via LRP (Figure 5, col. 1, row 2), the GridPG score for CLIP improves by 32p.p. (19%\u219251%), MoCov2 improves by 30p.p. (50%\u219280%), for BYOL by 18p.p. (48%\u219266%), and for DINO even by 40p.p. (52%\u219292%).\n\nSimilarly, the GridPG score for B-cos explanations also significantly increases (Figure 5, col. 1, row 1): for MoCov2, it improves by 33p.p. (48%\u219281%), for BYOL by 28p.p. (43%\u219271%), and for DINO by 37p.p. (43%\u219280%).\n\nInterestingly, I\u00d7G and GradCAM (see appendix) only show consistent improvements for B-cos models. For I\u00d7G, this is in line with prior work, as model gradients for conventional models are known to suffer from \u2018shattered gradients\u2019 (cf. (Balduzzi et al., 2017)).\n\nNotably, the observed improvements in localization lead to significant qualitative improvements regarding the class-specificity of the explanations, see Figure 6 (left). The B-cos attributions in Figure 6 (left) for probes trained with the BCE loss (top row) are much more localized as compared to the probes trained with CE loss (bottom row). We see this behavior is consistent across all pre-training approaches; for results on additional explanation methods, please see the appendix. To also compare with supervised models qualitatively, in Figure 6 (right) we show B-cos explanations for DINO (cols. 2+3) and supervised models (cols. 4+5). We find both explanations to visually look similar, and to follow a similar trend when trained using either CE (cols. 2+4) or BCE (cols. 3+5). For BCE, the model attributions are better localized for both types of training.\n\nIn Figure 2a similar improvements are seen for the EPG metric (on ImageNet) when using BCE probes for B-cos and LRP attributions.", "mimetype": "text/plain", "start_char_idx": 12978, "end_char_idx": 15559, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69dc2a78-d7e0-4d04-af6f-70a28d6fca29": {"__data__": {"id_": "69dc2a78-d7e0-4d04-af6f-70a28d6fca29", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b038a19c-c71f-4121-bd5a-82e96b947813", "node_type": "4", "metadata": {}, "hash": "174e05d397138babb9acc5327d18692ed92b972ad946563d09fcbab1d1ebda98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7e4035d-e71d-444b-bacf-08a674fad8fd", "node_type": "1", "metadata": {}, "hash": "7ab75c6c0d7f7f4d5c016b7b11d7bc079f16db32efb53671103d4e9d351f4068", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2+3) and supervised models (cols. 4+5). We find both explanations to visually look similar, and to follow a similar trend when trained using either CE (cols. 2+4) or BCE (cols. 3+5). For BCE, the model attributions are better localized for both types of training.\n\nIn Figure 2a similar improvements are seen for the EPG metric (on ImageNet) when using BCE probes for B-cos and LRP attributions. We specifically see a greater improvement on smaller bounding boxes thus highlighting the ability for improved localization as compared to CE probes.\n\n# Pixel Deletion\n\nUnder the pixel deletion setup, in fig. 2b we observe the BCE probes to lead to more stable predictions when least important pixels are successively removed. This is consistent across majority of the pre-training methods as well as attribution methods (see Appendix for more details).\nUnder review as a conference paper at ICLR 2025\n\nInput\nDINO\nBYOL\nMCov2\nSupervised\nInput\nDINO\nSupervised\nClass: Popsicle\n\n2x2 GridPG\n\nCE\nBCE\nCE\nBCE\nFigure 6: (a) BCE vs. CE. The B-cos attributions for a linear probe trained with CE loss (bottom row) leak into nearby cells in the 2x2 GridPG evaluation setting. The attributions for linear probes trained with a BCE loss (top row) are consistently much more localized. (b) SSL vs Supervised. The B-cos attributions for DINO (cols. 2+3) are visually very similar to supervised models (cols. 4+5), despite being optimized very differently, thus highlighting the importance of the training objective of the linear probe.\n\nBasketball Punch Bag Umbrella Dragonfly Plunger Nail Flute\n\n1\n1\n\n4\nFigure 7: Qualitative results of MLP probes on ImageNet. We find that explanations for B-cos MLPs trained on the DINO features exhibit better localization than a linear probe. For other pre-training and explanation methods, see Appendix.\n\nComplexity and Compactness. Table E1 demonstrates a consistent improvement in compactness (Gini index p.p.) and reduction in complexity (entropy) for BCE vs. CE, except for I\u00d7G and Grad-CAM in the case of conventional backbones. We thus note, that in contrast to the loss function, the choice of pre-training method has a limited impact only on explanation quality, with no particular method consistently outperforming others.\n\n# 5.2 EFFECT OF MORE COMPLEX PROBES\n\nIn this section, we present the experimental results on the effect of training more complex classifiers on top of the frozen features. In particular, we train 2- and 3-layer MLPs and evaluate how this affects the performance both in terms of accuracy and the quality of the explanations.\n\nIn Figure 8, we show the results on ImageNet for B-cos MLPs. It can be seen that for all pre-trained models there is not only a steady increase in accuracy but also an improvement in the localization ability (GridPG scores) of model explanations.\n\nE.g., for standard models explained via IntGrad we observe the following improvements when going from a single linear probe to a 3-layer MLP in (accuracy, GridPG): MoCov2 (66.9, 69.0) \u2192 (71.3, 83.0), BYOL (69.3, 83.0) \u2192 (72.1, 85.0), and DINO (70.6, 70.0) \u2192 (73.1, 89.0). A similar trend is observed for B-cos models, and across explanation methods and datasets (see Figure 8 for results on COCO), with GradCAM being an exception (for details and discussion on this, see the Appendix).\n\nIn Figure 9, the EPG scores (for ImageNet with bounding boxes) improve across all pre-training paradigms for B-cos MLP probes, which is especially prominent in smaller bounding boxes. Furthermore, Figure 7 depicts the observed localization improvements qualitatively. We find MLP probes to better localize the object class of interest, relying less on background. This indicates that they better capture class-specific features, which, in turn, improves their classification performance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3798, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7e4035d-e71d-444b-bacf-08a674fad8fd": {"__data__": {"id_": "c7e4035d-e71d-444b-bacf-08a674fad8fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b038a19c-c71f-4121-bd5a-82e96b947813", "node_type": "4", "metadata": {}, "hash": "174e05d397138babb9acc5327d18692ed92b972ad946563d09fcbab1d1ebda98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69dc2a78-d7e0-4d04-af6f-70a28d6fca29", "node_type": "1", "metadata": {}, "hash": "d8ab9a6198b1feb9ca02ba2dbdef3309326bc3ad6d5c6f73ccc524f38a3f2a49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb5b8920-a3bf-4919-a56b-50da41b12714", "node_type": "1", "metadata": {}, "hash": "8444063ebb461611323fd4efd5707e2fb807a711dd818d744cf1c3abaf6c2cc0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A similar trend is observed for B-cos models, and across explanation methods and datasets (see Figure 8 for results on COCO), with GradCAM being an exception (for details and discussion on this, see the Appendix).\n\nIn Figure 9, the EPG scores (for ImageNet with bounding boxes) improve across all pre-training paradigms for B-cos MLP probes, which is especially prominent in smaller bounding boxes. Furthermore, Figure 7 depicts the observed localization improvements qualitatively. We find MLP probes to better localize the object class of interest, relying less on background. This indicates that they better capture class-specific features, which, in turn, improves their classification performance.\n# Under review as a conference paper at ICLR 2025\n\n| | |B-RNSO, Bcos W(x) x Input| |B-RNSO, Input x Gradient| | | |RNSO, Integrated Gradients| | | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|100|3|80| | | | | | | | | | | | | | | | | | | |\n| |1|60|8| | | | | | | | | | | | | | | | | | |\n| |40| | | | | | | |3| | | | | | | | | | | | |\n| |20| | |LP|MLP-2|MLP-3| | | | | | | | | | | | | | | |\n| |64|58| | | | | | |Accuracy|70|72|64|56|68| | | | | | | |\n| | | | | | | | | |Accuracy|70|72| | | | | |14|64|66|68|70|\n| | | | | | | | | |Accuracy| | | | | | | |74|72|76|78|80|\n| | |B-RNSO, Bcos: W(x) x Input| |B-RNSO, Input x Gradient| | | |RNSO, Integrated Gradients| | | | | | | | | | | | | |\n|60| |MOCOV2|MOCOV2| | | | | | | | | | | | | | | | | | |\n| | |BYOL|BYOL|BYOL| | | | | | | | | | | | | | | | | |\n| |50|DINO|DINO|DINO| | | | | | | | | | | | | | | | | |\n| | |SUP|SUP|SUP| | | | | | | | | | | | | | | | | |\n| |1|40| | | | | | |2| | | | | | | | | | | | |\n| | |MOCOv2|DINO|BYOL|SUP| | | | | | | | | | | | | | | | |\n| |g|30| |DINO-| | | |MOCOv2| | | | | | | | | | | | | |\n| | | | | | | | |BYOL|SUP|MOcOv2|BYOL|DINO|\"SUP| | | | | | | | |\n|20| | | | | | | | | | | | | | | | | | | | | |\n| |10| | | | | | | | | | | | | | | | | | | | |\n\nFigure 8: Effect of more complex probes on accuracy and GridPG scores on ImageNet (top) and COCO (bottom). A more complex B-cos probe (MLP) not only increases performance on the downstream task (x-axis), but interestingly also the GridPG score (y-axis, top) and the EPG score (y-axis, bottom) of the explanations (individual plots). This is true for B-cos models (left 2 cols.) and for conventional models (right 1 cols.) probed via B-cos MLPs. For conventional MLPs, the trends are less consistent, see the Appendix for more details and additional results on this.\n\nInterestingly, this trend in improvement in both accuracy and localization is only seen consistently for B-cos MLPs, independently of them being applied to conventional or B-cos models. While an increase in localization as measured by the EPG score is observed for conventional MLPs on COCO, on ImageNet we see a consistent decrease in the GridPG score (see Appendix).", "mimetype": "text/plain", "start_char_idx": 3096, "end_char_idx": 6031, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb5b8920-a3bf-4919-a56b-50da41b12714": {"__data__": {"id_": "bb5b8920-a3bf-4919-a56b-50da41b12714", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b038a19c-c71f-4121-bd5a-82e96b947813", "node_type": "4", "metadata": {}, "hash": "174e05d397138babb9acc5327d18692ed92b972ad946563d09fcbab1d1ebda98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7e4035d-e71d-444b-bacf-08a674fad8fd", "node_type": "1", "metadata": {}, "hash": "7ab75c6c0d7f7f4d5c016b7b11d7bc079f16db32efb53671103d4e9d351f4068", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb8ad3af-5a02-45d2-b4a8-949d3c9842a6", "node_type": "1", "metadata": {}, "hash": "6431f29c167f4dc00068615fdd4bb5a6fd23065f86b44c9686b3acc5285bba45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A more complex B-cos probe (MLP) not only increases performance on the downstream task (x-axis), but interestingly also the GridPG score (y-axis, top) and the EPG score (y-axis, bottom) of the explanations (individual plots). This is true for B-cos models (left 2 cols.) and for conventional models (right 1 cols.) probed via B-cos MLPs. For conventional MLPs, the trends are less consistent, see the Appendix for more details and additional results on this.\n\nInterestingly, this trend in improvement in both accuracy and localization is only seen consistently for B-cos MLPs, independently of them being applied to conventional or B-cos models. While an increase in localization as measured by the EPG score is observed for conventional MLPs on COCO, on ImageNet we see a consistent decrease in the GridPG score (see Appendix). In order to get well-localizing attribution maps on downstream tasks, we thus recommend to probe pre-trained models via B-cos probes.\n\nIn short, we find that training relatively lightweight B-cos MLPs (\u223c 10% of entire model parameters) on frozen features of SSL-trained models with BCE loss is a versatile approach to obtain both highly interpretable and also highly performant classifiers on downstream tasks.\n\n|MoCov2|Bcos: W(x) x Input|BYOL|Bcos: W(x) x Input| |DINO|Bcos: W(x) x Input| | | |\n|---|---|---|---|---|---|---|---|---|---|\n|1.0|1.0|1.0| | | | | | | |\n|MLP-3|+1%/ 0.8|MLP-3| |+4%/|MLP-3| | | | |\n|0.8|MLP-2|+2%/|MLP-2|+5%/|0.8| | | | |\n|MLP-2|LP|+2%/|LP|+6%/|+5%/| | | | |\n|1|0.6|0.6|0.6| | | | | | |\n|+2%/| |+7%/| | | |+8%/| | | |\n|0.4|0.4|0.4| | | | | | | |\n|0.2|0.2|0.2| | | | | | | |\n|0.0|0.0|0.0| | | | | | | |\n|&lt;25%|&lt;50%|&lt;75%|&lt;100%| | | | | | |\n|BBOX %|BBOX %|BBOX %| | | | | | | |\n\nFigure 9: Stronger probes lead to more localized attributions. EPG scores on ImageNet.\n\nNote: We add more results for Vision Transformers(Xie et al., 2022; Chefer et al., 2020; Abnar & Zuidema, 2020; B\u00a8ohle et al., 2023) in Appendix C and ScoreCAM(Wang et al., 2020) in Table E3.\n\n# CONCLUSION\n\nWe discover an important and surprising finding, that the quality of explanations derived from a wide range of attribution methods for pre-trained models is more dependent on how the classification layer is trained for a given downstream task, than on the choice of pre-training paradigm itself. This places important practical considerations on end-users when using \u2018post-hoc\u2019 attribution methods that are typically assumed to be applied independent of model training. Further, we showed that employing lightweight multi-layer B-cos probes contributes to enhanced localization performance of the explanations, providing a simple and effective improvement. We support our findings with extensive experimental evaluation across several pre-training frameworks (fully-supervised, self-supervised, vision-language pre-training), and analysis on the quality of explanations for popular attribution methods on a diverse evaluation setting. As our findings are robust to the pre-training paradigms, and therefore can have broader implications for many DNN-based applications that rely on XAI methods.\n# Under review as a conference paper at ICLR 2025\n\n# Reproducibility Statement\n\nWe provide the complete code for pre-training, probing and evaluation of the trained models as well as for generating the quantitative and qualitative results of the explanation methods used. The code is well-documented with helper scripts to run the different parts of the pipeline and help with reproducibility. Additionally, we will also make the entire pipeline available to the broader community by open-sourcing our software and provide the pre-trained model checkpoints which further helps in reproducing the results in the manuscript.", "mimetype": "text/plain", "start_char_idx": 5203, "end_char_idx": 8959, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb8ad3af-5a02-45d2-b4a8-949d3c9842a6": {"__data__": {"id_": "bb8ad3af-5a02-45d2-b4a8-949d3c9842a6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b038a19c-c71f-4121-bd5a-82e96b947813", "node_type": "4", "metadata": {}, "hash": "174e05d397138babb9acc5327d18692ed92b972ad946563d09fcbab1d1ebda98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb5b8920-a3bf-4919-a56b-50da41b12714", "node_type": "1", "metadata": {}, "hash": "8444063ebb461611323fd4efd5707e2fb807a711dd818d744cf1c3abaf6c2cc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25103751-f41d-4b4f-8652-8b0c942c054c", "node_type": "1", "metadata": {}, "hash": "f17a6dcd3fd52f8819ad7d16855324c599295c6020b9a86bd2c1f2a73ae17887", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We support our findings with extensive experimental evaluation across several pre-training frameworks (fully-supervised, self-supervised, vision-language pre-training), and analysis on the quality of explanations for popular attribution methods on a diverse evaluation setting. As our findings are robust to the pre-training paradigms, and therefore can have broader implications for many DNN-based applications that rely on XAI methods.\n# Under review as a conference paper at ICLR 2025\n\n# Reproducibility Statement\n\nWe provide the complete code for pre-training, probing and evaluation of the trained models as well as for generating the quantitative and qualitative results of the explanation methods used. The code is well-documented with helper scripts to run the different parts of the pipeline and help with reproducibility. Additionally, we will also make the entire pipeline available to the broader community by open-sourcing our software and provide the pre-trained model checkpoints which further helps in reproducing the results in the manuscript. We were also careful to only use open-source software, and publicly available datasets, which thus supports reproducible research. This is an effort to provide transparency, and encourage further research in the field. Code to reproduce all experiments: https://anonymous.4open.science/r/how-to-probe-iclr/\n\n# Broader Impact and Ethics Statement\n\nThe findings and discussion in our work opens up a new conversation about designing attribution / XAI methods that do take the training details of underlying models into account when getting explanations for their decisions. The fact that this also impacts inherently interpretable models, which are designed to intrinsically learn interpretable features during training, speaks more to the importance of the finding (i.e. we need to be very careful about how to handle such models and methods). The user, really needs to consider such details when leveraging model explanations.\n\nTypically past research has focused on evaluating the interpretability of attribution methods given a fixed model, post-hoc. Yet, in the real world, we often have use-cases where one has a large pre-trained backbone over which probes are trained depending on the downstream task. In this work, we find that, surprisingly, how these probes are trained can have a significant impact on how interpretable the attributions of the model is using a fixed post hoc attribution method. Given that training such probes is usually much cheaper as compared to the backbone, our findings can be used to guide the training to yield models that are more interpretable post-hoc. Notably, we find that with this simple approach, the improvements hold across model architectures, pre-training paradigms, and even across different attribution methods.\n\nTo conclude, we see no apparent ethical concerns raised by the scientific discovery presented in our work. However, we do acknowledge the privilege and availability of resources that enable deep learning research, e.g. running large-scale training on GPUs that do result in an increased carbon footprint and efforts must be made to be more careful when using such resources.\n\n# REFERENCES\n\nSamira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Annual Meeting of the Association for Computational Linguistics, 2020.\n\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS\u201918, pp. 9525\u20139536, Red Hook, NY, USA, 2018. Curran Associates Inc.\n\nSebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PLoS ONE, 10, 2015.\n\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning Representations by Maximizing Mutual Information across Views. Curran Associates Inc., Red Hook, NY, USA, 2019.\n\nDavid Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, pp. 342\u2013350. PMLR, 2017.\n\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. In ICLR, 2022.\n\nWieland Brendel and Matthias Bethge. Approximating cnns with bag-of-local-features models works surprisingly well on imagenet. International Conference on Learning Representations, 2019.\nUnder review as a conference paper at ICLR 2025\n\nMoritz B\u00f6hle, Mario Fritz, and Bernt Schiele.", "mimetype": "text/plain", "start_char_idx": 7899, "end_char_idx": 12648, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25103751-f41d-4b4f-8652-8b0c942c054c": {"__data__": {"id_": "25103751-f41d-4b4f-8652-8b0c942c054c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b038a19c-c71f-4121-bd5a-82e96b947813", "node_type": "4", "metadata": {}, "hash": "174e05d397138babb9acc5327d18692ed92b972ad946563d09fcbab1d1ebda98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb8ad3af-5a02-45d2-b4a8-949d3c9842a6", "node_type": "1", "metadata": {}, "hash": "6431f29c167f4dc00068615fdd4bb5a6fd23065f86b44c9686b3acc5285bba45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, pp. 342\u2013350. PMLR, 2017.\n\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. In ICLR, 2022.\n\nWieland Brendel and Matthias Bethge. Approximating cnns with bag-of-local-features models works surprisingly well on imagenet. International Conference on Learning Representations, 2019.\nUnder review as a conference paper at ICLR 2025\n\nMoritz B\u00f6hle, Mario Fritz, and Bernt Schiele. Convolutional dynamic alignment networks for interpretable classifications. 2021.\n\nMoritz B\u00f6hle, Mario Fritz, and Bernt Schiele. B-cos networks: Alignment is all we need for interpretability. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10319\u201310328, 2022.\n\nMoritz B\u00f6hle, Navdeeppal Singh, Mario Fritz, and Bernt Schiele. B-cos Alignment for Inherently Interpretable CNNs and Vision Transformers, 2023.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, Red Hook, NY, USA, 2020. Curran Associates Inc.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9630\u20139640, 2021.\n\nPrasad R. Chalasani, Jiefeng Chen, Amrita Roy Chowdhury, Somesh Jha, and Xi Wu. Concise explanations of neural networks using adversarial training. In International Conference on Machine Learning, 2018.\n\nAditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 839\u2013847, 2018.\n\nHila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 782\u2013791, 2020. URL https://api.semanticscholar.org/CorpusID:229297908.\n\nChaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, and Cynthia Rudin. This Looks like That: Deep Learning for Interpretable Image Recognition. Curran Associates Inc., Red Hook, NY, USA, 2019.\n\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4): 834\u2013848, 2017.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML\u201920.", "mimetype": "text/plain", "start_char_idx": 11969, "end_char_idx": 15112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "921588a1-14f3-4360-97d8-c0e69e040e90": {"__data__": {"id_": "921588a1-14f3-4360-97d8-c0e69e040e90", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a885c675-2205-4e32-af60-be54d05f8a5c", "node_type": "4", "metadata": {}, "hash": "0171ff74fb4dc58b586a7b264c4187648fa1c366a1a65cdf538056c6a75bc3d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93fd2242-059f-4a53-9399-7bbc25608b6c", "node_type": "1", "metadata": {}, "hash": "aef303af2b6e950de07adec1651580a036855c68d058fe50a352db2eb2f87ef6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4): 834\u2013848, 2017.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML\u201920. JMLR.org, 2020a.\n\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020b.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15745\u201315753, 2020.\n\nXinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. ArXiv, abs/2003.04297, 2020c.\n\nBowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. 2022.\n\nPiotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, pp. 6970\u20136979, Red Hook, NY, USA, 2017. Curran Associates Inc.\n\nJessica Dai, Sohini Upadhyay, Ulrich Aivodji, Stephen H. Bach, and Himabindu Lakkaraju. Fairness via explanation quality: Evaluating disparities in the quality of post hoc explanations. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, AIES \u201922, pp. 203\u2013214, New York, NY, USA, 2022. Association for Computing Machinery.\nUnder review as a conference paper at ICLR 2025\n\n648 Saurabh Desai and Harish G. Ramaswamy. Ablation-cam: Visual explanations for deep convolu-\n\n649     tional network via gradient-free localization. In 2020 IEEE Winter Conference on Applications of\n\n650     Computer Vision (WACV), pp. 972\u2013980, 2020.\n\n651\n\n652  Ann-Kathrin Dombrowski, Maximilian Alber, Christopher J. Anders, Marcel Ackermann, Klaus-\n\n653     Robert M\u00a8                                             uller, and Pan Kessel. Explanations can be manipulated and geometry is to blame.\n\n654     Curran Associates Inc., Red Hook, NY, USA, 2019.\n\n655  M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.                                             The\n\n656     PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results.                                  http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.\n\n657  Ruth C. Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful per-\n\n658     turbation. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 3449\u20133457,\n\n659     2017.\n\n660\n\n661  Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile.\n\n662     Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):3681\u20133688, Jul. 2019.\n\n663  Priya Goyal, Piotr Doll\u00b4                                                         ar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,\n\n664     Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training ima-\n\n665     genet in 1 hour. ArXiv, abs/1706.02677, 2017.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3351, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93fd2242-059f-4a53-9399-7bbc25608b6c": {"__data__": {"id_": "93fd2242-059f-4a53-9399-7bbc25608b6c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a885c675-2205-4e32-af60-be54d05f8a5c", "node_type": "4", "metadata": {}, "hash": "0171ff74fb4dc58b586a7b264c4187648fa1c366a1a65cdf538056c6a75bc3d9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "921588a1-14f3-4360-97d8-c0e69e040e90", "node_type": "1", "metadata": {}, "hash": "f36749948accac7d9556364d598a1812239628c5c4d6647e55401d3375938107", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dcea69f3-ff89-4127-a2e8-f21e4e12b472", "node_type": "1", "metadata": {}, "hash": "b9ec4f7cacc4ab4dee6c6c0b5429e0b88dcc47f0ce62ac3e94c3dbf3ee52b6e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 3449\u20133457,\n\n659     2017.\n\n660\n\n661  Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile.\n\n662     Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):3681\u20133688, Jul. 2019.\n\n663  Priya Goyal, Piotr Doll\u00b4                                                         ar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,\n\n664     Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training ima-\n\n665     genet in 1 hour. ArXiv, abs/1706.02677, 2017.\n\n666  Jean-Bastien Grill, Florian Strub, Florent Altch\u00b4                                                                                  e, Corentin Tallec, Pierre H. Richemond, Elena\n\n667     Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Ghesh-\n\n668     laghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00b4                                                                                 emi Munos, and Michal Valko. Bootstrap your own\n\n669     latent a new approach to self-supervised learning. In Proceedings of the 34th International Con-\n\n670     ference on Neural Information Processing Systems, NIPS\u201920, Red Hook, NY, USA, 2020. Curran\n\n671     Associates Inc.\n\n672\n\n673  Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition\n\n674     supplementary materials. 2016.\n\n675  Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for\n\n676     unsupervised visual representation learning. 2020 IEEE/CVF Conference on Computer Vision\n\n677     and Pattern Recognition (CVPR), pp. 9726\u20139735, 2019.\n\n678  Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4                                                                                              ar, and Ross Girshick. Masked\n\n679     Autoencoders are Scalable Vision Learners. In Proceedings of the IEEE/CVF conference on\n\n680     computer vision and pattern recognition, pp. 16000\u201316009, 2022.\n\n681\n\n682  Anna Hedstr\u00a8                                             om, Leander Weber, Dilyara Bareeva, Daniel Krakowczyk, Franz Motzkus, Wojciech\n\n683     Samek, Sebastian Lapuschkin, and Marina M.-C. H\u00a8                                                                                    ohne. Quantus: An explainable ai toolkit for\n\n684     responsible evaluation of neural network explanations and beyond. J. Mach. Learn. Res., 24(1),\n\n685     March 2024. ISSN 1532-4435.\n\n686  John Hewitt and Percy Liang. Designing and interpreting probes with control tasks. In Conference\n\n687     on Empirical Methods in Natural Language Processing. Association for Computational Linguis-\n\n688     tics, 2019.\n\n689  Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretabil-\n\n690     ity methods in deep neural networks. Curran Associates Inc., Red Hook, NY, USA, 2019.\n\n691\n\n692  Peng-Tao Jiang, Chang-Bin Zhang, Qibin Hou, Ming-Ming Cheng, and Yunchao Wei. Layercam:\n\n693     Exploring hierarchical class activation maps for localization. IEEE Transactions on Image Pro-\n\n694     cessing, 30:5875\u20135888, 2021.\n\n695  Diederik P. Kingma and Jimmy Ba.                   Adam: A method for stochastic optimization.                     CoRR,\n\n696     abs/1412.6980, 2014.\n\n697\n\n698  Alexander Kolesnikov, Alexey Dosovitskiy, Dirk Weissenborn, Georg Heigold, Jakob Uszkoreit,\n\n699     Lucas Beyer, Matthias Minderer, Mostafa Dehghani, Neil Houlsby, Sylvain Gelly, Thomas Un-\n\n700     terthiner, and Xiaohua Zhai. An image is worth 16x16 words: Transformers for image recognition\n\n701     at scale. 2021.\n# Under review as a conference paper at ICLR 2025\n\nYazhe Li, Jorg Bornschein, and Marcus Hutter.", "mimetype": "text/plain", "start_char_idx": 2737, "end_char_idx": 6528, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dcea69f3-ff89-4127-a2e8-f21e4e12b472": {"__data__": {"id_": "dcea69f3-ff89-4127-a2e8-f21e4e12b472", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a885c675-2205-4e32-af60-be54d05f8a5c", "node_type": "4", "metadata": {}, "hash": "0171ff74fb4dc58b586a7b264c4187648fa1c366a1a65cdf538056c6a75bc3d9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93fd2242-059f-4a53-9399-7bbc25608b6c", "node_type": "1", "metadata": {}, "hash": "aef303af2b6e950de07adec1651580a036855c68d058fe50a352db2eb2f87ef6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be25a61e-f0ae-4f95-bfa8-a6e35c77247d", "node_type": "1", "metadata": {}, "hash": "007eca2326b0071de076d75145b0e1ff9064b217d9799a9491cd0c5423a22fac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Transactions on Image Pro-\n\n694     cessing, 30:5875\u20135888, 2021.\n\n695  Diederik P. Kingma and Jimmy Ba.                   Adam: A method for stochastic optimization.                     CoRR,\n\n696     abs/1412.6980, 2014.\n\n697\n\n698  Alexander Kolesnikov, Alexey Dosovitskiy, Dirk Weissenborn, Georg Heigold, Jakob Uszkoreit,\n\n699     Lucas Beyer, Matthias Minderer, Mostafa Dehghani, Neil Houlsby, Sylvain Gelly, Thomas Un-\n\n700     terthiner, and Xiaohua Zhai. An image is worth 16x16 words: Transformers for image recognition\n\n701     at scale. 2021.\n# Under review as a conference paper at ICLR 2025\n\nYazhe Li, Jorg Bornschein, and Marcus Hutter. Evaluating Representations with Readout Model Switching. In The Eleventh International Conference on Learning Representations, 2023.\n\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u2019a r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014.\n\nGr\u00e9goire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert M\u00fcller. Layer-Wise Relevance Propagation: An Overview, pp. 193\u2013209. Springer International Publishing, Cham, 2019.\n\nMeike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt, J\u00f6rg Schl\u00f6tterer, Maurice van Keulen, and Christin Seifert. From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai. ACM Comput. Surv., 55 (13s), July 2023. ISSN 0360-0300.\n\nVitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of black-box models. In Proceedings of the British Machine Vision Conference (BMVC), 2018.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8748\u20138763. PMLR, 18\u201324 Jul 2021.\n\nSukrut Rao, Moritz B\u00f6hle, and Bernt Schiele. Towards better understanding attribution methods. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10213\u201310222, 2022.\n\nSukrut Rao, Moritz D Boehle, and Bernt Schiele. Better understanding differences in attribution methods via systematic evaluations. ArXiv, abs/2303.11884, 2023a.\n\nSukrut Rao, Moritz B\u00f6hle, Amin Parchami-Araghi, and Bernt Schiele. Studying how to efficiently and effectively guide models with explanations. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1922\u20131933, October 2023b.\n\nMarco T\u00falio Ribeiro, Sameer Singh, and Carlos Guestrin. \u201cwhy should I trust you?\u201d: Explaining the predictions of any classifier. In HLT-NAACL Demos, pp. 97\u2013101. The Association for Computational Linguistics, 2016.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015.\n\nWojciech Samek, Alexander Binder, Gr\u00e9goire Montavon, Sebastian Lapuschkin, and Klaus-Robert M\u00fcller.", "mimetype": "text/plain", "start_char_idx": 5874, "end_char_idx": 9306, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be25a61e-f0ae-4f95-bfa8-a6e35c77247d": {"__data__": {"id_": "be25a61e-f0ae-4f95-bfa8-a6e35c77247d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a885c675-2205-4e32-af60-be54d05f8a5c", "node_type": "4", "metadata": {}, "hash": "0171ff74fb4dc58b586a7b264c4187648fa1c366a1a65cdf538056c6a75bc3d9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dcea69f3-ff89-4127-a2e8-f21e4e12b472", "node_type": "1", "metadata": {}, "hash": "b9ec4f7cacc4ab4dee6c6c0b5429e0b88dcc47f0ce62ac3e94c3dbf3ee52b6e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "253fdafe-73fa-44f7-b65b-68ed46fce9ab", "node_type": "1", "metadata": {}, "hash": "d009c881a8ac3d24cea975d3996c3eec9d17ff67633229f25e3288cda7b893ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Marco T\u00falio Ribeiro, Sameer Singh, and Carlos Guestrin. \u201cwhy should I trust you?\u201d: Explaining the predictions of any classifier. In HLT-NAACL Demos, pp. 97\u2013101. The Association for Computational Linguistics, 2016.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015.\n\nWojciech Samek, Alexander Binder, Gr\u00e9goire Montavon, Sebastian Lapuschkin, and Klaus-Robert M\u00fcller. Evaluating the visualization of what a deep neural network has learned. IEEE transactions on neural networks and learning systems, 28(11):2660\u20132673, 2016.\n\nWojciech Samek, Alexander Binder, Gregoire Montavon, Sebastian Lapuschkin, and Klaus-Robert M\u00fcller. Evaluating the visualization of what a deep neural network has learned. IEEE Transactions on Neural Networks and Learning Systems, 28:2660\u20132673, 11 2017.\n\nWojciech Samek, Gr\u00e9goire Montavon, Sebastian Lapuschkin, Christopher J. Anders, and Klaus-Robert M\u00fcller. Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE, 109(3):247\u2013278, 2021.\n\nRamprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 618\u2013626, 2017.\nUnder review as a conference paper at ICLR 2025\n\nRamprasaath R Selvaraju, Karan Desai, Justin Johnson, and Nikhil Naik. CASTing your model: Learning to Localize Improves Self-supervised Representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\n\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, pp. 3145\u20133153. JMLR.org, 2017.\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR, abs/1312.6034, 2013.\n\nDylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. Fooling lime and shap: Adversarial attacks on post hoc explanation methods. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, AIES \u201920, pp. 180\u2013186, New York, NY, USA, 2020. Association for Computing Machinery.\n\nJost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving for simplicity: The all convolutional net. In ICLR (Workshop), 2015.\n\nSuraj Srinivas and Francois Fleuret. Rethinking the role of gradient-based attribution methods for model interpretability. In International Conference on Learning Representations, 2021.\n\nSuraj Srinivas and Fran\u00e7ois Fleuret. Knowledge transfer with jacobian matching. In International Conference on Machine Learning, 2018.\n\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, pp. 3319\u20133328. JMLR.org, 2017.\n\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, pp. 1195\u20131204, Red Hook, NY, USA, 2017. Curran Associates Inc.", "mimetype": "text/plain", "start_char_idx": 8688, "end_char_idx": 12289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "253fdafe-73fa-44f7-b65b-68ed46fce9ab": {"__data__": {"id_": "253fdafe-73fa-44f7-b65b-68ed46fce9ab", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a885c675-2205-4e32-af60-be54d05f8a5c", "node_type": "4", "metadata": {}, "hash": "0171ff74fb4dc58b586a7b264c4187648fa1c366a1a65cdf538056c6a75bc3d9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be25a61e-f0ae-4f95-bfa8-a6e35c77247d", "node_type": "1", "metadata": {}, "hash": "007eca2326b0071de076d75145b0e1ff9064b217d9799a9491cd0c5423a22fac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In International Conference on Learning Representations, 2021.\n\nSuraj Srinivas and Fran\u00e7ois Fleuret. Knowledge transfer with jacobian matching. In International Conference on Machine Learning, 2018.\n\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, pp. 3319\u20133328. JMLR.org, 2017.\n\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, pp. 1195\u20131204, Red Hook, NY, USA, 2017. Curran Associates Inc.\n\nDamien Teney, Ehsan Abbasnedjad, and Anton van den Hengel. Learning what makes a difference from counterfactual examples and gradient supervision. pp. 580\u2013599, Berlin, Heidelberg, 2020. Springer-Verlag.\n\nAlex M. Tseng, Avanti Shrikumar, and Anshul Kundaje. Fourier-transform-based attribution priors improve the interpretability and stability of deep learning models for genomics. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS \u201920, Red Hook, NY, USA, 2020. Curran Associates Inc.\n\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In International Conference on Learning Representations, 2019.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\n\nHaofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu. Score-cam: Score-weighted visual explanations for convolutional neural networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 111\u2013119, 2020.\n\nRoss Wightman, Hugo Touvron, and Herve Jegou. Resnet strikes back: An improved training procedure in timm. In NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future, 2021.\n\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In Neural Information Processing Systems (NeurIPS), 2021.\nUnder review as a conference paper at ICLR 2025\n\nWeiyan Xie, Xiao hui Li, Caleb Chen Cao, and Nevin L. Zhang. Vit-cx: Causal explanation of vision transformers. In International Joint Conference on Artificial Intelligence, 2022.\n\nMatthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV (1), volume 8689 of Lecture Notes in Computer Science, pp. 818\u2013833. Springer, 2014.\n\nJianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126 (10):1084\u20131102, 2018.\n\nKe Zhu and Jianxin Wu.", "mimetype": "text/plain", "start_char_idx": 11554, "end_char_idx": 14715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e03c852-bbb7-4f8c-a6b6-f982e30aefa3": {"__data__": {"id_": "0e03c852-bbb7-4f8c-a6b6-f982e30aefa3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99b6828c-1bde-47f2-8d06-18aa053bc100", "node_type": "4", "metadata": {}, "hash": "b83badc1b39ddc55aaa2a40efea5af6ef91478a7d649a4ebdabf5d9dfe68133d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a18f3843-a381-48ce-8c5a-1fb2d3577082", "node_type": "1", "metadata": {}, "hash": "c8cda78ebb581e7d4b618c62e4135791b2b6e75125347495b8b82e392477fcdd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV (1), volume 8689 of Lecture Notes in Computer Science, pp. 818\u2013833. Springer, 2014.\n\nJianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126 (10):1084\u20131102, 2018.\n\nKe Zhu and Jianxin Wu. Residual attention: A simple but effective method for multi-label recognition. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 184\u2013193, 2021.\n# Appendix\n\n# Table of Contents\n\nIn this appendix to our work on simple yet effective techniques for improving post-hoc explanations, we provide:\n\n- (A) Discussion on Evaluation Metrics and Co-12 Properties . . . . . . . . . . . . . . . . 18\nIn this section, we provide a discussion on the importance of the interpretability metrics we have selected in our evaluations, and also place them in context with the recently proposed Co-12 Properties Nauta et al. (2023).\n- (B) Linear Probing On Frozen Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nIn this section, we provide a short mathematical proof to show that when linear probing on frozen pre-trained backbone features, it simply results in learning a weighted linear combination of the backbone features.\n- (C) Additional Architectures: Vision Transformers . . . . . . . . . . . . . . . . . . . . . . . . 20\nIn this section, we provide quantitative results on Vision Transformers (ViTs, Kolesnikov et al. (2021)) when evaluated on their related explanation methods.\n- (D) Additional Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nIn this section, we provide additional qualitative results for different evaluation settings, for both B-cos models as well as conventional models. Specifically, we qualitatively show examples highlighting the impact of using BCE vs. CE loss and the effect of using more complex probes. Further, we provide a comparison between different explanation methods for all pre-training (supervised and SSL) frameworks evaluated in our work. We provide more qualitative results in Sec. D.4.\n- (E) Additional Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\nIn this section, we provide additional quantitative results: (1) GridPG results of additional explanation methods on ImageNet, and (2) EPG results for additional methods on COCO and VOC. In table E3 we provide quantitative results for ScoreCAM (Wang et al., 2020).\n- (F) Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\nIn this section, we provide additional details regarding datasets, training, implementation and evaluation procedures.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2865, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a18f3843-a381-48ce-8c5a-1fb2d3577082": {"__data__": {"id_": "a18f3843-a381-48ce-8c5a-1fb2d3577082", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99b6828c-1bde-47f2-8d06-18aa053bc100", "node_type": "4", "metadata": {}, "hash": "b83badc1b39ddc55aaa2a40efea5af6ef91478a7d649a4ebdabf5d9dfe68133d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e03c852-bbb7-4f8c-a6b6-f982e30aefa3", "node_type": "1", "metadata": {}, "hash": "ccfa5c4e0e55bd5b993f1c621e216ff2003a70c64ce8541c8bbc0262efb2ba22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11958e16-6d14-4071-a88b-8ce34e958501", "node_type": "1", "metadata": {}, "hash": "7812005deda0f1a3ae9e2b101928cc5bd5639a16b15c0fb211afe6c233913912", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". . . . . . . . 34\nIn this section, we provide additional quantitative results: (1) GridPG results of additional explanation methods on ImageNet, and (2) EPG results for additional methods on COCO and VOC. In table E3 we provide quantitative results for ScoreCAM (Wang et al., 2020).\n- (F) Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\nIn this section, we provide additional details regarding datasets, training, implementation and evaluation procedures.\n# Under review as a conference paper at ICLR 2025\n\n# A DISCUSSION ON THE EVALUATION METRICS AND CO-12 PROPERTIES\n\nIn this section, we provide a discussion on the importance of the interpretability metrics we have selected in our evaluations, and also place them in context with the recently proposed Co-12 Properties by Nauta et al. (2023).\n\nNauta et al. (2023) posit that explainability is a multi-dimensional concept and propose various properties that describe the different aspects of explanation quality. Specifically, they introduce these properties as the Co-12 properties that are crucial to be evaluated for a comprehensive assessment of explanation methods. These 12 properties are namely the following:\n\n1. Correctness: denotes how faithful the explanation is with respect to the underlying \u2018black-box\u2019 model.\n2. Completeness: measures the extent to which the model is described by the explanation.\n3. Consistency: evaluates the degree of determinism and invariance of the explanation method.\n4. Continuity: measures continuity and generalizability of the explanation function.\n5. Contrastivity: measures the discriminativity of the explanation with respect to different targets.\n6. Covariate Complexity: assesses the complexity (human interpretable) of features in the explanation.\n7. Compactness: reports the overall size of the explanation.\n8. Composition: describes the presentation format and organization of the explanation.\n9. Confidence: a measure of confidence of the explanation or model output.\n10. Context: measures how relevant is the generated explanation to users.\n11. Coherence: evaluates the plausibility of the explanation.\n12. Controllability: measures the control or influence users have on the explanation.\n\nIn our work we present an important finding for explainable artificial intelligence (XAI), and conduct a systematic study across pre-training schemes and heatmap based attribution methods to evaluate to what extent the training influences the explanations derived from these attribution methods. To quantitatively evaluate the quality of the explanations we assess their ability to localize class-specific features using the grid pointing game (GPG) B\u00f6hle et al. (2021; 2022); Zhang et al. (2018); Samek et al. (2016) and the energy pointing game (EPG) Wang et al. (2020).\n\nThe grid pointing game (GPG) is an established metric that reflects various of the Co-12 properties (cf. Table 3 in (Nauta et al., 2023)). First, it constitutes a \u201ccontrolled synthetic data check\u201d which allows to (approximately) deduce ground truth explanations, thus reflecting the correctness of the explanations. Further, as multiple targets are present in the grid images, GPG reflects the target sensitivity of the explanations and thus their contrastivity. By highlighting relevant regions for the decision, explanations that score highly on GPG can be useful to end users (context).\n\nThe energy pointing game (EPG) can be seen as a general case of the pointing game (Hooker et al., 2019), and thus subsumes all the above properties of the GPG.\n\nWe also analyze the explanations under the pixel removal method, that has been shown to be a reliable measure of faithfulness (correctness) of an explanation (Samek et al., 2017; Hedstr\u00f6m et al., 2024).\n\nFinally, to have a comprehensive evaluation we also report explanation evaluations using the Gini index (as in (Chalasani et al., 2018)) to measure the compactness and the entropy (as in (Tseng et al., 2020)) to measure the complexity.", "mimetype": "text/plain", "start_char_idx": 2337, "end_char_idx": 6371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11958e16-6d14-4071-a88b-8ce34e958501": {"__data__": {"id_": "11958e16-6d14-4071-a88b-8ce34e958501", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99b6828c-1bde-47f2-8d06-18aa053bc100", "node_type": "4", "metadata": {}, "hash": "b83badc1b39ddc55aaa2a40efea5af6ef91478a7d649a4ebdabf5d9dfe68133d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a18f3843-a381-48ce-8c5a-1fb2d3577082", "node_type": "1", "metadata": {}, "hash": "c8cda78ebb581e7d4b618c62e4135791b2b6e75125347495b8b82e392477fcdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "680b3d2c-9116-44c0-9ae9-d61c58297611", "node_type": "1", "metadata": {}, "hash": "4e2276e949c0819d827d4faf0622b349ee8fde43ac2483c14132df0310c52888", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By highlighting relevant regions for the decision, explanations that score highly on GPG can be useful to end users (context).\n\nThe energy pointing game (EPG) can be seen as a general case of the pointing game (Hooker et al., 2019), and thus subsumes all the above properties of the GPG.\n\nWe also analyze the explanations under the pixel removal method, that has been shown to be a reliable measure of faithfulness (correctness) of an explanation (Samek et al., 2017; Hedstr\u00f6m et al., 2024).\n\nFinally, to have a comprehensive evaluation we also report explanation evaluations using the Gini index (as in (Chalasani et al., 2018)) to measure the compactness and the entropy (as in (Tseng et al., 2020)) to measure the complexity.\n\nImportantly, note that other Co-12 properties might not relate to heatmap-based explanations (composition, confidence, controllability) or remain unchanged by design as they are intrinsic to the explanation method itself, such as consistency, completeness, coherence and continuity.\n# Under review as a conference paper at ICLR 2025\n\n# LINEAR PROBING ON FROZEN BACKBONE FEATURES\n\nInterpretability of frozen backbone: Linear probing on the frozen pre-trained backbone features is an important step when using pre-trained models for downstream tasks and can also inform us about the interpretability of the backbone itself as we explain further. Since the linear probes themselves have limited modeling capacity, these explanations must therefore necessarily reflect the \u2018knowledge\u2019 of the backbone model. The different probes (CE / BCE), compute their outputs by nothing but a weighted mean of the frozen backbone features.\n\nIt can be demonstrated mathematically by the following: let z \u2208 RD represent the output feature vector from the frozen backbone after the global average pooling operation, where the vector has D dimensions. Let W \u2208 RC\u00d7D be the weight matrix of the fully connected classification layer, where C is the number of output classes (e.g., 1000 for ImageNet), and let b \u2208 RC be the bias term of the classification layer. The output prediction vector y \u2208 RC is then given by:\n\ny = Wz + b\n\nHere, Wz represents a linear combination of the backbone features z, with the weight matrix W. The bias term b is added to each class output.\n\nSince the feature extractor is frozen (i.e., its weights are not updated during training), the classification layer performs a simple linear combination of the extracted features, and only the weights W and bias b are learned during training. Thus, essentially when analyzing the explanations generated by the model, it is largely dominated by the backbone features. This is what makes our finding surprising, where BCE trained probes lend themselves to generate better quality explanations.\n# Under review as a conference paper at ICLR 2025\n\n# ADDITIONAL ARCHITECTURES: VISION TRANSFORMERS\n\nIn this section, we provide quantitative results on Vision Transformers (ViTs, Kolesnikov et al. (2021)) and the explanation (attribution) methods developed specifically for ViTs: (1) CGW1 (Chefer et al., 2020), (2) ViT-CX (CausalX (Xie et al., 2022)), (3) Rollout (Abnar & Zuidema, 2020), and (4) B-cos (B\u00f6hle et al., 2023). In particular, we first evaluate the impact of the training objective on probing in terms of accuracy and explanation quality using the metrics discussed in Section 3.1 on ImageNet. Next we analyze the effect of probe complexity on the explanation localization.\n\n# GridPG Scores for Vision Transformers with BCE vs CE Probing\n\n| |BCE|CE|\n|---|---|---|\n|Bcos: W(x) Input|+23%/|+33%/|\n|CausalX|+209/4|+1%|\n|CGW1|+5%|+4%|\n|Rollout|+16%|+12%|\n| |0.8|0.6|\n| |0.4|0.2|\n| |0.0| |\n\nFigure C1: ViT backbones BCE vs. CE Probing\u2014GridPG scores on ImageNet. GridPG for (1) B-cos (B\u00f6hle et al., 2023), (2) CausalX (Xie et al., 2022), (3) CGW1 (Chefer et al., 2020), and (4) Rollout (Abnar & Zuidema, 2020). BCE trained probes consistently outperform CE trained probes across all pre-training paradigms and explanation method.\n\n# C.1 IMPACT OF BCE VS.", "mimetype": "text/plain", "start_char_idx": 5643, "end_char_idx": 9675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "680b3d2c-9116-44c0-9ae9-d61c58297611": {"__data__": {"id_": "680b3d2c-9116-44c0-9ae9-d61c58297611", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99b6828c-1bde-47f2-8d06-18aa053bc100", "node_type": "4", "metadata": {}, "hash": "b83badc1b39ddc55aaa2a40efea5af6ef91478a7d649a4ebdabf5d9dfe68133d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11958e16-6d14-4071-a88b-8ce34e958501", "node_type": "1", "metadata": {}, "hash": "7812005deda0f1a3ae9e2b101928cc5bd5639a16b15c0fb211afe6c233913912", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CE Probing\u2014GridPG scores on ImageNet. GridPG for (1) B-cos (B\u00f6hle et al., 2023), (2) CausalX (Xie et al., 2022), (3) CGW1 (Chefer et al., 2020), and (4) Rollout (Abnar & Zuidema, 2020). BCE trained probes consistently outperform CE trained probes across all pre-training paradigms and explanation method.\n\n# C.1 IMPACT OF BCE VS. CE\n\nIn the following, we evaluate the impact of the training objective of the linear probes on ViTs similar to the evaluations for CNNs as described in Section 5.1 on both supervised (Kolesnikov et al., 2021) and self-supervised (He et al. (2019), Caron et al. (2021)) pre-trained backbones.\n\n# GridPG Localization\n\nIn Figure C1, we plot the GridPG scores for ViT backbones when probed with the BCE (orange) vs. CE (blue) training objective for both conventional and B-cos models. We find significant gains in the explanations\u2019 localization for all approaches, models, and explanations when using the BCE loss instead of the CE loss. E.g., for conventional backbones when explained via CGW1 (Figure C1, col. 3), the GridPG score for DINO improves by 12p.p (31%\u219243%), MoCov3 improves by 4p.p. (48%\u219252%), and for supervised by 16p.p. (34%\u219250%). For B-cos explanations, the increase in GridPG score is more drastic (Figure C1, col. 1): for DINO, it improves by up to 33p.p (47%\u219280%), and for supervised by 23p.p (61%\u219284%).\n\nIn Table C1, we also report the accuracies and when probing with different training objectives and observe that BCE probes achieve similar performance as CE probes while achieving significantly greater localization scores.\n\n# Table C1: ViT backbones BCE vs. CE Probing\u2014Accuracy and GridPG scores on ImageNet\n\nA consistent improvement in localization score for BCE over CE probes is seen for both conventional ViTs (Kolesnikov et al., 2021) and inherently interpretable B-cos B-ViTs (B\u00f6hle et al., 2023) for supervised and self-supervised pre-training of the backbones with comparable accuracies.\n\n| |Backbone|Pre-training|CE|BCE|\u2206bce\u2212ce|\u2206Bcosce| | | | |\u2206CausalX|\u2206CGW1|\u2206Rollout|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|ViT-B/16| |Sup.|73.2|72.8|\u2013|+0.7| |+15.7|+0.4| | | | |\n| |ViT-B/16|DINO|77.2|77.6|\u2013|+5.3| |+11.7|+4.8| | | | |\n| |ViT-B/16|MoCov3|76.3|75.1|\u2013|+3.8| |+4.4|+3.3| | | | |\n|B-ViTc-B/16| |Sup.|77.3|78.1|+23.5| |\u2013|\u2013|\u2013| | | | |\n| |B-ViTc-S/16|DINO|73.2|73.4|+32.7| |\u2013|\u2013|\u2013| | | | |\n|B-ViTc-B/16| |DINO|77.1|77.3|+20.4| |\u2013|\u2013|\u2013| | | | |\nUnder review as a conference paper at ICLR 2025\n\n# Table C2: ViT backbones BCE vs. CE Probing\u2014Accuracy and EPG scores on ImageNet.\n\nWe see improvement in localization score for BCE over CE probes for a majority of cases across Supervised and self-Supervised pre-training of the backbones and explanation methods with comparable accuracies. Note: We see a greater improvement in localization scores for smaller bounding boxes (with size 0 \u2212 50% of image area).", "mimetype": "text/plain", "start_char_idx": 9346, "end_char_idx": 12220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35df42b8-120a-4ff1-a1af-a8d31cb51229": {"__data__": {"id_": "35df42b8-120a-4ff1-a1af-a8d31cb51229", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d8bdb5b-9c77-46f5-af13-5aa0e0147749", "node_type": "4", "metadata": {}, "hash": "7ffd10070c7fb159b5feed98a9cefa13b023314fd9e24efb135599e4f9d41ca7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33538484-e919-4af5-927e-ff392b3cbc46", "node_type": "1", "metadata": {}, "hash": "de67167b67623324950f5b197bb1130c13a5b0c6a239718a9d5b8b8fed09b336", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CE Probing\u2014Accuracy and EPG scores on ImageNet.\n\nWe see improvement in localization score for BCE over CE probes for a majority of cases across Supervised and self-Supervised pre-training of the backbones and explanation methods with comparable accuracies. Note: We see a greater improvement in localization scores for smaller bounding boxes (with size 0 \u2212 50% of image area).\n\n| |Backbone|XAI Method|Pre-training|Loss|Acc.%|BBox size|BBox size| | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | |&lt;25%|&lt;50%|&lt;75%|&lt;100%| | | | | | | |\n| |ViT-B/16| | |CGW1|Sup|CE|73.2|26.4|42.8|53.7|71.0| | | | |\n| |ViT-B/16| | |CGW1|Sup|BCE|72.8|28.2(+1.8)|51.4| |44.5(+1.7)|61.5|55.2(+1.5)|77.5|72.1+(1.1)|\n|ViT-B/16| | | |CGW1|DINO|CE|77.2|40.2| | | | | | | |\n| |ViT-B/16| | |CGW1|DINO|BCE|77.6|41.7(+1.5)|50.5| |52.5(+1.1)|59.5|62.2(+0.7)|76.1|77.7(+0.2)|\n|ViT-B/16| | | |CGW1|MoCov3|CE|76.3|34.2| | | | | | | |\n| |ViT-B/16| | |CGW1|MoCov3|BCE|75.1|36.5(+2.3)|52.2(+1.7)|60.1(+0.6)|76.5(+0.4)| | | | |\n|ViT-B/16| | | |Rollout|Sup|CE|73.2|28.6|42.8|53.1|70.5| | | | |\n| |ViT-B/16| | |Rollout|Sup|BCE|72.8|30.1(+1.5)|53.3| |43.9(+1.1)|61.1|53.5(+0.4)|76.8|71.1(+0.6)|\n|ViT-B/16| | | |Rollout|DINO|CE|77.2|41.1| | | | | | | |\n| |ViT-B/16| | |Rollout|DINO|BCE|77.6|42.0(+0.9)|50.6| |52.6 (\u22120.7)|57.9|60.4(\u22120.7)|75.1|76.5(+0.3)|\n|ViT-B/16| | | |Rollout|MoCov3|CE|76.3|36.5| | | | | | | |\n| |ViT-B/16| | |Rollout|MoCov3|BCE|75.1|36.8(+0.3)|51.2(+0.6)|58.5(+0.5)|75.5(+0.4)| | | | |\n|ViT-B/16| | | |CausalX|Sup|CE|73.2|14.6|28.6|42.2|61.2| | | | |\n| |ViT-B/16| | |CausalX|Sup|BCE|72.8|14.6(0.0)|29.1(+0.5)|42.3| |41.6(\u22120.6)|62.2|61.4(+(0.2)| |\n|ViT-B/16| | | |CausalX|DINO|CE|77.2|14.5|28.4| | | | | | |\n| |ViT-B/16| | |CausalX|DINO|BCE|77.6|15.2(+0.7)|28.9| |29.3(+0.9)|43.3|43.1(+0.8)|63.2|63.1(+0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1833, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33538484-e919-4af5-927e-ff392b3cbc46": {"__data__": {"id_": "33538484-e919-4af5-927e-ff392b3cbc46", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d8bdb5b-9c77-46f5-af13-5aa0e0147749", "node_type": "4", "metadata": {}, "hash": "7ffd10070c7fb159b5feed98a9cefa13b023314fd9e24efb135599e4f9d41ca7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35df42b8-120a-4ff1-a1af-a8d31cb51229", "node_type": "1", "metadata": {}, "hash": "843a60aa7123b66f6182ce008435f6e23830a8a3e811121ec12dd899d9b0e008", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7a78683-ba39-495a-ad59-c6954f9200f1", "node_type": "1", "metadata": {}, "hash": "46b432681fc415ae50e7758026495b177d0d186db8ef1707e357870235604f4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2|14.6|28.6|42.2|61.2| | | | |\n| |ViT-B/16| | |CausalX|Sup|BCE|72.8|14.6(0.0)|29.1(+0.5)|42.3| |41.6(\u22120.6)|62.2|61.4(+(0.2)| |\n|ViT-B/16| | | |CausalX|DINO|CE|77.2|14.5|28.4| | | | | | |\n| |ViT-B/16| | |CausalX|DINO|BCE|77.6|15.2(+0.7)|28.9| |29.3(+0.9)|43.3|43.1(+0.8)|63.2|63.1(+0.9)|\n|ViT-B/16| | | |CausalX|MoCov3|CE|76.3|14.2| | | | | | | |\n| |ViT-B/16| | |CausalX|MoCov3|BCE|75.1|15.4(+1.2)|30.1(+1.1)|44.2(+0.9)|63.3(+0.1)| | | | |\n\n# Table C3: ViT backbones MLP Probing\u2014Accuracy and EPG scores on ImageNet.\n\nA consistent improvement in accuracy \u2206acc B-cos MLPs over a linear probe for DINO pre-trained small and using more complex 2 or 3 layer and localization score \u2206loc is seen for B-cos explanations when base ViTcs. This demonstrates that more powerful classifier heads are able distill \u2018class-specific\u2019 information better.\n\n|Backbone|Classifier|Acc.%|\u2206acc|Loc.%|\u2206loc|\n|---|---|---|---|---|---|\n|B-ViTc-S/16|Linear Probe|73.4|\u2013|79.9|\u2013|\n|B-ViTc-S/16|Bcos-MLP-2|74.2|+0.8|80.4|+0.5|\n|B-ViTc-S/16|Bcos-MLP-3|74.7|+1.3|82.4|+2.5|\n|B-ViTc-B/16|Bcos-MLP-2|\u2013|\u2013|\u2013|\u2013|\n|B-ViTc-B/16|Bcos-MLP-3|78.2|+0.9|82.1|+1.8|\n|B-ViTc| |79.7|+2.4|83.4|+3.1|\n\nEPG Localization. In Table C2 and Figure C2, similar improvements are seen for the EPG metric (on ImageNet) when using BCE probes for CGW1 (Chefer et al., 2020), Rollout (Abnar & Zuidema, 2020) and CausalX (Xie et al., 2022) attributions. We specifically see a greater improvement on smaller bounding boxes thus highlighting the ability for improved localization as compared to CE probes.\n\n# C.2 IMPACT OF COMPLEX PROBES\n\nIn Table C3, we show the results for probing DINO B-ViTc-S (small) and B-ViTc-B (base) with B-cos MLPs. It is seen that for these models there is an increase in both accuracy \u2206acc as well as the localization ability \u2206loc (GridPG scores) of the B-cos explanations. This does seem to suggest that using more complex probes helps distill \u2018class-specific\u2019 information from the frozen backbone features thus leading to improved localization scores even for transformer based architectures. These results are consistent with the improvements seen for convolutional backbones (see Section 5) and demonstrate the robustness of our findings across different model architectures (vision transformers and convolutional networks).", "mimetype": "text/plain", "start_char_idx": 1550, "end_char_idx": 3838, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7a78683-ba39-495a-ad59-c6954f9200f1": {"__data__": {"id_": "e7a78683-ba39-495a-ad59-c6954f9200f1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d8bdb5b-9c77-46f5-af13-5aa0e0147749", "node_type": "4", "metadata": {}, "hash": "7ffd10070c7fb159b5feed98a9cefa13b023314fd9e24efb135599e4f9d41ca7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33538484-e919-4af5-927e-ff392b3cbc46", "node_type": "1", "metadata": {}, "hash": "de67167b67623324950f5b197bb1130c13a5b0c6a239718a9d5b8b8fed09b336", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "035ffeab-be1d-4d46-8d91-e280da11eec4", "node_type": "1", "metadata": {}, "hash": "d4724643e83f2512afe1546d2bb64c7456042282162e711d9e841437d5c71288", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We specifically see a greater improvement on smaller bounding boxes thus highlighting the ability for improved localization as compared to CE probes.\n\n# C.2 IMPACT OF COMPLEX PROBES\n\nIn Table C3, we show the results for probing DINO B-ViTc-S (small) and B-ViTc-B (base) with B-cos MLPs. It is seen that for these models there is an increase in both accuracy \u2206acc as well as the localization ability \u2206loc (GridPG scores) of the B-cos explanations. This does seem to suggest that using more complex probes helps distill \u2018class-specific\u2019 information from the frozen backbone features thus leading to improved localization scores even for transformer based architectures. These results are consistent with the improvements seen for convolutional backbones (see Section 5) and demonstrate the robustness of our findings across different model architectures (vision transformers and convolutional networks).\n# EPG Scores for Vision Transformers with BCE vs CE Probing\n\n| | | | | | | | | | | | | | |CGWI Explanations|BCE|CE|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|SUP Vit-B/16|1.0| | | | | | | | |0%/|+1%/| | | | | |\n| |0.8| | | | | | | |+1%/|+1%/| | | | | | |\n|1|0.6| | |+2%| | |+1%/| | | | | | | | | |\n| | | |+2%/|+2%/| | | | | | | | | | | | |\n| |8|0.4|+2%/| | | | | | | | | | | | | |\n| |0.2| | | | | | | | | | | | | | | |\n| |0.0| | | | | | | | | | | | | | | |\n| |&lt;25%|&lt;50%|&lt;75%|&lt;100%| | | | | | | | | | | | |\n| | | |SUP VitT-B/16| | | | |DINO ViT-B/16| | | | |MOCOV3 Vit-B/16| | | |\n| |1.0| | | |+1%/| | | | |0%/0| | | | | | |\n| |0.8| | | | | | | |-1%/| | | | | | | |\n|1|0.6| | | | | |+1%/|+1%/| | | | | | | | |\n| |8|0.4|+2%| | | | | | | | | | | | | |\n| |0.2|0%/| | | |+1%/| | | | | | | | | | |\n| |0.0|&lt;25%|&lt;50%|&lt;75%|&lt;100%| | | | | | | | | | | |\n| | | | |BBOX %|BBOX %|BBOX %| | | | | | | | | | |\n| | | |SUP Vit-B/16| | | | |DINO ViT-B/16| | | | |MOCOV3 Vit-B/16| | | |\n| |1.0| |+1%/| | | | | | | | | | | | | |\n| |0.8| | | | | | | | | | | | |0%/| | |\n|1|0.6| | |-1%/| | | | |+1%/| | | | | | | |\n| |8|0.4|+1%/|+1%/| | | | | | | | | | | | |\n| |0.2|0%/| | | |+1%/| | | | | | | | | | |\n| |0.0|&lt;25%|&lt;50%|&lt;75%|&lt;100%| | | | | | | | | | | |\n\nFigure C2: ViT backbones BCE vs. CE Probing\u2014EPG scores on ImageNet. EPG for (1) CGW1 (Chefer et al., 2020), (2) Rollout (Abnar & Zuidema, 2020), and (3) CausalX (Xie et al., 2022).", "mimetype": "text/plain", "start_char_idx": 2937, "end_char_idx": 5315, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "035ffeab-be1d-4d46-8d91-e280da11eec4": {"__data__": {"id_": "035ffeab-be1d-4d46-8d91-e280da11eec4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d8bdb5b-9c77-46f5-af13-5aa0e0147749", "node_type": "4", "metadata": {}, "hash": "7ffd10070c7fb159b5feed98a9cefa13b023314fd9e24efb135599e4f9d41ca7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7a78683-ba39-495a-ad59-c6954f9200f1", "node_type": "1", "metadata": {}, "hash": "46b432681fc415ae50e7758026495b177d0d186db8ef1707e357870235604f4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34e2e113-2421-4f1c-ae2a-cf5cd5773d08", "node_type": "1", "metadata": {}, "hash": "b95199c196b028de36e8984013f1fb7098127c2dc5bb6e7620e582bf5bf5e9d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CE Probing\u2014EPG scores on ImageNet. EPG for (1) CGW1 (Chefer et al., 2020), (2) Rollout (Abnar & Zuidema, 2020), and (3) CausalX (Xie et al., 2022). BCE trained probes outperform CE trained probes for a majority of cases; with larger and more consistent gains for smaller bounding boxes (occupying 0 \u2212 50% of the image area).\n# Under review as a conference paper at ICLR 2025\n\n# D\n\n# ADDITIONAL QUALITATIVE RESULTS\n\nIn this section we show additional qualitative results for the explanations of both conventional as well as B-cos models under the different evaluation settings studied in the main paper. We first qualitatively show the impact of training the probes with binary cross entropy (BCE) vs. cross entropy (CE) loss in Sec. D.1. Then we show the effect of more complex probes on localization in Sec. D.2. Additionally, in Sec. D.3 we also provide qualitative comparisons between different explanation methods for all SSL models as well as fully supervised models. Finally, in Sec. D.4 we add more qualitative results showing a diverse set of samples for all self-supervised pre-trainings.\n\n# D.1 IMPACT OF BCE VS. CE\n\nThe probing strategy can have a significant influence on the localization ability of model explanations. As described in Sec. 5.2 of the main paper, probes trained with BCE loss localize more strongly as compared to probes trained with CE loss. Figures D1, D2, D3 (cf. Figure 6 in main paper), show the B-cos (B\u00f6hle et al., 2022) explanations in the 2 \u00d7 2 GridPG evaluation setting for DINO, BYOL and MoCov2 respectively when probed with BCE or CE loss. It can be observed that for all SSL-frameworks as well as supervised training, the explanations for linear probes trained with BCE loss (cols. 3+5) are much more localized as compared to probing with CE loss. Interestingly, however, while the focus region seems to be very similar between the SSL-trained and the fully supervised models, the B-cos explanations of BYOL and MoCov2 show lower saturation.\n\n|DINO|Input|Input|Supervised|Supervised| | | | |\n|---|---|---|---|---|---|---|\n| | | |CE|BCE|CE|BCE|\n|1|FHEEDOREALEERIDGE DWARD KENNEDY ELLINGTO COMPOSER Gooum|FHEEDOREALEERIDGE DWARD KENNEDY ELLINGTO COMPOSER Gooum|FHEEDOREALEERIDGE DWARD KENNEDY ELLINGTO COMPOSER Gooum|FHEEDOREALEERIDGE DWARD KENNEDY ELLINGTO COMPOSER Gooum| | | | | |\n\nFigure D1: DINO vs. Supervised Explanations. The B-cos explanations for DINO (cols. 2+3) are visually very similar to supervised models (cols. 4+5), despite being optimized very differently. We also see that for B-cos models the improvements in attribution localization for probes trained with BCE loss are consistent for both DINO and supervised models.\nUnder review as a conference paper at ICLR 2025\n\n| | |BYOL| | |Supervised|\n|---|---|---|---|---|---|\n| |Input|CE|BCE|CE|BCE|\n| |FHE EDOREALEEKIDGEA| | | | |\n| |DWARD KENNEDY ELLINGTA| | | | |\n| | | | | |31899|\n| |Goopi| | | | |\n\nFigure D2: BYOL vs. Supervised Explanations. Similar to Figure D1 the B-cos explanations for models trained via BYOL (cols. 2+3) exhibit significant improvements in localization when trained via BCE. Interestingly, compared to the explanations of supervised models (cols. 4+5), we find BYOL explanations to exhibit less saturation.\n\n|MoCov2|MoCov2|Supervised|Supervised| | | | |\n|---|---|---|---|---|---|\n| |Input|CE|BCE|CE|BCE|\n| |HEERORIALEERIDGEA| | | | |\n| |DWARD KENNEDY ELLINGTA| | | | |\n| |COMPOSER| | | | |\n\nFigure D3: MoCov2 vs. Supervised Explanations.", "mimetype": "text/plain", "start_char_idx": 5168, "end_char_idx": 8639, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34e2e113-2421-4f1c-ae2a-cf5cd5773d08": {"__data__": {"id_": "34e2e113-2421-4f1c-ae2a-cf5cd5773d08", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d8bdb5b-9c77-46f5-af13-5aa0e0147749", "node_type": "4", "metadata": {}, "hash": "7ffd10070c7fb159b5feed98a9cefa13b023314fd9e24efb135599e4f9d41ca7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "035ffeab-be1d-4d46-8d91-e280da11eec4", "node_type": "1", "metadata": {}, "hash": "d4724643e83f2512afe1546d2bb64c7456042282162e711d9e841437d5c71288", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Supervised Explanations. Similar to Figure D1 the B-cos explanations for models trained via BYOL (cols. 2+3) exhibit significant improvements in localization when trained via BCE. Interestingly, compared to the explanations of supervised models (cols. 4+5), we find BYOL explanations to exhibit less saturation.\n\n|MoCov2|MoCov2|Supervised|Supervised| | | | |\n|---|---|---|---|---|---|\n| |Input|CE|BCE|CE|BCE|\n| |HEERORIALEERIDGEA| | | | |\n| |DWARD KENNEDY ELLINGTA| | | | |\n| |COMPOSER| | | | |\n\nFigure D3: MoCov2 vs. Supervised Explanations. Similar to Figure D1, D2 the B-cos explanations for models trained via MoCov2 (cols. 2+3) exhibit significant improvements in localization when trained via BCE. Interestingly, compared to the explanations of supervised models (cols. 4+5), we find MoCov2 explanations to exhibit less saturation.\nUnder review as a conference paper at ICLR 2025\n\nIn Figure D4, we additionally visualize the LRP (Bach et al., 2015) explanations for conventional models. Similar to the B-cos models in the preceding figures, we find the explanations for BCE-trained models to exhibit significantly higher localization. Interestingly, despite the consistent improvements in localization, we again observe clear qualitative differences between the differently trained backbones: e.g., the explanations for the DINO model seem to cover the full object, whereas the explanations for the models trained via BYOL and MoCov2 are much sparser. See also Figure D5 for LRP visualizations for the CLIP (Radford et al., 2021) (vision-language) model.\n\n|Model|DINO|BYOL|MoCov2| | | | |\n|---|---|---|---|---|---|---|---|\n| | |CE|BCE|CE|BCE|CE|BCE|\n| |6|3|6|1|8| | |\n\nFigure D4: Layer-wise Relevance Propagation Explanations. Even for conventional models we see a consistent improvement in attribution localization for probes trained with BCE loss. This is seen across all SSL methods (DINO, BYOL and MoCov2). We also observe significant qualitative differences between differently trained backbones: e.g., the explanations for DINO model (left) cover the full object, as compared to explanations for models trained via BYOL (center) or MoCov2 (right), that are much sparser.\nUnder review as a conference paper at ICLR 2025\n\n|CLIP|BCE|CE|BCE|CE|\n|---|---|---|---|---|\n|2|2g|8|1| |\n|1| |J|3| |\n\nFigure D5: Layer-wise Relevance Propagation Explanations for CLIP. Similar to SSL and supervised pre-trained models, for CLIP (Radford et al., 2021) which is a vision-language pre-trained model we see BCE-trained probes (cols. 2+5) to localize better as compared to CE-trained (cols. 3+6) probes.\n# D.2 EFFECT OF COMPLEX PROBES\n\nFigure D6 shows visual results depicting the impact of training with MLP probes. Notice, e.g., in cols. 3+4, two- and three-layer MLPs better localize the object class of interest with minimal reliance on background features for DINO model.", "mimetype": "text/plain", "start_char_idx": 8097, "end_char_idx": 10967, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b488bbe-e17e-4cb6-b6ad-fa33b2c20916": {"__data__": {"id_": "9b488bbe-e17e-4cb6-b6ad-fa33b2c20916", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510", "node_type": "4", "metadata": {}, "hash": "1f976bf51c039fb41eaa8ccc16adeb8ab9a439bf03b50b0e096f105f376cd900", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8952b2b-4672-4cdb-931d-ab5c4bce6da3", "node_type": "1", "metadata": {}, "hash": "01bc81cd6a528e788a60236088829cb9ac0c6f5c28c6d77b08648341cd0ec8be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similar to SSL and supervised pre-trained models, for CLIP (Radford et al., 2021) which is a vision-language pre-trained model we see BCE-trained probes (cols. 2+5) to localize better as compared to CE-trained (cols. 3+6) probes.\n# D.2 EFFECT OF COMPLEX PROBES\n\nFigure D6 shows visual results depicting the impact of training with MLP probes. Notice, e.g., in cols. 3+4, two- and three-layer MLPs better localize the object class of interest with minimal reliance on background features for DINO model. Since the features learned by SSL models may not always be linearly separable with respect to the classes present in different downstream tasks, stronger MLP classifiers are able to distill class-specific features better, leading to an improvement in localization scores and also improved performance on the downstream task. This behaviour is consistent across all SSL frameworks when we go from single probes to MLP probes. As seen previously, the B-cos explanations for BYOL and MoCov2 show lower saturation as compared to DINO.\n\n|DINO|Single Probe|2-layer MLP|3-layer MLP|\n|---|---|---|---|\n|Input|0|]|2|\n|BYOL|Single Probe|2-layer MLP|3-layer MLP|\n|Input|0|]|2|\n|MoCov2|Single Probe|2-layer MLP|3-layer MLP|\n|Input|0|]|2|\n\nFigure D6: Qualitative results of MLP probes on ImageNet. We find explanations for B-cos MLPs (cols. 3+4+6+7+9+10) trained on SSL pre-trained features to exhibit better localization than a single linear probe (cols. 2+5+8). This behavior is seen more prominently for DINO (left) as compared to BYOL (center) and MoCov2 (right).\n\n# D.3 ADDITIONAL EXPLANATION METHODS\n\nComparison between explanation methods. Figures D7, D8, D9 show additional comparisons between different attribution methods for each SSL model (both B-cos and conventional) and how it compares to their fully-supervised counterparts; in particular we show results for B-cos (B\u00f6hle et al., 2022), Layer-wise Relevance Propagation (LRP) (Bach et al., 2015), GradCAM (Selvaraju et al., 2017), Integrated Gradients (IntGrad) (Sundararajan et al., 2017), and Input\u00d7Gradient (I\u00d7G) (Shrikumar et al., 2017). Notice, every pair of consecutive rows illustrates this for a single SSL method and supervised training. We observe that visually the explanations for SSL models and supervised models are quite similar. Interestingly, this is more consistent for B-cos models as compared to conventional models. Note: For all SSL models, these are explanations for the setting when the frozen backbone features are trained using only a single linear probe with BCE loss.\n# Under review as a conference paper at ICLR 2025\n\n# Comparison DINQ vs Supervised Models\n\n|Input|B-cos|GradCAM|IntGrad|IxG|LRP|GradCAM|IntGrad|IxG|\n|---|---|---|---|---|---|---|---|---|\n|J| | | | | | | | |\n| |8| | | | | | | |\n| |8| | | | | | | |\n|L| | | | | | | | |\n| |2| | | | | | | |\n| |2| | | | | | | |\n\n# (a) B-cos Models\n\n# (b) Conventional Models\n\nFigure D7: Qualitative comparison of different explanation methods for DINO and supervised training for (a) B-cos and (b) conventional models. Notice that B-cos explanations are able to highlight the object of interest quite well. Also LRP explanations for conventional models seem to localize well to object features however are quite sparse. GradCAM explanations although do focus on the object but are more spread out for DINO model as compared to supervised model. IntGrad and I\u00d7G explanations are scattered across the entire image and also highlight background regions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3481, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8952b2b-4672-4cdb-931d-ab5c4bce6da3": {"__data__": {"id_": "c8952b2b-4672-4cdb-931d-ab5c4bce6da3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510", "node_type": "4", "metadata": {}, "hash": "1f976bf51c039fb41eaa8ccc16adeb8ab9a439bf03b50b0e096f105f376cd900", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b488bbe-e17e-4cb6-b6ad-fa33b2c20916", "node_type": "1", "metadata": {}, "hash": "e20f196516f3ea3287b6110e4ecb7b980c0b7a18c9e0585d42ae0a53cf63b249", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "599dddd6-22be-497e-b7e4-dc5cccb4ab9a", "node_type": "1", "metadata": {}, "hash": "14175f56078f348ff289524dffb4569dad264fb723abd171c804e800530f7460", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Notice that B-cos explanations are able to highlight the object of interest quite well. Also LRP explanations for conventional models seem to localize well to object features however are quite sparse. GradCAM explanations although do focus on the object but are more spread out for DINO model as compared to supervised model. IntGrad and I\u00d7G explanations are scattered across the entire image and also highlight background regions.\nUnder review as a conference paper at ICLR 2025\n\n# Comparison BYOL_vs_Supervised Models\n\n|Input|B-cos|GradCAM|IntGrad|IxG|LRP|GradCAM|IntGrad|IxG|\n|---|---|---|---|---|---|---|---|---|\n| |2| | | | | | | |\n|J| |2| | | | | | |\n| |6| | | | | | | |\n| |2| | | | | | | |\n|8|L| | | | | | | |\n| |2| | | | | | | |\n| |2| | | | | | | |\n|2| | | | | | | | |\n\n# Figure D8\n\nQualitative comparison of different explanation methods for BYOL and supervised training for (a) B-cos and (b) conventional models. Similar to Figure D7, we observe that B-cos explanations are able to highlight the object of interest quite well. The LRP explanations for conventional models seem to localize well to object features however are quite sparse. GradCAM explanations although do focus on the object but are more spread out for BYOL model as compared to supervised model (especially for conventional models). IntGrad and I\u00d7G explanations are scattered across the entire image and also highlight background regions.\nUnder review as a conference paper at ICLR 2025\n\n# Comparison MoCov2 vs Supervised Models\n\n|Input|B-cos|GradCAM|IntGrad|IxG|LRP|GradCAM|IntGrad|IxG|\n|---|---|---|---|---|---|---|---|---|\n| |3| | | | |8| |6|\n| |3|8|L| | |3|2|2|\n\n# (a) B-cos Models\n\n# (b) Conventional Models\n\nFigure D9: Qualitative comparison of different explanation methods for MoCov2 and supervised training for (a) B-cos and (b) conventional models. Similar to Figures D7 (DINO), D8 (BYOL) we observe that B-cos explanations for MoCov2 are able to highlight the object of interest quite well. The LRP explanations for conventional models seem to localize well to object features however are quite sparse. GradCAM explanations although do focus on the object but are more spread out for MoCov2 model as compared to supervised model (especially for conventional models). IntGrad and I\u00d7G explanations are scattered across the entire image and also highlight background regions.\n# Under review as a conference paper at ICLR 2025\n\n# D.4 ADDITIONAL QUALITATIVE RESULTS\n\nIn this sub-section we add more qualitative results for B-cos, ScoreCAM, and GradCAM explanations to demonstrate the impact of the training objective and probe complexity on model explanations.\n\n# BCE vs.", "mimetype": "text/plain", "start_char_idx": 3050, "end_char_idx": 5705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "599dddd6-22be-497e-b7e4-dc5cccb4ab9a": {"__data__": {"id_": "599dddd6-22be-497e-b7e4-dc5cccb4ab9a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510", "node_type": "4", "metadata": {}, "hash": "1f976bf51c039fb41eaa8ccc16adeb8ab9a439bf03b50b0e096f105f376cd900", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8952b2b-4672-4cdb-931d-ab5c4bce6da3", "node_type": "1", "metadata": {}, "hash": "01bc81cd6a528e788a60236088829cb9ac0c6f5c28c6d77b08648341cd0ec8be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b3bbd18-3411-4a78-b092-bd5ec3a6b762", "node_type": "1", "metadata": {}, "hash": "7199c0716960b71681428ca2702de1c8b80343c502ac3d10616d5c39d431bc3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similar to Figures D7 (DINO), D8 (BYOL) we observe that B-cos explanations for MoCov2 are able to highlight the object of interest quite well. The LRP explanations for conventional models seem to localize well to object features however are quite sparse. GradCAM explanations although do focus on the object but are more spread out for MoCov2 model as compared to supervised model (especially for conventional models). IntGrad and I\u00d7G explanations are scattered across the entire image and also highlight background regions.\n# Under review as a conference paper at ICLR 2025\n\n# D.4 ADDITIONAL QUALITATIVE RESULTS\n\nIn this sub-section we add more qualitative results for B-cos, ScoreCAM, and GradCAM explanations to demonstrate the impact of the training objective and probe complexity on model explanations.\n\n# BCE vs. CE Probing: Samples with Top 10% Localization Scores\n\n|Class: 'black wdow|CE, Loc: 50.%|BCE, Loc: 97.%|Class: 'cliff dwelling|CE, Loc: 48.%|BCE, Loc: 95.%|Class: 'brain coral'|CE, Loc: 42.%|BCE, Loc: 91.%|\n|---|---|---|---|---|---|---|---|---|\n|Class: 'organ|CE, Loc: 44.%|BCE, Loc: 93.%|Class: 'lakeside|CE, Loc: 38.%|BCE, Loc: 87.%|Class: 'dugong|CE, Loc: 45.%|BCE, Loc: 96.%|\n|Class: 'black wdow|CE, Loc: 53.%|BCE, Loc: 96.%|Class: 'spider web|CE, Loc: 54.%|BCE, Loc: 96.%|Class: 'hourglass|CE, Loc: 48.%|BCE, Loc: 91.%|\n|Class: 'picket fence|CE, Loc: 50.%|BCE, Loc: 95.%|Class: 'dugong|CE, Loc: 46.%|BCE, Loc: 93.%|Class: 'organ|CE, Loc: 44.%|BCE, Loc: 93.%|\n|Class: 'picket fence|CE, Loc: 48.%|BCE, Loc: 96.%|Class: 'dugong|CE, Loc: 42.%|BCE, Loc: 91.%|Class: 'monarch|CE, Loc: 45.%|BCE, Loc: 94.%|\n|Class: 'picket fence|CE, Loc: 45.%|BCE, Loc: 95.%|Class: 'pizza|CE, Loc: 43.%|BCE, Loc: 93.%|Class: 'bucket|CE, Loc: 35.%|BCE, Loc: 88.%|\n\n# BCE vs.", "mimetype": "text/plain", "start_char_idx": 4887, "end_char_idx": 6659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b3bbd18-3411-4a78-b092-bd5ec3a6b762": {"__data__": {"id_": "7b3bbd18-3411-4a78-b092-bd5ec3a6b762", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510", "node_type": "4", "metadata": {}, "hash": "1f976bf51c039fb41eaa8ccc16adeb8ab9a439bf03b50b0e096f105f376cd900", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "599dddd6-22be-497e-b7e4-dc5cccb4ab9a", "node_type": "1", "metadata": {}, "hash": "14175f56078f348ff289524dffb4569dad264fb723abd171c804e800530f7460", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "002f2372-cf27-49ca-8e77-3a02d34c7da0", "node_type": "1", "metadata": {}, "hash": "8a90ed123879155be9c4935fa01756f28533015f37fc08b4e7cea3f71b1ad9d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CE Probing: Randomly Sampled\n\n|Class: 'potpie'|CE, Loc: 70.%|BCE, Loc: 93.%|Class: 'butcher shop'|CE, Loc: 63.%|BCE, Loc: 91.%|Class: 'sea cucumber'|CE, Loc: 84.%|BCE, Loc: 95.%|\n|---|---|---|---|---|---|---|---|---|\n|Class: 'police van'|CE, Loc: 81.%|BCE, Loc: 92.%|Class: 'beach wagon'|CE, Loc: 71.%|BCE, Loc: 88.%|Class: 'spider web'|CE, Loc: 72.%|BCE, Loc: 87.%|\n|Class: 'mashed potato'|CE, Loc: 56.%|BCE, Loc: 70.%|Class: 'Ice lolly'|CE, Loc: 70.%|BCE, Loc: 85.%|Class: 'binoculars'|CE, Loc: 78.%|BCE, Loc: 93.%|\n|Class: 'punching bag'|CE, Loc: 75.%|BCE, Loc: 70.%|Class: 'sea cucumber'|CE, Loc: 68.%|BCE, Loc: 87.%|Class: 'pop bottle'|CE, Loc: 68.%|BCE, Loc: 83.%|\n|Class: 'sea cucumber'|CE, Loc: 76.%|BCE, Loc: 84.%|Class: 'barrel'|CE, Loc: 54.%|BCE, Loc: 84.%|Class: 'barbershop'|CE, Loc: 38.%|BCE, Loc: 52.%|\n|Class: 'European fire salamander'|CE, Loc: 72.%|BCE, Loc: 87.%|Class: 'butcher shop'|CE, Loc: 50.%|BCE, Loc: 65.%|Class: 'tarantula'|CE, Loc: 70.%|BCE, Loc: 86.%|\n\n# Figure D10\n\nQualitative results for BCE vs. CE probing on ImageNet. (a) Shows six examples each for DINO, BYOL, and MoCov2 sampled from the set of top 10% of images to show greatest improvement of BCE probing over CE probing (\u2206bce\u2212ce GridPG scores) when explained with B-cos explanations. (b) Additionally, shows a set six images randomly sampled for each SSL method; out of 18 samples we find only 1 sample where the BCE probe localizes worse than the CE probe (highlighted with a red box). Please zoom-in to notice the finer differences in the visual explanations.\nUnder review as a conference paper at ICLR 2025\n\n# BCE vs.", "mimetype": "text/plain", "start_char_idx": 6660, "end_char_idx": 8270, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "002f2372-cf27-49ca-8e77-3a02d34c7da0": {"__data__": {"id_": "002f2372-cf27-49ca-8e77-3a02d34c7da0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510", "node_type": "4", "metadata": {}, "hash": "1f976bf51c039fb41eaa8ccc16adeb8ab9a439bf03b50b0e096f105f376cd900", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b3bbd18-3411-4a78-b092-bd5ec3a6b762", "node_type": "1", "metadata": {}, "hash": "7199c0716960b71681428ca2702de1c8b80343c502ac3d10616d5c39d431bc3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5ef7850-936f-4cd0-af9a-b59bc03aeb4e", "node_type": "1", "metadata": {}, "hash": "6080c6eb8bc9601f430c23e601adfe7e1c2528ce97b20212ed2f4210a6ae6251", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CE probing on ImageNet. (a) Shows six examples each for DINO, BYOL, and MoCov2 sampled from the set of top 10% of images to show greatest improvement of BCE probing over CE probing (\u2206bce\u2212ce GridPG scores) when explained with B-cos explanations. (b) Additionally, shows a set six images randomly sampled for each SSL method; out of 18 samples we find only 1 sample where the BCE probe localizes worse than the CE probe (highlighted with a red box). Please zoom-in to notice the finer differences in the visual explanations.\nUnder review as a conference paper at ICLR 2025\n\n# BCE vs. CE Probing: Randomly Sampled Examples for ScoreCAM\n\n|Class: sea slug|CE, Loc: 88.%|BCE, Loc: 94.%|Class: ladybug|CF, Loc: 5.7%|BCE, Loc: 63.%|Class: American lobster|CE, Loc: 76.%|BCE, Loc: 85.%|\n|---|---|---|---|---|---|---|---|---|\n|Class: American lobster|CE, Loc: 36.%|BCE, Loc: 88.%|Class: German shepherd|CE, Loc: 61.%|BCE, Loc: 84.%|Class: beach wagon|CE, Loc: 57.%|BCE, Loc: 78.%|\n|Class: pomegranate|CE, Loc: 17.%|BCE, Loc: 51.%|Class: barrel|CE, Loc: 16.%|BCE, Loc: 29.%|Class: coral reef|CE, Loc: 0.1%|BCE, Loc: 22.%|\n|Class: Ice lolly|CE, Loc: 7.3%|BCE, Loc: 35.%|Class: ladybug|CE, Loc: 71.%|BCE, Loc: 39.%|Class: trilobite|CE, Loc: 44.%|BCE, Loc: 53.%|\n|Class: cardigan|CE, Loc: 43.%|BCE, Loc: 57.%|Class: steel arch bridge|CE, Loc: 20.%|BCE, Loc: 36.%|Class: Pod|CE, Loc: 0.0%|BCE, Loc: 12.%|\n|Class: turnstile|CE, Loc: 7.8%|BCE, Loc: 57.%|Class: birdhouse|CE, Loc: 32.%|BCE, Loc: 31.%|Class: barrel|CE, Loc: 19.%|BCE, Loc: 18.%|\n\nFigure D11: Qualitative results for BCE vs. CE probing on ImageNet for ScoreCAM. The figure shows a set of six examples sampled randomly for each SSL method, i.e. DINO, BYOL and MOCO when explained using ScoreCAM explanations. Overall BCE probes lead to more localized explanations over CE probes. We highlight examples where BCE probes perform worse than CE probes with a red box. Please zoom-in to notice the finer differences in the visual explanations.\n\n# BCE vs.", "mimetype": "text/plain", "start_char_idx": 7689, "end_char_idx": 9684, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e5ef7850-936f-4cd0-af9a-b59bc03aeb4e": {"__data__": {"id_": "e5ef7850-936f-4cd0-af9a-b59bc03aeb4e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510", "node_type": "4", "metadata": {}, "hash": "1f976bf51c039fb41eaa8ccc16adeb8ab9a439bf03b50b0e096f105f376cd900", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "002f2372-cf27-49ca-8e77-3a02d34c7da0", "node_type": "1", "metadata": {}, "hash": "8a90ed123879155be9c4935fa01756f28533015f37fc08b4e7cea3f71b1ad9d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CE probing on ImageNet for ScoreCAM. The figure shows a set of six examples sampled randomly for each SSL method, i.e. DINO, BYOL and MOCO when explained using ScoreCAM explanations. Overall BCE probes lead to more localized explanations over CE probes. We highlight examples where BCE probes perform worse than CE probes with a red box. Please zoom-in to notice the finer differences in the visual explanations.\n\n# BCE vs. CE Probing: Randomly Sampled Examples for GradCAM\n\n|Class: potter's wheel|CE, Loc: 67.%|BCE, Loc: 100%|Class: European fire salamander|CE, Loc: 57.%|BCE, Loc: 0.8%|Class: hourglass|CF, Loc: 1.4%|BCE, Loc: 100%|\n|---|---|---|---|---|---|---|---|---|\n|Class: mushroom|CE, Loc: 3.8%|BCE, Loc: 0.0%|Class: lawn mower|CE, Loc: 2.7%|BCE, Loc: 74.%|Class: obelisk|CE, Loc: 4.3%|BCE, Loc: 0.0%|\n|Class: Persian cat|CE, Loc: 49.%|BCE, Loc: 73.%|Class: sandal|CE, Loc: 41.%|BCE, Loc: 55.%|Class: sea cucumber|CE, Loc: 52.%|BCE, Loc: 83.%|\n|Class: sandal|CE, Loc: 39.%|BCE, Loc: 74.%|Class: black widow|CE, Loc: 72.%|BCE, Loc: 99.%|Class: bucket|CE, Loc: 42.%|BCE, Loc: 98.%|\n|Class: hog|CE, Loc: 88.%|BCE, Loc: 93.%|Class: umbrella|CE, Loc: 9.0%|BCE, Loc: 23.%|Class: abacus|CE, Loc: 67.%|BCE, Loc: 84.%|\n|Class: IO-kart|CE, Loc: 59.%|BCE, Loc: 76.%|Class: water tower|CE, Loc: 90.%|BCE, Loc: 98.%|Class: black stork|CE, Loc: 84.%|BCE, Loc: 46.%|\n\nFigure D12: Qualitative results for BCE vs. CE probing on ImageNet for GradCAM. The figure shows a set of six examples sampled randomly for each SSL method, i.e. DINO, BYOL and MOCO when explained using GradCAM explanations. Overall BCE probes lead to more localized explanations over CE probes. We highlight examples where BCE probes perform worse than CE probes with a red box. Please zoom-in to notice the finer differences in the visual explanations.", "mimetype": "text/plain", "start_char_idx": 9261, "end_char_idx": 11077, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83be867f-1739-4404-949a-62f15c183419": {"__data__": {"id_": "83be867f-1739-4404-949a-62f15c183419", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "75f91c95-e67a-4a4f-836f-fe6f8a4e8f61", "node_type": "4", "metadata": {}, "hash": "8334c081061f122a58feab1bd82a81a210d358bf9fd63059061958bf53ad5a6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8aff54e3-1eb4-438f-be7a-f35813354d28", "node_type": "1", "metadata": {}, "hash": "d28aafbd02fa4efc955dca17f4d0cbbff56810a4bf4800fa924bb38857bc4e5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CE probing on ImageNet for GradCAM. The figure shows a set of six examples sampled randomly for each SSL method, i.e. DINO, BYOL and MOCO when explained using GradCAM explanations. Overall BCE probes lead to more localized explanations over CE probes. We highlight examples where BCE probes perform worse than CE probes with a red box. Please zoom-in to notice the finer differences in the visual explanations.\nUnder review as a conference paper at ICLR 2025\n\n# MLP vs Linear\n\n|Class: 'alp'|LP, Loc: 90.%|MLP-3, Loc: 95.%|\n|---|---|---|\n|Class: 'cannon'|LP, Loc: 90.%|MLP-3, Loc: 93.%|\n|Class: 'sewing machine'|LP, Loc: 87.%|MLP-3, Loc: 93.%|\n|Class: 'hourglass'|LP, Loc: 85.%|MLP-3, Loc: 92.%|\n|Class: 'sea slug'|LP, Loc: 98.%|MLP-3, Loc: 96.%|\n|Class: 'guacamole'|LP, Loc: 93.%|MLP-3, Loc: 96.%|\n|Class: 'bannister'|LP, Loc: 87.%|MLP-3, Loc: 92.%|\n|Class: 'king penguin'|LP, Loc: 96.%|MLP-3, Loc: 94.%|\n|Class: 'standard poodle'|LP, Loc: 97.%|MLP-3, Loc: 97.%|\n|Class: 'poncho'|LP, Loc: 86.%|MLP-3, Loc: 94.%|\n|Class: 'snorkel'|LP, Loc: 89.%|MLP-3, Loc: 95.%|\n|Class: 'lawn mower'|LP, Loc: 85.%|MLP-3, Loc: 88.%|\n|Class: 'meat loaf'|LP, Loc: 97.%|MLP-3, Loc: 93.%|\n|Class: 'boa constrictor'|LP, Loc: 98.%|MLP-3, Loc: 95.%|\n|Class: 'sports car'|LP, Loc: 88.%|MLP-3, Loc: 96.%|\n|Class: 'lawn mower'|LP, Loc: 82.%|MLP-3, Loc: 75.%|\n|Class: 'meat loaf'|LP, Loc: 68.%|MLP-3, Loc: 94.%|\n|Class: 'lakeside'|LP, Loc: 90.%|MLP-3, Loc: 85.%|\n\nFigure D13: More qualitative results for MLP Probing. The figure shows a set of six examples sampled randomly for each SSL method, i.e. DINO, BYOL and MOCO when explained using B-cos explanations. MLP probes on average tend to get visually more localized samples as compared to a linear probe. Please zoom-in to notice the finer differences in the visual explanations.\n# Under review as a conference paper at ICLR 2025\n\n# ADDITIONAL QUANTITATIVE RESULTS\n\nIn this section, for completeness we present quantitative results for all explanation (or attribution) methods; specifically for B-cos (B\u00f6hle et al., 2022), Layer-wise Relevance Propagation (LRP) (Bach et al., 2015), GradCAM (Selvaraju et al., 2017), Integrated Gradients (IntGrad) (Sundararajan et al., 2017), Input\u00d7Gradient (I\u00d7G) (Shrikumar et al., 2017), LIME (Ribeiro et al., 2016) and Guided-Backpropagation (Springenberg et al., 2015). In Sec. E.1 we first evaluate the impact of the optimization objective on probing in terms of accuracy and explanation quality using the different metrics (see section 3.1) on ImageNet. Then in Sec. E.2 we analyze the effect of probe complexity on explanation localization. Finally in the second part of Sec. E.2, we also present the results on the multi-label classification setting on COCO and VOC datasets, where the explanation localization metric used is EPG score.\n\n# E.1 IMPACT OF BCE VS.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2827, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8aff54e3-1eb4-438f-be7a-f35813354d28": {"__data__": {"id_": "8aff54e3-1eb4-438f-be7a-f35813354d28", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "75f91c95-e67a-4a4f-836f-fe6f8a4e8f61", "node_type": "4", "metadata": {}, "hash": "8334c081061f122a58feab1bd82a81a210d358bf9fd63059061958bf53ad5a6e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83be867f-1739-4404-949a-62f15c183419", "node_type": "1", "metadata": {}, "hash": "a74859b60d9bca3321e6426f6e95a1892a6118e58d23c116b217dd44c27257a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e692e3aa-bbc4-4c68-9f46-7e77c8320470", "node_type": "1", "metadata": {}, "hash": "a67159ea104f3ff6678d58be2bc56014984ab61cefabbf3bc1827139180b0017", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Sec. E.1 we first evaluate the impact of the optimization objective on probing in terms of accuracy and explanation quality using the different metrics (see section 3.1) on ImageNet. Then in Sec. E.2 we analyze the effect of probe complexity on explanation localization. Finally in the second part of Sec. E.2, we also present the results on the multi-label classification setting on COCO and VOC datasets, where the explanation localization metric used is EPG score.\n\n# E.1 IMPACT OF BCE VS. CE\n\n|Bcos: W(x) x Input|Bcos: W(x) x Input|Bcos: W(x) x Input|Integrated Gradients|Integrated Gradients|Integrated Gradients|Input x Gradient|Input x Gradient|Input x Gradient|\n|---|---|---|\n|2|100| |2|100| |2|100| |\n|1|60| |1|60| |1|60| |\n|0|40| |0|40| |0|40| |\n|20|BCE| |20|BCE| |20|BCE| |\n| |CE| | |CE| | |CE| |\n|66|66|66|66|66|66|66|66|66|\n|68|68|68|68|68|68|68|68|68|\n|70|70|70|70|70|70|70|70|70|\n|72|72|72|72|72|72|72|72|72|\n|74|74|74|74|74|74|74|74|74|\n|76|76|76|76|76|76|76|76|76|\n\nFigure E1: BCE vs. CE \u2014 Accuracy and GridPG scores on ImageNet. To study the impact of the optimization objective on probing we plot the accuracy vs localization (GridPG) score for B-cos models (B-RN50). For BCE trained probes (solid markers), a steady improvement in the localization score is seen for all SSL methods and across most explanation methods on ImageNet. For GradCAM (bottom left), DINO model does not show an improvement in localization score for BCE trained probe, and Guided Backpropagation explanations (bottom right) are inconsistent for SSL models.\n\nIn fig. E1 we plot the linear probe accuracies vs GridPG score for MoCov2 (red), BYOL (green), DINO (blue), CLIP (yellow) and supervised (black) models for (a) B-cos models (B-RN50) and (b) conventional models (RN50) on ImageNet. The linear probes trained with BCE loss are depicted with filled markers, and the hollow markers represent CE loss. We see consistent improvement in localization score for most explanation methods. Note, as discussed in the main paper I\u00d7G and GradCAM do not show consistent behaviour and only show consistent improvements for B-cos models (compare the bottom rows in fig. E1 (a) vs fig. E1 (b)). For I\u00d7G, this is in line with prior work, as model gradients for conventional models are known to suffer from \u2018shattered gradients\u2019 (cf. (Balduzzi et al., 2017)) and provide highly noisy attributions. Additionally, Guided Backpropagation also performs poorly overall to see any obvious trend (as it has been shown earlier to produce noisy explanations (Rao et al., 2022)). Finally, for GradCAM it has been shown that especially for self-supervised models, it can perform poorly due to focusing on background regions rather than object of interest (this can be induced due to the SSL specific training objectives) (Selvaraju et al., 2021).\n\nFigures E2, E3 show the EPG localization scores on the bounding boxes provided for the ImageNet validation set. For B-cos backbones we observe a consistent improvement for BCE trained probes for 11 out of 12 cases. The only odd case is GradCAM explanations for the DINO model. For conventional backbones this is consistent for 9 out of 12 cases (GradCAM explanations being the exception again).\nUnder review as a conference paper at ICLR 2025\n\nNext figures E4, E5 show the pixel deletion plots on the ImageNet validation set. For B-cos backbones we observe a more consistent improvement in terms of stability when compared to conventional models. For B-cos backbones for GradCAM method, CE probes produce more stability, and for conventional backbones we only see consistent results for LRP attribution methods (where BCE probes are more stable than CE probes). For other attribution methods and pre-trained backbones we see mixed results.", "mimetype": "text/plain", "start_char_idx": 2332, "end_char_idx": 6092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e692e3aa-bbc4-4c68-9f46-7e77c8320470": {"__data__": {"id_": "e692e3aa-bbc4-4c68-9f46-7e77c8320470", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "75f91c95-e67a-4a4f-836f-fe6f8a4e8f61", "node_type": "4", "metadata": {}, "hash": "8334c081061f122a58feab1bd82a81a210d358bf9fd63059061958bf53ad5a6e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8aff54e3-1eb4-438f-be7a-f35813354d28", "node_type": "1", "metadata": {}, "hash": "d28aafbd02fa4efc955dca17f4d0cbbff56810a4bf4800fa924bb38857bc4e5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8524edff-44a4-470f-be8a-0dab06c57d5d", "node_type": "1", "metadata": {}, "hash": "70b0a8f56da70981b7790613ebdc6f6a0219442ef436605635ed76173713cfbb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For B-cos backbones we observe a consistent improvement for BCE trained probes for 11 out of 12 cases. The only odd case is GradCAM explanations for the DINO model. For conventional backbones this is consistent for 9 out of 12 cases (GradCAM explanations being the exception again).\nUnder review as a conference paper at ICLR 2025\n\nNext figures E4, E5 show the pixel deletion plots on the ImageNet validation set. For B-cos backbones we observe a more consistent improvement in terms of stability when compared to conventional models. For B-cos backbones for GradCAM method, CE probes produce more stability, and for conventional backbones we only see consistent results for LRP attribution methods (where BCE probes are more stable than CE probes). For other attribution methods and pre-trained backbones we see mixed results.\n\n# Layerwise Relevance Propagation\n\n# Integrated Gradients\n\n# Input x Gradient\n\n|2|100| |80|5| | |\n|---|---|---|---|---|---|---|\n|1|60| | | | | |\n|8|40| | | | | |\n| |20|BCE| | | | |\n| | | |CE| | | |\n\n66\n68\n70\n72\n74\n76\n\n# Accuracy\n\n# GradCAM\n\n# LIME\n\n# Guided Backpropagation\n\n|2|100| |80|8| | |\n|---|---|---|---|---|---|---|\n|1|60| | | | | |\n| |40| | | | | |\n|8|20|BCE| | | | |\n| | | |CE| | | |\n\n66\n68\n70\n72\n74\n76\n\n# Accuracy\n\n(b)\n\nFigure E1: BCE vs. CE \u2014 Accuracy and GridPG scores on ImageNet. To study the impact of the optimization objective on probing we plot the accuracy vs localization (GridPG) score for conventional models (RN50). For BCE trained probes (solid markers), an improvement in the localization score is seen for all SSL methods for LRP, IntGrad and LIME explanations on ImageNet. However, for I\u00d7G, and GradCAM this is not seen. For Guided Backpropagation there is a slight improvement in localization score for BCE trained probes (except for DINO model).\nUnder review as a conference paper at ICLR 2025\n\n| |MoCov2| |BYOL| |DINO| | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Bcos: W(x) x Input|1.0| |1.0| |1.0| | | | | | | |\n| |BCE| |BCE| |BCE| | | | | | | |\n|0.8|CE|0.8|CE|0.8|CE| | | | | | | |\n|1|0.6| |0.6| |0.6| | | | | | | |\n|2|0.4| |0.4| |0.4| | | | | | | |\n| |0.2| |0.2| |0.2| | | | | | | |\n| |0.0|&lt;25%|&lt;75%|&lt;100%|0.0|&lt;25%|&lt;75%|&lt;100%|0.0|&lt;25%|&lt;75%|&lt;100%|\n| |&lt;50%| |&lt;50%| |&lt;50%| | | | | | | |\n\n| |MoCov2| |BYOL| |DINO| | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Integrated Gradients|1.0| |1.0| |1.0| | | | | | | |\n| |BCE| |BCE| |BCE| | | | | | | |\n|0.8|CE|0.8|CE|0.8|CE| | | | | | | |\n|1|0.6| |0.6| |0.6| | | | | | | |\n|2|0.4| |0.4| |0.4| | | | | | | |\n| |0.2| |0.2| |0.2| | | | | | | |\n| |0.0|&lt;25%|&lt;50%|&lt;", "mimetype": "text/plain", "start_char_idx": 5265, "end_char_idx": 7920, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8524edff-44a4-470f-be8a-0dab06c57d5d": {"__data__": {"id_": "8524edff-44a4-470f-be8a-0dab06c57d5d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "75f91c95-e67a-4a4f-836f-fe6f8a4e8f61", "node_type": "4", "metadata": {}, "hash": "8334c081061f122a58feab1bd82a81a210d358bf9fd63059061958bf53ad5a6e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e692e3aa-bbc4-4c68-9f46-7e77c8320470", "node_type": "1", "metadata": {}, "hash": "a67159ea104f3ff6678d58be2bc56014984ab61cefabbf3bc1827139180b0017", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0| |1.0| |1.0| | | | | | | |\n| |BCE| |BCE| |BCE| | | | | | | |\n|0.8|CE|0.8|CE|0.8|CE| | | | | | | |\n|1|0.6| |0.6| |0.6| | | | | | | |\n|2|0.4| |0.4| |0.4| | | | | | | |\n| |0.2| |0.2| |0.2| | | | | | | |\n| |0.0|&lt;25%|&lt;50%|&lt;75%|0.0|&lt;25%|&lt;50%|&lt;75%|0.0|&lt;25%|&lt;50%|&lt;75%|\n| |&lt;100%| |&lt;100%| |&lt;100%| | | | | | | |\n\n| |MoCov2| |BYOL| |DINO| | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Input x Gradient|1.0| |1.0| |1.0| | | | | | | |\n| |BCE| |BCE| |BCE| | | | | | | |\n|0.8|CE|0.8|CE|0.8|CE| | | | | | | |\n|1|0.6| |0.6| |0.6| | | | | | | |\n|2|0.4| |0.4| |0.4| | | | | | | |\n| |0.2| |0.2| |0.2| | | | | | | |\n| |0.0|0.0|0.0| | | | | | | | | |\n| |&lt;25%|&lt;50%|&lt;75%|&lt;100%|&lt;25%|&lt;50%|&lt;75%|&lt;100%|&lt;25%|&lt;50%|&lt;75%|&lt;100%|\n| |&lt;50%| |&lt;50%| |&lt;50%| | | | | | | |\n\n| |MoCov2| |BYOL| |DINO| | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|GradCAM|1.0| |1.0| |1.0| | | | | | | |\n| |BCE| |BCE| |BCE| | | | | | | |\n|0.8|CE|0.8|CE|0.8|CE| | | | | | | |\n|1|0.6| |0.6| |0.6| | | | | | | |\n|2|0.4| |0.4| |0.4| | | | | | | |\n| |0.2| |0.2| |0.2| | | | | | | |\n| |0.0|0.0|0.0| | | | | | | | | |\n| |&lt;25%|&lt;50%|&lt;75%|&lt;100%|&lt;25%|&lt;50%|&lt;75%|&lt;100%|&lt;25%|&lt;50%|&lt;75%|&lt;100%|\n| |&lt;50%| |&lt;50%| |&lt;50%| | | | | | | |\n\nFigure E2: BCE vs. CE \u2014 EPG scores for B-cos models on ImageNet.", "mimetype": "text/plain", "start_char_idx": 7691, "end_char_idx": 9084, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b0629401-432c-44a1-8bdb-51a55cef775e": {"__data__": {"id_": "b0629401-432c-44a1-8bdb-51a55cef775e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5b5d091-a362-4b06-9ab4-a6257983cb23", "node_type": "4", "metadata": {}, "hash": "9b0f3e50f2970049434ba82c72ca2ff06d635661a04042112d6711676cbaf4bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "728752c2-7f9b-41e7-87a2-55523d10065c", "node_type": "1", "metadata": {}, "hash": "1499d3b172c77eb31d1673d384a61ae1e713e42f8b4f9dc001472350cefb1243", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CE \u2014 EPG scores for B-cos models on ImageNet.\n# Under review as a conference paper at ICLR 2025\n\n| |MoCov2| | |Layerwise Relevance Propagation| | |BYOL| | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|1.0| | | |BCE|BCE|BCE| | | | | | | | | | |\n| | |0.8| |CE|0.8|CE|0.8|CE| | | | | | | | |\n|1|0.6| |0.6| |0.6| | | | | | | | | | | |\n| |2|0.4| |0.4| |0.4| | | | | | | | | | |\n| | |0.2| |0.2| |0.2| | | | | | | | | | |\n| | |0.0| |0.0| |0.0| | | | | | | | | | |\n| |<25%|<25%|<25%|<25%|<50%|<50%|<50%|<50%|<75%|<75%|<75%|<75%|<100%|<100%|<100%|<100%| | | | | | | | | | | | |\n| | |MoCov2| | |Integrated Gradients| | |BYOL|Integrated Gradients| | |DINO|Integrated Gradients| | | |\n|1.0| | | |BCE|BCE|BCE| | | | | | | | | | |\n| | |0.8| |CE|0.8|CE|0.8|CE| | | | | | | | |\n|8|0.6| |0.6| |0.6| | | | | | | | | | | |\n| |8|0.4| |0.4| |0.4| | | | | | | | | | |\n| | |0.2| |0.2| |0.2| | | | | | | | | | |\n| | |0.0| |0.0| |0.0| | | | | | | | | | |\n| |<25%|<25%|<25%|<25%|<50%|<50%|<50%|<50%|<75%|<75%|<75%|<75%|<100%|<100%|<100%|<100%| | | | | | | | | | | | |\n| | | |MoCov2| |Input x Gradient| | | |BYOL|Input x Gradient| | |DINO|Input x Gradient| | |\n|1.0| | | |BCE|BCE|BCE| | | | | | | | | | |\n| | |0.8| |CE|0.8|CE|0.8|CE| | | | | | | | |\n|8|0.6| |0.6| |0.6| | | | | | | | | | | |\n| |8|0.4| |0.4| |0.4| | | | | | | | | | |\n| | |0.2| |0.2| |0.2| | | | | | | | | | |\n| | |0.0| |0.0| |0.0| | | | | | | | | | |\n| |<25%|<25%|<25%|<25%|<50%|<50%|<50%|<50%|<75%|<75%|<75%|<75%|<100%|<100%|<100%|<100%| | | | | | | | | | | | |\n| | | |MoCov2| |GradCAM| | | |BYOL|GradCAM| | |DINO|GradCAM| | |\n|1.0| | | |BCE|BCE|BCE| | | | | | | | | | |\n| | |0.8| |CE|0.8|CE|0.8|CE| | | | | | | | |\n|1|0.6| |0.6| |0.6| | | | | | | | | | | |\n| |8|0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1747, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "728752c2-7f9b-41e7-87a2-55523d10065c": {"__data__": {"id_": "728752c2-7f9b-41e7-87a2-55523d10065c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5b5d091-a362-4b06-9ab4-a6257983cb23", "node_type": "4", "metadata": {}, "hash": "9b0f3e50f2970049434ba82c72ca2ff06d635661a04042112d6711676cbaf4bf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0629401-432c-44a1-8bdb-51a55cef775e", "node_type": "1", "metadata": {}, "hash": "ba19999c2ab237102e7c63ebe9f70632e0f9aa3cd21dfbd7ba0bf7c6ffee28d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aafde6a4-5e10-47a0-a722-16f71fcef598", "node_type": "1", "metadata": {}, "hash": "35eed1007a8a1208f2083b7ef191d306c8dfe8d593823bbf5c11b4149e56423b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0| | | |BCE|BCE|BCE| | | | | | | | | | |\n| | |0.8| |CE|0.8|CE|0.8|CE| | | | | | | | |\n|1|0.6| |0.6| |0.6| | | | | | | | | | | |\n| |8|0.4| |0.4| |0.4| | | | | | | | | | |\n| | |0.2| |0.2| |0.2| | | | | | | | | | |\n| | |0.0| |0.0| |0.0| | | | | | | | | | |\n| |<25%|<25%|<25%|<25%|<50%|<50%|<50%|<50%|<75%|<75%|<75%|<75%|<100%|<100%|<100%|<100%| | | | | | | | | | | | |\n\nFigure E3: BCE vs. CE \u2014 EPG scores for Conventional models on ImageNet.\nUnder review as a conference paper at ICLR 2025\n\n| |MoCov2|Bcos: W(x) x Input| | |BYOL|Bcos: W(x) x Input| | | | | | |DINO|Bcos: W(x) x Input| | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | |BCE|CE|BCE|CE|BCE|CE| | | | | | | | | |\n|0.8|0.8|0.8| |0.6|0.6|0.6|0.4|0.4|0.4| | | | | | | | |\n|0.2|0.2|0.2|0.2|0.2|0.2|0.2|0.2|0.2| | | | | | | | | |\n|5 10 15 20| | | | | | | | | | | | | | | | | |\n| | | | | | | |% of pixels removed| | | | | | | | | | |\n\n| |MoCov2|Integrated Gradients| | |BYOL|Integrated Gradients| | | | | | |DINO|Integrated Gradients| | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | |BCE|CE|BCE|CE|BCE|CE| | | | | | | | | |\n|0.8|0.8|0.8| |0.6|0.6|0.6|0.4|0.4|0.4| | | | | | | | |\n|0.2|0.2|0.2|0.2|0.2|0.2|0.2|0.2|0.2| | | | | | | | | |\n|5 10 15 20| | | | | | | | | | | | | | | | | |\n| | | | | | | |% of pixels removed| | | | | | | | | | |\n\n| |MoCov2|Input x Gradient| | | | |BYOL|Input x Gradient| | | | | |DINO|Input x Gradient| | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | |BCE|CE|BCE|CE|BCE|CE| | | | | | | | |\n|0.8|0.8|0.8| | |0.6|0.6|0.6|0.4|0.4|0.4| | | | | | | | |\n|0.2|0.2|0.2|0.2|0.2|0.2|0.2|0.2|0.", "mimetype": "text/plain", "start_char_idx": 1612, "end_char_idx": 3296, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aafde6a4-5e10-47a0-a722-16f71fcef598": {"__data__": {"id_": "aafde6a4-5e10-47a0-a722-16f71fcef598", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5b5d091-a362-4b06-9ab4-a6257983cb23", "node_type": "4", "metadata": {}, "hash": "9b0f3e50f2970049434ba82c72ca2ff06d635661a04042112d6711676cbaf4bf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "728752c2-7f9b-41e7-87a2-55523d10065c", "node_type": "1", "metadata": {}, "hash": "1499d3b172c77eb31d1673d384a61ae1e713e42f8b4f9dc001472350cefb1243", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36c66177-7ef8-48ac-a4c6-8bd2a1699c3f", "node_type": "1", "metadata": {}, "hash": "0a0d45597fafe998bfb6629082da1e6f713ddb9153040de4ac8d52bce2073ffc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8|0.8|0.8| | |0.6|0.6|0.6|0.4|0.4|0.4| | | | | | | | |\n|0.2|0.2|0.2|0.2|0.2|0.2|0.2|0.2|0.2| | | | | | | | | | |\n|5 10 15 20| | | | | | | | | | | | | | | | | | |\n| | | | | | | | |% of pixels removed| | | | | | | | | | |\n\n| | |MoCov2| |GradCAM| | | |BYOL|GradCAM| | | | | |DINO|GradCAM| | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | | |BCE|CE|BCE|CE|BCE|CE| | | | | | | | | |\n|0.8|0.8|0.8| | | |0.6|0.6|0.6|0.4|0.4|0.4| | | | | | | | | |\n|0.2|0.2|0.2|0.2|0.2|0.2|0.2|0.2|0.2| | | | | | | | | | | | |\n|5 10 15 20| | | | | | | | | | | | | | | | | | | | |\n| | | | | | | | | |% of pixels removed| | | | | | | | | | | |\n\nFigure E4: BCE vs. CE \u2014 Pixel deletion scores for B-cos models on ImageNet.\n\n|MoCov2| |Layerwise Relevance Propagation| | | |BYOL|Layerwise Relevance Propagation| | | | |DINO|Layerwise Relevance Propagation| | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | |BCE|CE|BCE|CE|BCE|CE| | | | | | | | | |\n|0.8|0.8|0.8| | |0.6|0.6|0.6|0.4|0.4|0.4| | | | | | | | | |\n|0.2|0.2|0.2|0.2|0.2|0.2|0.2|0.2|0.2| | | | | | | | | | | |\n|5 10 15 20| | | | | | | | | | | | | | | | | | | |\n| | | | | | | | |% of pixels removed| | | | | | | | | | | |\n\nFigure E5: BCE vs. CE \u2014 Pixel deletion scores for Conventional models on ImageNet.\n# Under review as a conference paper at ICLR 2025\n\n# Table E1: Compactness and Complexity of BCE vs CE Probing\n\nA consistent improvement in compactness (Gini index pp. as in (Chalasani et al., 2018)) and reduction in complexity (entropy as in (Tseng et al., 2020)) for BCE vs. CE, except for Input\u00d7Gradients and GradCAM with conventional backbones.", "mimetype": "text/plain", "start_char_idx": 3206, "end_char_idx": 4895, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "36c66177-7ef8-48ac-a4c6-8bd2a1699c3f": {"__data__": {"id_": "36c66177-7ef8-48ac-a4c6-8bd2a1699c3f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5b5d091-a362-4b06-9ab4-a6257983cb23", "node_type": "4", "metadata": {}, "hash": "9b0f3e50f2970049434ba82c72ca2ff06d635661a04042112d6711676cbaf4bf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aafde6a4-5e10-47a0-a722-16f71fcef598", "node_type": "1", "metadata": {}, "hash": "35eed1007a8a1208f2083b7ef191d306c8dfe8d593823bbf5c11b4149e56423b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f37fb67-79e6-45dc-b4b1-fd29556f2d38", "node_type": "1", "metadata": {}, "hash": "8673f0a8cf416d57e572a63c178fc957005887a5f0a69c9229bfc27c57993c2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CE \u2014 Pixel deletion scores for Conventional models on ImageNet.\n# Under review as a conference paper at ICLR 2025\n\n# Table E1: Compactness and Complexity of BCE vs CE Probing\n\nA consistent improvement in compactness (Gini index pp. as in (Chalasani et al., 2018)) and reduction in complexity (entropy as in (Tseng et al., 2020)) for BCE vs. CE, except for Input\u00d7Gradients and GradCAM with conventional backbones.\n\n| |Backbone|Pre-training Method|XAI Method|Accuracy| |\u2206Complexity\u2193|\u2206Compactness\u2191|\n|---|---|---|---|---|---|---|---|\n| |B-RN50|MoCoV2|Bcos: W(x) x Input|66.3|65.4|-2.72|+0.10|\n| |B-RN50|BYOL|Bcos: W(x) x Input|68.3|68.1|-1.87|0.14|\n| |B-RN50|DINO|Bcos: W(x) x Input|68.8|67.5|-2.35|+0.16|\n| |B-RN50|MoCoV2|Input x Gradient|66.3|65.4|-0.13|0.00|\n| |B-RN50|BYOL|Input x Gradient|68.3|68.1|-0.08|0.00|\n| |B-RN50|DINO|Input x Gradient|68.8|67.5|-0.08|0.00|\n| |B-RN50|MoCoV2|IntGrad|66.3|65.4|-0.09|0.00|\n| |B-RN50|BYOL|IntGrad|68.3|68.1|-0.04|0.00|\n| |B-RN50|DINO|IntGrad|68.8|67.5|-0.05|0.00|\n| |B-RN50|MoCoV2|GradCAM|66.3|65.4|-1.07|+0.13|\n| |B-RN50|BYOL|GradCAM|68.3|68.1|-2.82|+0.20|\n| |B-RN50|DINO|GradCAM|68.8|67.5|-2.51|+0.19|\n| |B-RN50|MoCoV2|GBP|66.3|65.4|-0.23|+0.02|\n| |B-RN50|BYOL|GBP|68.3|68.1|-0.14|+0.01|\n| |B-RN50|DINO|GBP|68.8|67.5|-0.13|+0.01|\n| |RN50|MoCoV2|Input x Gradient|67.5|66.9|+0.01|0.00|\n| |RN50|BYOL|Input x Gradient|70.1|69.3|-0.04|+0.01|\n| |RN50|DINO|Input x Gradient|71.2|70.6|+0.01|0.00|\n| |RN50|MoCoV2|IntGrad|67.5|66.9|-0.01|0.00|\n| |RN50|BYOL|IntGrad|70.1|69.3|-0.08|0.00|\n| |RN50|DINO|IntGrad|71.2|70.6|0.00|0.00|\n| |RN50|MoCoV2|LRP|67.5|66.9|-1.48|+0.07|\n| |RN50|BYOL|LRP|70.1|69.3|-0.85|+0.04|\n| |RN50|DINO|LRP|71.2|70.6|-2.01|+0.12|\n| |RN50|MoCoV2|GradCAM|67.5|66.9|+1.13|-0.25|\n| |RN50|BYOL|GradCAM|70.1|69.3|+1.61|-0.25|\n| |RN50|DINO|GradCAM|71.2|70.6|-2.94|+0.23|\n| |RN50|MoCoV2|GBP|67.5|66.9|-0.14|+0.01|\n| |RN50|BYOL|GBP|70.1|69.3|-0.24|+0.02|\n| |RN50|DINO|GBP|71.2|70.6|0.00|0.00|\nUnder review as a conference paper at ICLR 2025\n\n# Table E2: Supervised Probing\n\nWe present the effect of probing fully-supervised models (ref.", "mimetype": "text/plain", "start_char_idx": 4483, "end_char_idx": 6562, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f37fb67-79e6-45dc-b4b1-fd29556f2d38": {"__data__": {"id_": "8f37fb67-79e6-45dc-b4b1-fd29556f2d38", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5b5d091-a362-4b06-9ab4-a6257983cb23", "node_type": "4", "metadata": {}, "hash": "9b0f3e50f2970049434ba82c72ca2ff06d635661a04042112d6711676cbaf4bf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36c66177-7ef8-48ac-a4c6-8bd2a1699c3f", "node_type": "1", "metadata": {}, "hash": "0a0d45597fafe998bfb6629082da1e6f713ddb9153040de4ac8d52bce2073ffc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 4.1 in main paper) for different attribution methods (B-cos B\u00a8 ohle et al. (2022), IntGrad Sundararajan et al. (2017), I\u00d7G Shrikumar et al. (2017), GradCAM Selvaraju et al. (2017), and LRP Bach et al. (2015)), and demonstrate how this impacts the localization (GridPG) score. We observe similar accuracy for both (CE and BCE) probes. However, as seen earlier (cf. fig. E1a,b), the probes trained with BCE loss lead to consistently high improvements in GridPG scores.\n\n|Accuracy (%)|GridPG (%)| | | |Att. Method|CE|BCE|\u2206|CE|BCE|\u2206| |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | |-|72.4|73.7|+1.3|-|-|-| |\n| | | | |B-RN50|B-cos B\u00a8 ohle et al. (2022)| | | | |15.0|87.0|+72.0|\n|-| | | | |IntGrad Sundararajan et al. (2017)|-|-| |24.0|86.0|+62.0| |\n| | | | |-|I\u00d7G Shrikumar et al. (2017)|-|-| | |15.0|65.0|+50.0|\n| | | | |-|GradCAM Selvaraju et al. (2017)|-|-|14.0| |72.0|+58.0| |\n| | | | | |-|75.5|75.6|+0.1|-|-|-| |\n| | | | | |RN50LRP Bach et al. (2015)|-|-| |9.0|32.0|+23.0| |\n|-| | | | |IntGrad Sundararajan et al.", "mimetype": "text/plain", "start_char_idx": 6563, "end_char_idx": 7603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c71ddbaa-324e-447f-809e-b283a23f2f81": {"__data__": {"id_": "c71ddbaa-324e-447f-809e-b283a23f2f81", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fa68b9a-1700-4b2f-9ff7-f97b0b162055", "node_type": "4", "metadata": {}, "hash": "8dca31c19dd3c8d7a5c13381cb9afbab7de23b828f684bf4fae46b0e28ba42b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecf7a71b-9b4b-42ee-a83b-043585fedfdd", "node_type": "1", "metadata": {}, "hash": "4b81c30d5524524de4cd27a92f1a5518503ce2e17f4d8ac08ced4f0c77cb6d48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2017)|-|-|14.0| |72.0|+58.0| |\n| | | | | |-|75.5|75.6|+0.1|-|-|-| |\n| | | | | |RN50LRP Bach et al. (2015)|-|-| |9.0|32.0|+23.0| |\n|-| | | | |IntGrad Sundararajan et al. (2017)|-|-| |14.0|28.0|+14.0| |\n|-| | | | |I\u00d7G Shrikumar et al. (2017)|-|-| |10.0|18.0|+8.0| |\n|-| | | | |GradCAM Selvaraju et al. (2017)|-|-| |16.0|42.0|+26.0| |\n\nIn table E1 we present the results for probing with BCE vs CE loss, when evaluated on the compactness and complexity metrics. A consistent improvement in compactness (Gini index pp. as in (Chalasani et al., 2018)) and reduction in complexity (entropy as in (Tseng et al., 2020)) for BCE vs. CE, except for Input\u00d7Gradients and GradCAM with conventional backbones.\n\n# Probing Supervised Models\n\nIn table E2, we demonstrate the effect of probing fully-supervised backbones with CE and BCE linear probes. This is to also increase comparability with the self-supervised and vision-language approaches that we present earlier (cf. fig. E1 (a) and (b)).\n\n# Additional Perturbation-based Explanations\n\nIn table E3, we show the improved localization ability of BCE probes over CE probes for another perturbation-based explanation method called Score-CAM (Wang et al., 2020). See Figure D11 for qualitative results.\n\n# Table E3: ScoreCAM BCE vs. CE Probing\u2014GridPG scores on ImageNet\n\nA consistent improvement in localization score for BCE over CE probes is seen for both conventional and inherently interpretable B-cos backbones for supervised and self-supervised pre-training.\n\n|Localization%|Backbone|Pretraining|CE|BCE|\u2206loc|\n|---|---|---|---|---|---|\n|RN50|MoCov2|52.6|54.4|+1.8| |\n|RN50|BYOL|38.1|40.5|+2.4| |\n|RN50|DINO|44.2|55.9|+11.7| |\n|BRN50|MoCov2|30.2|43.2|+13.0| |\n|BRN50|BYOL|45.3|50.2|+4.9| |\n|BRN50|DINO|51.0|67.7|+16.7| |\n\n# E.2\n\nIn this subsection, we present the quantitative results for probing with more complex MLP probes, specifically instead of using a single linear probe on top of frozen backbone features, we use two- and three-layer MLPs for training on downstream tasks and study its effect on performance (in terms of classification accuracy or f1-score) and explanation localization ability. For this setting, we train both B-cos and conventional MLPs, and show results for the same.\n\n# Effect of MLP Probes on ImageNet\n\nIn fig. E6 we show the results on ImageNet for B-cos MLPs for both (a) B-cos and (b) conventional SSL models. It is observed that for all SSL models there is both an increase in accuracy as well as in the localization (GridPG) score across most attribution methods. Next, in fig. E7 we plot the results to see the effect of conventional MLPs. In contrast to B-cos MLPs, here the stronger MLP based probing does lead to an increase in accuracy but does not lead to an increase in GridPG localization score. This is observed independent of B-cos or conventional model.\nUnder review as a conference paper at ICLR 2025\n\n2160 backbones. We attribute this to the alignment pressure introduced by B-cos layers (cf. (B\u00a8 ohle et al., 2022)) that helps distill object-class relevant features and rely less on background context. Thus, as discussed in the main paper in order to get attribution maps with good localization on downstream tasks, we suggest to probe pre-trained models with B-cos probes.\n\nThese are followed by presenting the bar plots for MLP probing experiments for EPG scores on ImageNet in figures E8, E9 for B-cos probes, and in figures E10, E11 for conventional probes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3453, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ecf7a71b-9b4b-42ee-a83b-043585fedfdd": {"__data__": {"id_": "ecf7a71b-9b4b-42ee-a83b-043585fedfdd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fa68b9a-1700-4b2f-9ff7-f97b0b162055", "node_type": "4", "metadata": {}, "hash": "8dca31c19dd3c8d7a5c13381cb9afbab7de23b828f684bf4fae46b0e28ba42b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c71ddbaa-324e-447f-809e-b283a23f2f81", "node_type": "1", "metadata": {}, "hash": "3e9573fa0386ad96ed7f81d642c62086ff75114f81a2a46905f00e438be7c432", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "366953ba-e3e6-4f02-8e23-356f687fa945", "node_type": "1", "metadata": {}, "hash": "222b37d402280ba4358fab5625332a6a8fa225b6847f05ec8fbcaf27c7b68996", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In contrast to B-cos MLPs, here the stronger MLP based probing does lead to an increase in accuracy but does not lead to an increase in GridPG localization score. This is observed independent of B-cos or conventional model.\nUnder review as a conference paper at ICLR 2025\n\n2160 backbones. We attribute this to the alignment pressure introduced by B-cos layers (cf. (B\u00a8 ohle et al., 2022)) that helps distill object-class relevant features and rely less on background context. Thus, as discussed in the main paper in order to get attribution maps with good localization on downstream tasks, we suggest to probe pre-trained models with B-cos probes.\n\nThese are followed by presenting the bar plots for MLP probing experiments for EPG scores on ImageNet in figures E8, E9 for B-cos probes, and in figures E10, E11 for conventional probes.\n\nFigures E12, E13, and E14 contain the pixel deletion plots on ImageNet for the MLP probing setup.\n\n|B-RNSO, Bcos W(x) x Input| |B-RN5O, Integrated Gradients| |B-RNSO, Input X Gradient|100|\n|---|---|---|---|---|---|\n|3|80| | | | |\n|1|60| | | | |\n|8|40| | | | |\n|20|20| | | | |\n|0 64 66 68 Accuracy| | |70|72|74|\n\n|B-RNSO, GradCAM| |B-RNSO, LIME| | |B-RNSO, Guided Backpropagation|100| |\n|---|---|---|---|---|---|---|---|\n|3|80| | | | | | |\n|1|60| | | | | | |\n|8|40| | | | | | |\n|20|20| | | | | | |\n|0 64 66 68 Accuracy| | |70| | |72|74|\n\nFigure E6: Effect of complex B-cos probes on accuracy and GridPG scores on ImageNet on (a) B-cos backbones (B-RN50) and (b) conventional backbones (RN50). Notice that there is a consistent improvement in accuracy (x-axis) as well as GridPG localization score (y-axis) as we move from a single probe to two- and three-layer MLP probes in most cases. Except only for B-cos models trained with MoCov2, for IntGrad (top-middle, subplot a) and GradCAM (bottom-left, subplot a) explanations we see a slight drop in localization scores. For the remaining cases (SSL frameworks and explanation methods), there is a steady increase in both accuracy and localization.", "mimetype": "text/plain", "start_char_idx": 2618, "end_char_idx": 4649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "366953ba-e3e6-4f02-8e23-356f687fa945": {"__data__": {"id_": "366953ba-e3e6-4f02-8e23-356f687fa945", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fa68b9a-1700-4b2f-9ff7-f97b0b162055", "node_type": "4", "metadata": {}, "hash": "8dca31c19dd3c8d7a5c13381cb9afbab7de23b828f684bf4fae46b0e28ba42b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecf7a71b-9b4b-42ee-a83b-043585fedfdd", "node_type": "1", "metadata": {}, "hash": "4b81c30d5524524de4cd27a92f1a5518503ce2e17f4d8ac08ced4f0c77cb6d48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6dbcc888-9064-42f3-89c5-11485572f074", "node_type": "1", "metadata": {}, "hash": "daeae81fa2b2f2362ca434c9cf69d7860c86aa7065199bd8c82f726091f6d394", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Notice that there is a consistent improvement in accuracy (x-axis) as well as GridPG localization score (y-axis) as we move from a single probe to two- and three-layer MLP probes in most cases. Except only for B-cos models trained with MoCov2, for IntGrad (top-middle, subplot a) and GradCAM (bottom-left, subplot a) explanations we see a slight drop in localization scores. For the remaining cases (SSL frameworks and explanation methods), there is a steady increase in both accuracy and localization.\n# Under review as a conference paper at ICLR 2025\n\n| | | |B-RN5O, Bcos W(x) x Input| |B-RNSO, Integrated Gradients| | | | | | |B-RNSO, Input X Gradient| | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | |100|80|60|100|80|60| | | | | | | | | | | |\n| |1|60|1|60|1|60| | | | | | | | | | | | |\n| |8|40|8|40|8|40| | | | | | | | | | | | |\n| |20|20|20|20|20|20| | | | | | | | | | | | |\n| | | |Accuracy| | | | | | | | | | | | | | | |\n| | | | |LP|MLP-2|MLP-3| | | | | | | | | | | | |\n| | |100|80|60|100|80|60| | | | | | | | | | | |\n| | | | |3|80|3|80|3|80| | | | | | | | | |\n| |1|60|1|60|1|60| | | | | | | | | | | | |\n| |8|40|8|40|8|40| | | | | | | | | | | | |\n| |20|20|20|20|20|20| | | | | | | | | | | | |\n| | | |Accuracy| | | | | | | | | | | | | | | |\n| | |RNSO, Layerwise Relevance Propagation| | |RN5O, Integrated Gradients| | | | | | |RNSO, Input x Gradient| | | | | | |\n| | |100|80|60|100|80|60| | | | | | | | | | | |\n| | | | |3|80|3|80|3|80| | | | | | | | | |\n| |1|60|1|60|1|60| | | | | | | | | | | | |\n| |8|40|8|40|8|40| | | | | | | | | | | | |\n| |20|20|20|20|20|20| | | | | | | | | | | | |\n| | | |Accuracy| | | | | | | | | | | | | | | |\n| | | |RN5O, GradCAM| |RN5O, LIME| | | | | |RNSO, Guided Backpropagation| | | | | | | |\n| | |100|80|60|100|80|60| | | | | | | | | | | |\n| | | | |3|80|3|80|3|80| | | | | | | | | |\n| |1|60|1|60|1|60| | | | | | | | | | | | |\n| |8|40|8|40|8|40| | | | | | | | | | | | |\n| |20|20|20|20|20|20| | | | | | | | | | | | |\n| | | |Accuracy| | | | | | | | | | | | | | | |\n\nFigure E7: Effect of complex conventional probes on accuracy and GridPG scores on ImageNet on (a) B-cos backbones (B-RN50) and (b) conventional backbones (RN50). Interestingly, in contrast to B-cos MLP probes (cf. fig. E6), when conventional MLP probes are used, although there is an improvement in accuracy for all SSL models, there is no consistent improvement in the GridPG localization score.", "mimetype": "text/plain", "start_char_idx": 4147, "end_char_idx": 6586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6dbcc888-9064-42f3-89c5-11485572f074": {"__data__": {"id_": "6dbcc888-9064-42f3-89c5-11485572f074", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fa68b9a-1700-4b2f-9ff7-f97b0b162055", "node_type": "4", "metadata": {}, "hash": "8dca31c19dd3c8d7a5c13381cb9afbab7de23b828f684bf4fae46b0e28ba42b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "366953ba-e3e6-4f02-8e23-356f687fa945", "node_type": "1", "metadata": {}, "hash": "222b37d402280ba4358fab5625332a6a8fa225b6847f05ec8fbcaf27c7b68996", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Interestingly, in contrast to B-cos MLP probes (cf. fig. E6), when conventional MLP probes are used, although there is an improvement in accuracy for all SSL models, there is no consistent improvement in the GridPG localization score. In fact, in several cases there is a drop in localization score as we move from a single probe to conventional MLP probes.\nUnder review as a conference paper at ICLR 2025\n\n| |MoCov2|Bcos: W(x) x Input|BYOL|Bcos: W(x) x Input| |DINO|Bcos: W(x) x Input| | | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |1.0|0.8|0.6|0.4|1.0|0.8|0.6|0.4|1.0|0.8|0.6|0.4| | | | | | | | |\n| |0.2|0.0|<25%|<50%|<75%|<100%| |0.2|0.0|<25%|<50%|<75%|<100%| |0.2|0.0|<25%|<50%|<75%|<100%|\n\n|MoCov2|Integrated Gradients|BYOL|Integrated Gradients|DINO|Integrated Gradients| | | | | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|1.0|0.8|0.6|0.4|1.0|0.8|0.6|0.4|1.0|0.8|0.6|0.4| | | | | | | | | |\n| |0.2|0.0|<25%|<50%|<75%|<100%| |0.2|0.0|<25%|<50%|<75%|<100%| |0.2|0.0|<25%|<50%|<75%|<100%|\n\n| |MoCov2|Input x Gradient|BYOL|Input x Gradient|DINO|Input x Gradient| | | | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |1.0|0.8|0.6|0.4|1.0|0.8|0.6|0.4|1.0|0.8|0.6|0.4| | | | | | | | |\n| |0.2|0.0|<25%|<50%|<75%|<100%| |0.2|0.0|<25%|<50%|<75%|<100%| |0.2|0.0|<25%|<50%|<75%|<100%|\n\n| |MoCov2|GradCAM| |BYOL|GradCAM|DINO|GradCAM| | | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |1.0|0.8|0.6|0.4|1.0|0.8|0.6|0.4|1.0|0.8|0.6|0.4| | | | | | | | |\n| |0.2|0.0|<25%|<50%|<75%|<100%| |0.2|0.0|<25%|<50%|<75%|<100%| |0.2|0.0|<25%|<50%|<75%|<100%|\n\nFigure E8: Bcos-MLP \u2014 EPG scores for Bcos models on ImageNet.", "mimetype": "text/plain", "start_char_idx": 6352, "end_char_idx": 8211, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b5dc589-333e-4822-8d64-e44356622efe": {"__data__": {"id_": "7b5dc589-333e-4822-8d64-e44356622efe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b571e5f2-5732-41c9-ab76-1583a8b95ea5", "node_type": "4", "metadata": {}, "hash": "cb020070bc4c0e7a47c8294fe5217a0f7ca06fb67022bf2f1c99278f252e6167", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a3890db-eb00-484c-81cb-00c4753ccd73", "node_type": "1", "metadata": {}, "hash": "77d6260c9275be877168be8683de376ae61dfdecfb78a913e2622cb9f8ab6c32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Under review as a conference paper at ICLR 2025\n\n| |MoCov2|Integrated Gradients| |BYOL|Integrated Gradients| | |DINO|Integrated Gradients| |\n|---|---|---|---|---|---|---|---|---|---|---|\n|1.0| |MLP-3|MLP-3|MLP-3| | | | | | |\n| |0.8|MLP-2|0.8|MLP-2|0.8|MLP-2| | | | |\n| | |LP|LP|LP| | | | | | |\n| |0.6| |0.6| |0.6| | | | | |\n|2|0.4|0.4|0.4| | | | | | | |\n| |0.2|0.2|0.2| | | | | | | |\n| |0.0|&lt;100%|0.0|250%|0.0|&lt;50%|&lt;100%| | | |\n| |225%|250%|275%| |225%|275%|2100%|225%|275%| |\n| | |BBOX %| |BBOX %| |BBOX %| | | | |\n\n| |MoCov2|Input x Gradient| |BYOL|Input x Gradient|DINO|Input x Gradient| | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|1.0| |MLP-3|MLP-3|MLP-3| | | | | | | | |\n| |0.8|MLP-2|0.8|MLP-2|0.8|MLP-2| | | | | | |\n| | |LP|LP|LP| | | | | | | | |\n| |0.6| |0.6| |0.6| | | | | | | |\n|2|0.4|0.4|0.4| | | | | | | | | |\n| |0.2|0.2|0.2| | | | | | | | | |\n| |0.0|0.0|0.0| | | | | | | | | |\n| |225%|250%|275%|&lt;100%|225%|250%|275%|2100%|225%|&lt;50%|275%|&lt;100%|\n| | |BBOX %| |BBOX %| |BBOX %| | | | | | |\n\n| | |MoCov2|GradCAM|BYOL|GradCAM|DINO|GradCAM| | |\n|---|---|---|---|---|---|---|---|---|---|\n|1.0| |MLP-3|MLP-3|MLP-3| | | | | |\n| |0.8|MLP-2|0.8|MLP-2|0.8|MLP-2| | | |\n| | |LP| |LP| |LP| | | |\n| |0.6| |0.6| |0.6| | | | |\n|2|0.4|0.4|0.4| | | | | | |\n| |0.2|0.2|0.2| | | | | | |\n| |0.0|&lt;50%|0.0|&lt;250%|2100%|0.0|&lt;50%|275%|&lt;100%|\n| |225%|275%| |225%|275%|225%| | | |\n| | |BBOX %| |BBOX %| |BBOX %| | | |\n\nFigure E9: Bcos-MLP \u2014EPG scores for Conventional models on ImageNet.\nUnder review as a conference paper at ICLR 2025\n\n| |MoCov2|Bcos: W(x) x Input| |BYOL|Bcos: W(x) x Input| |DINO|Bcos: W(x) x Input| | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |1.0|1.0|1.0| | | | | | | | | |\n| |0.8|MLP-2|0.8|MLP-2|0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1774, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a3890db-eb00-484c-81cb-00c4753ccd73": {"__data__": {"id_": "3a3890db-eb00-484c-81cb-00c4753ccd73", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b571e5f2-5732-41c9-ab76-1583a8b95ea5", "node_type": "4", "metadata": {}, "hash": "cb020070bc4c0e7a47c8294fe5217a0f7ca06fb67022bf2f1c99278f252e6167", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b5dc589-333e-4822-8d64-e44356622efe", "node_type": "1", "metadata": {}, "hash": "87a40564b35e612b0056e47b8e02d60e7364a490560a74011563f812c83ca144", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7472a06c-aa43-4420-8521-8e601d3ab4fc", "node_type": "1", "metadata": {}, "hash": "c34f1697aca29852aac059aeff0a7976a5169b4a5217b6550f0c9ec9c7ea9213", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Under review as a conference paper at ICLR 2025\n\n| |MoCov2|Bcos: W(x) x Input| |BYOL|Bcos: W(x) x Input| |DINO|Bcos: W(x) x Input| | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |1.0|1.0|1.0| | | | | | | | | |\n| |0.8|MLP-2|0.8|MLP-2|0.8|MLP-2| | | | | | |\n| |0.6|LP|0.6|LP|0.6|LP| | | | | | |\n| |0.4| |0.4| |0.4| | | | | | | |\n| |0.2| |0.2| |0.2| | | | | | | |\n| |0.0| |0.0| |0.0| | | | | | | |\n| |<25%|<50%|<75%|<100%|<25%|<50%|<75%|<100%|<25%|<50%|<75%|<100%|\n| | |BBOX %| |BBOX %| |BBOX %| | | | | | |\n\n|MoCov2|Integrated Gradients|BYOL|Integrated Gradients|DINO|Integrated Gradients| | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|1.0|1.0|1.0| | | | | | | | | |\n|0.8|MLP-2|0.8|MLP-2|0.8|MLP-2| | | | | | |\n|0.6|LP|0.6|LP|0.6|LP| | | | | | |\n|0.4| |0.4| |0.4| | | | | | | |\n|0.2| |0.2| |0.2| | | | | | | |\n|0.0| |0.0| |0.0| | | | | | | |\n|<25%|<50%|<75%|<100%|<25%|<50%|<75%|<100%|<25%|<50%|<75%|<100%|\n| |BBOX %| |BBOX %| |BBOX %| | | | | | |\n\n| |MoCov2|Input x Gradient|BYOL|Input x Gradient|DINO|Input x Gradient| | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |1.0|1.0|1.0| | | | | | | | | |\n| |0.8|MLP-2|0.8|MLP-2|0.8|MLP-2| | | | | | |\n| |0.6|LP|0.6|LP|0.6|LP| | | | | | |\n| |0.4| |0.4| |0.4| | | | | | | |\n| |0.2| |0.2| |0.2| | | | | | | |\n| |0.0| |0.0| |0.0| | | | | | | |\n| |<25%|<50%|<75%|<100%|<25%|<50%|<75%|<100%|<25%|<50%|<75%|<100%|\n| | |BBOX %| |BBOX %| |BBOX %| | | | | | |\n\n| |MoCov2|GradCAM|BYOL|GradCAM|DINO|GradCAM| | | | |\n|---|---|---|---|---|---|---|---|---|---|---|\n| |1.0|1.0|1.0| | | | | | | |\n| |0.8|MLP-2|0.8|MLP-2|0.8|MLP-2| | | | |\n| |0.6|LP|0.6|LP|0.6|LP| | | | |\n| |0.4| |0.4| |0.4| | | | | |\n| |0.", "mimetype": "text/plain", "start_char_idx": 1522, "end_char_idx": 3210, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7472a06c-aa43-4420-8521-8e601d3ab4fc": {"__data__": {"id_": "7472a06c-aa43-4420-8521-8e601d3ab4fc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b571e5f2-5732-41c9-ab76-1583a8b95ea5", "node_type": "4", "metadata": {}, "hash": "cb020070bc4c0e7a47c8294fe5217a0f7ca06fb67022bf2f1c99278f252e6167", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a3890db-eb00-484c-81cb-00c4753ccd73", "node_type": "1", "metadata": {}, "hash": "77d6260c9275be877168be8683de376ae61dfdecfb78a913e2622cb9f8ab6c32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93199c91-5086-488e-91f4-f208a2030ee8", "node_type": "1", "metadata": {}, "hash": "f1736cbdcaee1195ddae59b7558a46d270e4fd89ecfc93337d65694fe0461349", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0|1.0|1.0| | | | | | | |\n| |0.8|MLP-2|0.8|MLP-2|0.8|MLP-2| | | | |\n| |0.6|LP|0.6|LP|0.6|LP| | | | |\n| |0.4| |0.4| |0.4| | | | | |\n| |0.2| |0.2| |0.2| | | | | |\n| |0.0| | |<75%|0.0|<75%|0.0| | | |\n| |<25%|<50%|<100%|<25%|<50%|<100%|<25%|<50%|<75%|<100%|\n| | |BBOX %| |BBOX %| |BBOX %| | | | |\n\nFigure E10: Conventional-MLP \u2014 EPG for B-cos models on ImageNet.\nUnder review as a conference paper at ICLR 2025\n\n|MoCov2| | | |Layerwise Relevance Propagation| | |BYOL| | | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | | | | | | | | | | | |1.0|1.0|1.0| | | |\n| | | | |MLP-3|MLP-3|MLP-3| | | | | | | | | | | | | | |\n| | |0.8|0.8|0.8| | | | | | | | | | | | | | | | |\n| | | | |MLP-2|MLP-2|MLP-2| | | | | | | | | | | | | | |\n| |0.6|0.6|0.6| | | | | | | | | | | | | | | | | |\n| |0.4|0.4|0.4| | | | | | | | | | | | | | | | | |\n| | |0.2|0.2|0.2| | | | | | | | | | | | | | | | |\n| | |0.0|0.0|0.0| | | | | | | | | | | | | | | | |\n|&lt;25%|&lt;50%|&lt;75%|&lt;100%|&lt;25%|&lt;50%| | | | | | |275%|&lt;100%|&lt;25%|&lt;50%|275%|&lt;100%| | | |\n| | |MoCov2| | |Integrated Gradients| | |BYOL|Integrated Gradients| | | |DINO|Integrated Gradients| | | | | | |\n| | | | | | | | | | | | | | | |1.0|1.0|1.0|1.0|1.0|1.0|\n| | | | |MLP-3|MLP-3|MLP-3|MLP-3|MLP-3|MLP-3| | | | | | | | | | | |\n| | |0.8|0.8|0.8|0.8|0.8|0.8| | | | | | | | | | | | | |\n| | | | |LP|LP|LP|LP|LP|LP| | | | | | | | | | | |\n| |0.6|0.6|0.6|0.6|0.6|0.6| | | | | | | | | | | | | | |\n| |0.4|0.4|0.4|0.4|0.4|0.4| | | | | | | | | | | | | | |\n| | |0.2|0.2|0.2|0.2|0.2|0.2| | | | | | | | | | | | | |\n| | |0.0|0.0|0.0|0.0|0.0|0.0| | | | | | | | | | | | | |\n|&lt;25%|&lt;50%|&lt;75%|&lt;", "mimetype": "text/plain", "start_char_idx": 3075, "end_char_idx": 4770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93199c91-5086-488e-91f4-f208a2030ee8": {"__data__": {"id_": "93199c91-5086-488e-91f4-f208a2030ee8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b571e5f2-5732-41c9-ab76-1583a8b95ea5", "node_type": "4", "metadata": {}, "hash": "cb020070bc4c0e7a47c8294fe5217a0f7ca06fb67022bf2f1c99278f252e6167", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7472a06c-aa43-4420-8521-8e601d3ab4fc", "node_type": "1", "metadata": {}, "hash": "c34f1697aca29852aac059aeff0a7976a5169b4a5217b6550f0c9ec9c7ea9213", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "291d4b2a-f2da-4d90-b1e8-6f43d4814703", "node_type": "1", "metadata": {}, "hash": "cec8b04bd1564653d2e340b579aab9aa300fcd2cc5d5a13a52d8b7dee62eff49", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6|0.6|0.6|0.6|0.6|0.6| | | | | | | | | | | | | | |\n| |0.4|0.4|0.4|0.4|0.4|0.4| | | | | | | | | | | | | | |\n| | |0.2|0.2|0.2|0.2|0.2|0.2| | | | | | | | | | | | | |\n| | |0.0|0.0|0.0|0.0|0.0|0.0| | | | | | | | | | | | | |\n|&lt;25%|&lt;50%|&lt;75%|&lt;100%|&lt;25%|&lt;50%| | | | | | |275%|&lt;100%|&lt;25%|&lt;50%|&lt;75%|&lt;100%| | | |\n| | | |MoCov2| |Input x Gradient| | | |BYOL|Input x Gradient| | |DINO|Input x Gradient| | | | | | |\n| | | | | | | | | | | | | | | |1.0|1.0|1.0|1.0|1.0|1.0|\n| | | | |MLP-3|MLP-3|MLP-3|MLP-3|MLP-3|MLP-3| | | | | | | | | | | |\n| | |0.8|0.8|0.8|0.8|0.8|0.8| | | | | | | | | | | | | |\n| | | | |LP|LP|LP|LP|LP|LP| | | | | | | | | | | |\n| |0.6|0.6|0.6|0.6|0.6|0.6| | | | | | | | | | | | | | |\n| |0.4|0.4|0.4|0.4|0.4|0.4| | | | | | | | | | | | | | |\n| | |0.2|0.2|0.2|0.2|0.2|0.2| | | | | | | | | | | | | |\n| | |0.0|0.0|0.0|0.0|0.0|0.0| | | | | | | | | | | | | |\n|&lt;25%|&lt;50%|&lt;75%|&lt;100%|&lt;25%|&lt;50%| | | | | | |275%|&lt;100%|&lt;25%|&lt;50%|&lt;75%|&lt;100%| | | |\n| | | |MoCov2| |GradCAM| | | | |BYOL|GradCAM| | |DINO|GradCAM| | | | | |\n| | | | | | | | | | | | | | | |1.0|1.0|1.0|1.0|1.0|1.0|\n| | | | |MLP-3|MLP-3|MLP-3|MLP-3|MLP-3|MLP-3| | | | | | | | | | | |\n| | |0.8|0.8|0.8|0.8|0.8|0.8| | | | | | | | | | | | | |\n| | | | |MLP-2|MLP-2|MLP-2|MLP-2|MLP-2|MLP-2| | | | | | | | | | | |\n| |0.6|0.6|0.6|0.6|0.6|0.6| | | | | | | | | | | | | | |\n| |0.4|0.4|0.4|0.4|0.4|0.4| | | | | | | | | | | | | | |\n| | |0.2|0.2|0.2|0.2|0.2|0.2| | | | | | | | | | | | | |\n| | |0.0|0.0|0.0|0.0|0.0|0.", "mimetype": "text/plain", "start_char_idx": 4522, "end_char_idx": 6043, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "291d4b2a-f2da-4d90-b1e8-6f43d4814703": {"__data__": {"id_": "291d4b2a-f2da-4d90-b1e8-6f43d4814703", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b571e5f2-5732-41c9-ab76-1583a8b95ea5", "node_type": "4", "metadata": {}, "hash": "cb020070bc4c0e7a47c8294fe5217a0f7ca06fb67022bf2f1c99278f252e6167", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93199c91-5086-488e-91f4-f208a2030ee8", "node_type": "1", "metadata": {}, "hash": "f1736cbdcaee1195ddae59b7558a46d270e4fd89ecfc93337d65694fe0461349", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6|0.6|0.6|0.6|0.6|0.6| | | | | | | | | | | | | | |\n| |0.4|0.4|0.4|0.4|0.4|0.4| | | | | | | | | | | | | | |\n| | |0.2|0.2|0.2|0.2|0.2|0.2| | | | | | | | | | | | | |\n| | |0.0|0.0|0.0|0.0|0.0|0.0| | | | | | | | | | | | | |\n|&lt;25%|&lt;50%|&lt;75%|&lt;100%|&lt;25%|&lt;50%| | | | | | |275%|&lt;100%|&lt;25%|&lt;50%|&lt;75%|&lt;100%| | | |\n\nFigure E11: Conventional-MLP \u2014 EPG for Conventional models on ImageNet.", "mimetype": "text/plain", "start_char_idx": 5853, "end_char_idx": 6260, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d89490a8-9cce-49fe-a837-a721281cca2e": {"__data__": {"id_": "d89490a8-9cce-49fe-a837-a721281cca2e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "773594f0-5d4a-49ab-9516-d152f7991587", "node_type": "4", "metadata": {}, "hash": "a8b463b4a6524da51231a2f3fe1208b92efc60ddded73fd9ea9f27d1c715ebf6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75668d34-9b3b-49a2-a6a2-2afe99aecacd", "node_type": "1", "metadata": {}, "hash": "ca1f7ee8b9f6c70219d0748e6865988aeef63ec3e1776136c7af3ac1e3855809", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Under review as a conference paper at ICLR 2025\n\n| |MoCov2|Bcos: W(x) x Input| | | |BYOL|Bcos: W(x) x Input| | | |DINO|Bcos: W(x) x Input| | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|[ 0.8| | | | |MLP-3|MLP-2|0.8|0.8| | | | | | | | |\n| |0.6| |0.6| |0.6| | | | | | | | | | | |\n|0.4| | | | | | |MLP-3| |MLP-3| | | | | | | |\n|0.2| | | | | |MLP-2| |MLP-2| | | | | | | | |\n| | | | | |LP| |LP| |LP| | | | | | | |\n| | |5|10|15|20| | | | | | | | | | | |\n| | | | | | | |% of pixels removed|% of pixels removed|% of pixels removed| | | | | | | |\n\n| |MoCov2|Integrated Gradients| | | |BYOL|Integrated Gradients| | | |DINO|Integrated Gradients| | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|[ 0.8| | | | |MLP-3|MLP-2|0.8|0.8| | | | | | | | |\n| |0.6| |0.6| |0.6| | | | | | | | | | | |\n|0.4| | | | | | |MLP-3| |MLP-3| | | | | | | |\n|0.2| | | | | |MLP-2| |MLP-2| | | | | | | | |\n| | | | | |LP| |LP| |LP| | | | | | | |\n| | |5|10|15|20| | | | | | | | | | | |\n| | | | | | | |% of pixels removed|% of pixels removed|% of pixels removed| | | | | | | |\n\n| |MoCov2| |Input x Gradient| | |BYOL|Input x Gradient| | | |DINO|Input x Gradient| | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|[ 0.8| | | | |MLP-3|MLP-2|0.8|0.8| | | | | | | | |\n| |0.6| |0.6| |0.6| | | | | | | | | | | |\n|0.4| | | | | | |MLP-3| |MLP-3| | | | | | | |\n|0.2| | | | | |MLP-2| |MLP-2| | | | | | | | |\n| | | | | |LP| |LP| |LP| | | | | | | |\n| | |5|10|15|20| | | | | | | | | | | |\n| | | | | | | |% of pixels removed|% of pixels removed|% of pixels removed| | | | | | | |\n\n| | |MoCov2|GradCAM| | | |BYOL|GradCAM| | | |DINO|GradCAM| | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|[ 0.8|0.8|0.8| | | | | | | | | | | | | | |\n| |0.6|0.6|0.6| | | | | | | | | | | | | |\n|0.4| | | | | | |MLP-3|0.4|MLP-3|0.4|MLP-3| | | | | |\n|0.2| |LP| | | |MLP-2|0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1917, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75668d34-9b3b-49a2-a6a2-2afe99aecacd": {"__data__": {"id_": "75668d34-9b3b-49a2-a6a2-2afe99aecacd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "773594f0-5d4a-49ab-9516-d152f7991587", "node_type": "4", "metadata": {}, "hash": "a8b463b4a6524da51231a2f3fe1208b92efc60ddded73fd9ea9f27d1c715ebf6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d89490a8-9cce-49fe-a837-a721281cca2e", "node_type": "1", "metadata": {}, "hash": "d07f5a968c820fa66277c8d53e93939d8ba5c45334973a356a9a9067683cb0b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6022b8c-d37a-4c3f-82e7-1d1710f6f149", "node_type": "1", "metadata": {}, "hash": "d7b0ad5224c831dc4e8bb05e565f994a468f9d10fb77cd22649a278c966aa9b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8|0.8|0.8| | | | | | | | | | | | | | |\n| |0.6|0.6|0.6| | | | | | | | | | | | | |\n|0.4| | | | | | |MLP-3|0.4|MLP-3|0.4|MLP-3| | | | | |\n|0.2| |LP| | | |MLP-2|0.2|LP|MLP-2| | | | | | | |\n| | | | | |LP| |LP| |LP| | | | | | | |\n| | |5|10|15|20| | | | | | | | | | | |\n| | | | | | | |% of pixels removed|% of pixels removed|% of pixels removed| | | | | | | |\n\nFigure E12: Bcos-MLP \u2014 Pixel deletion scores for Bcos models on ImageNet.", "mimetype": "text/plain", "start_char_idx": 1758, "end_char_idx": 2185, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6022b8c-d37a-4c3f-82e7-1d1710f6f149": {"__data__": {"id_": "c6022b8c-d37a-4c3f-82e7-1d1710f6f149", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "773594f0-5d4a-49ab-9516-d152f7991587", "node_type": "4", "metadata": {}, "hash": "a8b463b4a6524da51231a2f3fe1208b92efc60ddded73fd9ea9f27d1c715ebf6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75668d34-9b3b-49a2-a6a2-2afe99aecacd", "node_type": "1", "metadata": {}, "hash": "ca1f7ee8b9f6c70219d0748e6865988aeef63ec3e1776136c7af3ac1e3855809", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0add8097-61e2-4b6a-9211-8efa93a89164", "node_type": "1", "metadata": {}, "hash": "8c5429a48198039ce19c7a4877f616cb90da8b9d806479b68da94c14d06e8ff0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6|0.6|0.6| | | | | | | | | | | | | |\n|0.4| | | | | | |MLP-3|0.4|MLP-3|0.4|MLP-3| | | | | |\n|0.2| |LP| | | |MLP-2|0.2|LP|MLP-2| | | | | | | |\n| | | | | |LP| |LP| |LP| | | | | | | |\n| | |5|10|15|20| | | | | | | | | | | |\n| | | | | | | |% of pixels removed|% of pixels removed|% of pixels removed| | | | | | | |\n\nFigure E12: Bcos-MLP \u2014 Pixel deletion scores for Bcos models on ImageNet.\n\n| |MoCov2|Integrated Gradients| | | |BYOL|Integrated Gradients| | | |DINO|Integrated Gradients| | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|[ 0.8| | | | |MLP-3|MLP-2|0.8|0.8| | | | | | | | |\n| |0.6| |0.6| |0.6| | | | | | | | | | | |\n|0.4| | | | | | |MLP-3| |MLP-3| | | | | | | |\n|0.2| | | | | |MLP-2| |MLP-2| | | | | | | | |\n| | | | | |LP| |LP| |LP| | | | | | | |\n| | |5|10|15|20| | | | | | | | | | | |\n| | | | | | | |% of pixels removed|% of pixels removed|% of pixels removed| | | | | | | |\n\n| |MoCov2| |Input x Gradient| | |BYOL|Input x Gradient| | | |DINO|Input x Gradient| | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|[ 0.8| | | | |MLP-3|MLP-2|0.8|0.8| | | | | | | | |\n| |0.6| |0.6| |0.6| | | | | | | | | | | |\n|0.4| | | | | | |MLP-3| |MLP-3| | | | | | | |\n|0.2| | | | | |MLP-2| |MLP-2| | | | | | | | |\n| | | | | |LP| |LP| |LP| | | | | | | |\n| | |5|10|15|20| | | | | | | | | | | |\n| | | | | | | |% of pixels removed|% of pixels removed|% of pixels removed| | | | | | | |\n\n| | |MoCov2|GradCAM| | | |BYOL|GradCAM| | | |DINO|GradCAM| | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|[ 0.8|0.8|0.8| | | | | | | | | | | | | | |\n| |0.6|0.6|0.6| | | | | | | | | | | | | |\n|0.4| | | | | | |MLP-3|0.4|MLP-3|0.4|MLP-3| | | | | |\n|0.2| |LP| | | |MLP-2|0.2|LP|MLP-2| | | | | | | |\n| | | | | |LP| |LP| |LP| | | | | | | |\n| | |5|10|15|20| | | | | | | | | | | |\n| | | | | | | |% of pixels removed|% of pixels removed|% of pixels removed| | | | | | | |\n\nFigure E13: Bcos-MLP \u2014 Pixel deletion scores for Conventional models on ImageNet.", "mimetype": "text/plain", "start_char_idx": 1802, "end_char_idx": 3808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0add8097-61e2-4b6a-9211-8efa93a89164": {"__data__": {"id_": "0add8097-61e2-4b6a-9211-8efa93a89164", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "773594f0-5d4a-49ab-9516-d152f7991587", "node_type": "4", "metadata": {}, "hash": "a8b463b4a6524da51231a2f3fe1208b92efc60ddded73fd9ea9f27d1c715ebf6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6022b8c-d37a-4c3f-82e7-1d1710f6f149", "node_type": "1", "metadata": {}, "hash": "d7b0ad5224c831dc4e8bb05e565f994a468f9d10fb77cd22649a278c966aa9b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3716fe1d-b48d-4ac3-9b5f-3fa4cebeae3c", "node_type": "1", "metadata": {}, "hash": "4b618bc00a9390919c7ec464f0050c0af835f1623f1c87afbdb5e36a979a0318", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Under review as a conference paper at ICLR 2025\n\n|MoCov2| |Layerwise Relevance Propagation|BYOL|Layerwise Relevance Propagation|DINO|Layerwise Relevance Propagation|\n|---|---|---|---|---|---|---|\n|MLP-3| |MLP-2|MLP-2| | | |\n|0.8|0.8|0.6|0.4|0.4|0.4| |\n| |1|0.2| | | | |\n|5 10 15 20 % of pixels removed| | | | | | |\n\n| |MoCov2| |Integrated Gradients| |BYOL|Integrated Gradients|DINO|Integrated Gradients|\n|---|---|---|---|---|---|---|---|---|\n|MLP-3| | | |MLP-2|MLP-2| | | |\n|0.8|0.8|0.6|0.4|0.4|0.4| | | |\n| |1|0.2| | | | | | |\n|10 15 20 % of pixels removed| | | | | | | | |\n\n| |MoCov2| |Input x Gradient|BYOL|Input x Gradient|DINO|Input x Gradient|\n|---|---|---|---|---|---|---|---|\n|MLP-3| | |MLP-2|MLP-2| | | |\n|0.8|0.8|0.6|0.4|0.4|0.4| | |\n| |1|0.2| | | | | |\n|5 10 15 20 % of pixels removed| | | | | | | |\n\n| |MoCov2|GradCAM|BYOL|GradCAM|DINO|GradCAM|\n|---|---|---|---|---|---|---|\n|MLP-3| |MLP-2|MLP-2| | | |\n|0.8|0.8|0.6|0.4|0.4|0.4| |\n| |1|0.2| | | | |\n|5 10 15 20 % of pixels removed| | | | | | |\n\nFigure E14: Conventional-MLP \u2014 Pixel deletion scores for Conventional models on ImageNet.\n# Under review as a conference paper at ICLR 2025\n\n# Evaluation on Multi-label Classification\n\nWe also evaluate all SSL models (both B-cos and conventional backbones) and explanation methods under the multi-label classification setting. We train single probes and MLP probes using the BCE loss (as is typical) on COCO and VOC datasets. To measure model performance we report f1-score and explanation localization we report the EPG score. Figures E15, E16 show the f1-score vs EPG score plots for models probed with B-cos MLPs and conventional MLPs respectively on COCO. Here we notice, that stronger MLPs result in an improvement in both classification performance (depicted by an increase in f1-score), and also improved localization EPG scores.", "mimetype": "text/plain", "start_char_idx": 3809, "end_char_idx": 5654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3716fe1d-b48d-4ac3-9b5f-3fa4cebeae3c": {"__data__": {"id_": "3716fe1d-b48d-4ac3-9b5f-3fa4cebeae3c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "773594f0-5d4a-49ab-9516-d152f7991587", "node_type": "4", "metadata": {}, "hash": "a8b463b4a6524da51231a2f3fe1208b92efc60ddded73fd9ea9f27d1c715ebf6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0add8097-61e2-4b6a-9211-8efa93a89164", "node_type": "1", "metadata": {}, "hash": "8c5429a48198039ce19c7a4877f616cb90da8b9d806479b68da94c14d06e8ff0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbfcbde5-0704-4928-93c6-e0f9dc9e7700", "node_type": "1", "metadata": {}, "hash": "0a0987beea26ccb20bc2de47b512d4088f9b0b2ed5b2e586529378da658bc766", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Under review as a conference paper at ICLR 2025\n\n# Evaluation on Multi-label Classification\n\nWe also evaluate all SSL models (both B-cos and conventional backbones) and explanation methods under the multi-label classification setting. We train single probes and MLP probes using the BCE loss (as is typical) on COCO and VOC datasets. To measure model performance we report f1-score and explanation localization we report the EPG score. Figures E15, E16 show the f1-score vs EPG score plots for models probed with B-cos MLPs and conventional MLPs respectively on COCO. Here we notice, that stronger MLPs result in an improvement in both classification performance (depicted by an increase in f1-score), and also improved localization EPG scores.\n\n| | |B-RNSO, Bcos: W(x) x Input| |B-RNSO, Integrated Gradients| | | |B-RN5O, Input x Gradient|F1 Score| | | | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |50|55|60|65|70|75|80| | | | | | | | | | | | | | | | |\n|60|MOCOV2|MOCOV2| | | | |BYOL|BYOL|BYOL|DINO|DINO| | | | | | | | | | | | |\n|50| | | | | | |DINO|SUP|SUP|SUP|SUP|SUP| | | | | | | | | | | |\n| |40|1|g|30|20|10| | | | | | | | | | | | | | | | | |\n\n|B-RN5O, Guided Backpropagation| |B-RN5O, GradCAM| | | | |B-RN5O, LIME|F1 Score| | | | | | | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |50|55|60|65|70|75|80| | | | | | | | | | | | | | | | | | |\n|60|MOCOV2|MOCOV2| | | | | | |BYOL|BYOL|BYOL|DINO|DINO| | | | | | | | | | | | |\n|50| | | | | | | | |DINO|SUP|SUP|SUP|SUP|SUP| | | | | | | | | | | |\n| |40|1|g|30|20|10| | | | | | | | | | | | | | | | | | | |\n\n# Figure E15\n\nEffect of complex B-cos probes on accuracy and EPG scores on COCO on (a) B-cos backbones (B-RN50) and (b) conventional backbones (RN50). When we train with B-cos MLP probes as compared to a single probe, we observe an improvement in both f1-score (x-axis) and EPG localization score (y-axis) for all SSL models and explanation methods, except for GradCAM where improvement in localization is not seen consistently.", "mimetype": "text/plain", "start_char_idx": 4908, "end_char_idx": 7055, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bbfcbde5-0704-4928-93c6-e0f9dc9e7700": {"__data__": {"id_": "bbfcbde5-0704-4928-93c6-e0f9dc9e7700", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "773594f0-5d4a-49ab-9516-d152f7991587", "node_type": "4", "metadata": {}, "hash": "a8b463b4a6524da51231a2f3fe1208b92efc60ddded73fd9ea9f27d1c715ebf6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3716fe1d-b48d-4ac3-9b5f-3fa4cebeae3c", "node_type": "1", "metadata": {}, "hash": "4b618bc00a9390919c7ec464f0050c0af835f1623f1c87afbdb5e36a979a0318", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When we train with B-cos MLP probes as compared to a single probe, we observe an improvement in both f1-score (x-axis) and EPG localization score (y-axis) for all SSL models and explanation methods, except for GradCAM where improvement in localization is not seen consistently.\n# Under review as a conference paper at ICLR 2025\n\n| | | | |B-RNSO, Bcos: W(x) x Input| | | | |B-RNSO, Integrated Gradients| | | | | |B-RN5O, Input x Gradient| | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|MOCOV2| | | | | | | | | | | | | | | | | | | |BYOL|DINO|SUP|MOCOV2|BYOL|\n| | | | | |60|50|40|30|20|10| | | | | | | | | | | | | | |\n| | | | | |Fl Score| | | | | | | | | | | | | | | | | | | |\n\n| | | | |B-RN5O, GradCAM| | | | | | |B-RN5O, LIME| | | | |B-RNSO, Guided Backpropagation| | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|MOCOV2| | | | | | | | | | | | | | | | | | | | | |BYOL|DINO|SUP|MOCOV2|BYOL|\n| | | | |60|50|40|30|20|10| | | | | | | | | | | | | | | | | |\n| | | | | |Fl Score| | | | | | | | | | | | | | | | | | | | | |\n\n# Figure E16: Effect of complex conventional probes on accuracy and EPG scores on COCO\n\n(a) B-cos backbones (B-RN50) and (b) conventional backbones (RN50). Similar to fig. E15, when training with conventional MLP probes as compared to a single probe, we observe an improvement in both f1-score (x-axis) and EPG localization score (y-axis) for all SSL models and explanation methods, except for GradCAM (bottom-left, subplots a,b) where improvement in localization is not seen consistently.\nUnder review as a conference paper at ICLR 2025\n\nLikewise, Figures E17, E18 show the corresponding plots for VOC for B-cos MLPs and conventional MLPs respectively. We see a similar trend as we saw on COCO, complex (two- and three-layer MLPs) probes lead to an increased f1-score and EPG score. Interestingly, we notice that GradCAM shows inconsistent behaviour, especially in the case of conventional MLPs (see Figures E16 and E18). This is a well-known limitation for GradCAM, which has been shown to perform poorly (in terms of localization) at earlier layers (Jiang et al., 2021). Note: For B-cos MLPs we omit showing the results for LRP explanations as the relevance propagation rules for B-cos layers are not defined.", "mimetype": "text/plain", "start_char_idx": 6778, "end_char_idx": 9162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02c28097-932c-433a-9fd4-f4ffc87a4941": {"__data__": {"id_": "02c28097-932c-433a-9fd4-f4ffc87a4941", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37589ad1-7249-4c95-919f-c59b67b41ace", "node_type": "4", "metadata": {}, "hash": "0012c781a1e0ca3bd3b19552bc3a6d6bca56431b3bdc9eb8037a669082f0493d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a3de1ee-dde0-473b-b211-a95aedda4abd", "node_type": "1", "metadata": {}, "hash": "11606d782fb94972326bc709488dabcdb0941e2df699b2ee4ff7049c3e5a5c1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Interestingly, we notice that GradCAM shows inconsistent behaviour, especially in the case of conventional MLPs (see Figures E16 and E18). This is a well-known limitation for GradCAM, which has been shown to perform poorly (in terms of localization) at earlier layers (Jiang et al., 2021). Note: For B-cos MLPs we omit showing the results for LRP explanations as the relevance propagation rules for B-cos layers are not defined.\n\n| |B-RNSO, Attribution: Input X Gradient|MOCOV2|BYOL|DINO|SUP| | |\n|---|---|---|---|---|---|---|---|\n|80|70|60|50|40|30|20|10|\n|B-RNSO, Attribution: Bcos: W(x) x Input|MOCOV2|BYOL|DINO|SUP| | | |\n|80|70|60|50|40|30|20|10|\n| |B-RNSO, Attribution: Integrated Gradients|MOCOV2|BYOL|DINO|SUP| | |\n|80|70|60|50|40|30|20|10|\n\n|B-RNSO, Attribution: GradCAM| |MOCOV2|BYOL|DINO|SUP| | |\n|---|---|---|---|---|---|---|---|\n|80|70|60|50|40|30|20|10|\n\nFigure E17: Effect of complex B-cos probes on accuracy and EPG scores on VOC on (a) B-cos backbones and (b) conventional backbones. Notice as seen for COCO (cf. fig. E15), stronger two- and three-layer MLPs lead to an improvement in both f1-score (x-axis) and EPG localization score (y-axis). Once again, for GradCAM this is not very consistent. Also note that for B-cos models trained with MoCov2, B-cos explanations (top-left, subplot a) for MLP probes perform worse in EPG-score as compared to a single probe.\nUnder review as a conference paper at ICLR 2025\n\n|B-RNSO, Attribution: Bcos: W(x) x Input| | | | | | | |MOCOV2| | | | | | | | | | | | |BYOL|DINO|SUP| | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |80|70|60|80|70|60|80|70|60|80|70|60| | | | | | | | | | | | | |\n| | | | | | | | | | | | | | |1|50|40|30|20|10|1|50|40|30|20|10|\n\n|B-RNSO, Attribution: GradCAM| | | | | | | |MOCOV2| | | | | | | | | | | | |BYOL|DINO|SUP| | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |80|70|60|80|70|60|80|70|60|80|70|60| | | | | | | | | | | | | |\n| | | | | | | | | | | | | | |1|50|40|30|20|10|1|50|40|30|20|10|\n\nFigure E18: Effect of complex conventional probes on accuracy and EPG scores on VOC on (a) B-cos backbones and (b) conventional backbones. Even conventional two- and three-layer MLPs lead to an improvement in both f1-score (x-axis) and EPG localization score (y-axis). Yet again, for GradCAM this is not consistent.\n# IMPLEMENTATION DETAILS\n\nIn this section we describe in detail our experimental setting, i.e. the datasets we train and evaluate on, the training and implementation details, and finally the evaluation metrics used.\n\n# F.1 DATASETS\n\nFor our experiments we use three datasets namely ImageNet (Russakovsky et al., 2015), VOC 2007 (Everingham et al.) and MS COCO 2014 (Lin et al., 2014).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2804, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4a3de1ee-dde0-473b-b211-a95aedda4abd": {"__data__": {"id_": "4a3de1ee-dde0-473b-b211-a95aedda4abd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37589ad1-7249-4c95-919f-c59b67b41ace", "node_type": "4", "metadata": {}, "hash": "0012c781a1e0ca3bd3b19552bc3a6d6bca56431b3bdc9eb8037a669082f0493d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02c28097-932c-433a-9fd4-f4ffc87a4941", "node_type": "1", "metadata": {}, "hash": "0e404aa9a046ca58c37069766f4186afe514ca7feccb321bf69a673756f475dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "895c1b36-0277-45e4-a7d4-8d16e9d06503", "node_type": "1", "metadata": {}, "hash": "2a458f3e1cab848ee12251c2079ee3afc3551ea316583b826db4f1f8f8798731", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Even conventional two- and three-layer MLPs lead to an improvement in both f1-score (x-axis) and EPG localization score (y-axis). Yet again, for GradCAM this is not consistent.\n# IMPLEMENTATION DETAILS\n\nIn this section we describe in detail our experimental setting, i.e. the datasets we train and evaluate on, the training and implementation details, and finally the evaluation metrics used.\n\n# F.1 DATASETS\n\nFor our experiments we use three datasets namely ImageNet (Russakovsky et al., 2015), VOC 2007 (Everingham et al.) and MS COCO 2014 (Lin et al., 2014).\n\n# ImageNet\n\nWe use the ImageNet-1K dataset that is part of the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2015). This has 1000 classes, with roughly 1000 images belonging to each category. In total, there are 1,281,167 training images, and 50,000 validation images. We perform all self-supervised pre-training on the training set, and evaluation on the validation set. For training, the images are resized to an input resolution of 224\u00d7224. For measuring the performance of the model the top-1 accuracy (proportion of correctly classified samples) is reported, and to evaluate model explanations the grid pointing game (GridPG) localization score is reported (discussed in more detail below).\n\n# VOC 2007\n\nVOC 2007 (Everingham et al.) is a popularly used multi-label image classification dataset. It comprises of 9,963 images in total and 20 object classes, and is split into the train-val set with 5,011 images and the test set with 4,952 images. We use the train-val set for training and test set for evaluation. For training, the images are resized to a fixed resolution of 224\u00d7224. As is typical (Rao et al., 2023b) for evaluating multi-label classification datasets, we report the f1-score and use the energy pointing game (EPG) score to evaluate the model explanations.\n\n# MS COCO 2014\n\nMicrosoft COCO (Lin et al., 2014) is another popular dataset generally used for image classification, segmentation, object detection and captioning tasks. We use COCO-2014 in our experiments, that has 82,081 training and 40,137 validation images and 80 object classes. For our training, we resize the images to a fixed size of 448\u00d7448. Similar to VOC for evaluation we report the f1-score and use the EPG score to evaluate model explanations. Also note that the variation of objects\u2019 shapes and sizes are more complicated (Zhu & Wu, 2021) on COCO than those in VOC, and it is also substantially larger with more object classes.\n\nIn short, we evaluate the models on (1) single-label and (2) multi-label classification. For (1), we train probes on top of the frozen pre-trained features on ImageNet (Russakovsky et al., 2015), and report top-1 accuracy on the validation set. For (2), the probes are trained to predict all classes that are present in an image; for this, we use VOC (Everingham et al.) and COCO (Lin et al., 2014) and report the F1 scores on the test set.\n\n# F.2 MODELS\n\nWe evaluate both conventional (ResNet-50, ViT-B/16) and the recently proposed inherently interpretable B-cos models (B-cos ResNet-50, ViTc-S/16, ViTc-B/16). To adapt these to the various pre-training paradigms, we follow (B\u00f6hle et al., 2023) in converting the MLP heads of the respective SSL methods. Specifically, we replace the linear layers in the MLP projection heads (MoCov2, BYOL, DINO) and the prediction head (BYOL) with corresponding B-cos layers (B=2), remove all ReLU non-linearities, and replace all batch normalization layers with the corresponding uncentered version, see (B\u00f6hle et al., 2023).\n\n# F.3 PRE-TRAINING FRAMEWORKS\n\nAs discussed previously, we aim to have a broad enough representative set that highlights how our evaluation generalizes across differently trained feature extractors, particularly (1) fully-supervised, (2) self-supervised, and (3) contrastive vision-language learning.\n\n(1) Fully-Supervised Learning We first, evaluate the explanation methods on fully supervised backbones.", "mimetype": "text/plain", "start_char_idx": 2243, "end_char_idx": 6235, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "895c1b36-0277-45e4-a7d4-8d16e9d06503": {"__data__": {"id_": "895c1b36-0277-45e4-a7d4-8d16e9d06503", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37589ad1-7249-4c95-919f-c59b67b41ace", "node_type": "4", "metadata": {}, "hash": "0012c781a1e0ca3bd3b19552bc3a6d6bca56431b3bdc9eb8037a669082f0493d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a3de1ee-dde0-473b-b211-a95aedda4abd", "node_type": "1", "metadata": {}, "hash": "11606d782fb94972326bc709488dabcdb0941e2df699b2ee4ff7049c3e5a5c1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c993343f-0b07-4e8f-bc46-1c1d8066c7c1", "node_type": "1", "metadata": {}, "hash": "269037bd1738e03c66430608102b6e8657110cbf2ed136cc3c721009c60311e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Specifically, we replace the linear layers in the MLP projection heads (MoCov2, BYOL, DINO) and the prediction head (BYOL) with corresponding B-cos layers (B=2), remove all ReLU non-linearities, and replace all batch normalization layers with the corresponding uncentered version, see (B\u00f6hle et al., 2023).\n\n# F.3 PRE-TRAINING FRAMEWORKS\n\nAs discussed previously, we aim to have a broad enough representative set that highlights how our evaluation generalizes across differently trained feature extractors, particularly (1) fully-supervised, (2) self-supervised, and (3) contrastive vision-language learning.\n\n(1) Fully-Supervised Learning We first, evaluate the explanation methods on fully supervised backbones. Since, the backbones pre-trained in a supervised manner are still often used for transfer learning (Cheng et al., 2022; Xie et al., 2021; Chen et al., 2017). Additionally, an evaluation of fully supervised models also provides a useful reference value, as most explanation methods have been developed in this context.\n# Under review as a conference paper at ICLR 2025\n\nIn addition to evaluating the end-to-end trained classifiers, we also evaluate linear probes on the frozen representations of these models, in order to increase the comparability to the self-supervised approaches we present in the following.\n\n# (2) Self-Supervised Learning.\n\nWe consider three popular self-supervised pretraining frameworks\u2014MoCov2 (Chen et al., 2020c), BYOL (Grill et al., 2020), and DINO (Caron et al., 2021)\u2014regarding their interpretability. In the following, we briefly describe each of them.\n\nMoCov2 (He et al., 2019) employs a contrastive learning paradigm, which can be regarded as a form of instance classification. In particular, the backbone is trained to yield similar representations for different augmentations of the same image, whilst ensuring that representations of different images (\u2018negatives\u2019) are dissimilar.\n\nBYOL (Grill et al., 2020) constructs self-supervised learning as a form of Mean Teacher self-distillation (Tarvainen & Valpola, 2017) with no labels. Unlike MoCov2, it does not rely on negative pairs for training. The mean squared error between the normalized predictions (student) and target projections (teacher) is minimized during training.\n\nDINO (Caron et al., 2021) uses a similar setup as BYOL, i.e., a self-distillation approach. Instead of employing an MSE loss, DINO optimizes for a low cross-entropy loss between the output representations of the teacher and the student models.\n\n# (3) Vision-Language Learning.\n\nCLIP (Radford et al., 2021) employs contrastive learning on a large-scale noisy dataset comprising of image-text pairs. It comprises of two encoders, an image encoder (ResNet (He et al., 2016) or ViT (Kolesnikov et al., 2021)) and a text encoder (Transformer (Vaswani et al., 2017)), and is optimized to align the embedding spaces of the two encoders.\n\nThus, to summarize, we evaluate across a broad spectrum of pre-training mechanisms: a contrastive, two self-distillation-based, and a multi-modal pre-training paradigm, which cover some of the most popular approaches to self-supervised learning. This is contrasted with fully-supervised trained models.\n\n# F.4 TRAINING DETAILS\n\nHere we mention the training details for the pre-training (self-supervised and fully-supervised), as well as probing experiments.\n\n# Self-supervised Pre-training.\n\nWe pretrain all models on the ImageNet dataset (Russakovsky et al., 2015). For each self-supervised pre-training framework, we follow the standard recipes as mentioned in their respective works1. To keep the configuration consistent we use a batch size of 256 for all models, distributed over 4 GPUs and train for 200 epochs. The learning rate for each SSL framework is updated following the linear scaling rule (Goyal et al., 2017): lr = 0.0005 \u00d7 batchsize/256.\n\n# Supervised Training\n\nWhen training fully supervised models we train the model for 100 epochs. For probing the pre-trained SSL features, on ImageNet we train the probes for 100 epochs as is standard (Caron et al., 2021; Grill et al., 2020) and for 50 epochs when training on COCO and VOC datasets. Random resize crops2 and horizontal flips augmentations are applied during training, and we report accuracy on a central crop.", "mimetype": "text/plain", "start_char_idx": 5522, "end_char_idx": 9810, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c993343f-0b07-4e8f-bc46-1c1d8066c7c1": {"__data__": {"id_": "c993343f-0b07-4e8f-bc46-1c1d8066c7c1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37589ad1-7249-4c95-919f-c59b67b41ace", "node_type": "4", "metadata": {}, "hash": "0012c781a1e0ca3bd3b19552bc3a6d6bca56431b3bdc9eb8037a669082f0493d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "895c1b36-0277-45e4-a7d4-8d16e9d06503", "node_type": "1", "metadata": {}, "hash": "2a458f3e1cab848ee12251c2079ee3afc3551ea316583b826db4f1f8f8798731", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbce4d86-a647-4d02-89c4-5bd4b2727b95", "node_type": "1", "metadata": {}, "hash": "350445f0b48b7d38e2da3361358da450ce7587f61aaf44dc59419f538d06834a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each self-supervised pre-training framework, we follow the standard recipes as mentioned in their respective works1. To keep the configuration consistent we use a batch size of 256 for all models, distributed over 4 GPUs and train for 200 epochs. The learning rate for each SSL framework is updated following the linear scaling rule (Goyal et al., 2017): lr = 0.0005 \u00d7 batchsize/256.\n\n# Supervised Training\n\nWhen training fully supervised models we train the model for 100 epochs. For probing the pre-trained SSL features, on ImageNet we train the probes for 100 epochs as is standard (Caron et al., 2021; Grill et al., 2020) and for 50 epochs when training on COCO and VOC datasets. Random resize crops2 and horizontal flips augmentations are applied during training, and we report accuracy on a central crop. Note that we do not perform any hyperparameter tuning separately for every dataset, and use the same training procedure for all settings (except the number of epochs for which the probe is trained).\n\n# Probing on Pre-trained SSL Features\n\nWhen probing on the pre-trained SSL (and CLIP) features, as is done for B-cos (B\u00f6hle et al., 2022) models we first apply the classifier as a 1\u00d71 convolution to the feature volume and then apply Global Average Pooling (GAP) to get the class-wise logits. To keep it consistent across all models, we apply this scheme to both B-cos as well as conventional models. Also it is important to note that when using conventional probes with B-cos models, we remove all biases.\n\nNote. For fully supervised models and probing the pre-trained SSL features we use the following training configuration: the Adam (Kingma & Ba, 2014) optimizer, a batch size of 256, and a cosine learning rate schedule with warmup. Weight decay of 0.0001 is applied only for end-to-end training.\n\n1 For B-cos models with DINO, Adam optimizer and a learning rate of 0.001 was used\n\n2 For VOC and COCO when applying random resized cropping, the crop size is limited within (0.7, 1.0) fraction of the original image size.\nUnder review as a conference paper at ICLR 2025\n\nof standard models (i.e. non B-cos models). Random resize crops and horizontal flips augmentations are applied during training.\n\nFinally, for the CLIP vision-language model we use the checkpoint for the conventional model provided by the authors\u2019 original implementation (Radford et al., 2021).\n\n# F.5 EVALUATION DETAILS\n\n# Model Performance\n\nTo evaluate the model performance on single-label datasets, i.e. ImageNet we report the top-1 accuracy: which is the proportion of correctly classified samples in the validation set. For multi-label datasets, i.e. COCO and VOC we report the f1-score as is typically done (Rao et al., 2023b).\n\n# Grid Pointing Game\n\nAs described in Sec. 3.3 of the main paper, we use the grid pointing game (GridPG) (B\u00f6hle et al., 2021) to evaluate the model explanations on single-label image classification datasets (ImageNet). Originally, GridPG was used to quantitatively evaluate explanation methods, instead in our work we use it to compare between differently trained models. Fig. 2 in the main paper shows an illustration of our evaluation setup for a 2\u00d72 grid image. Similar to the original setting (B\u00f6hle et al., 2021; 2022), we construct 500, 3\u00d73 image grids from the most confidently and correctly classified images for each model independently and report the mean GridPG score. In every grid all images belong to distinct classes, and for each of the corresponding class logits we measure the fraction of positive attribution an explanation method assigns to the correct location in the grid. To put it mathematically the GridPG localization score Li for the ith cell in the grid is defined as:\n\nLi =\nP\np\u2208celli A+(p)\n/ Pn2 P\nj=1 p\u2208cellj A+(p)\n\nwhere A+(p) denotes the amount of positive attributions given to the pth pixel in the n \u00d7 n grid image. Also note that in the case that an image has no positive attributions (all attributions are negative), we ignore that sample from the evaluation set as Li is undefined in such cases since the denominator is 0.\n\n# Energy Pointing Game\n\nFor both single-label and multi-label classification datasets we use the energy pointing game (EPG) to evaluate model explanations.", "mimetype": "text/plain", "start_char_idx": 8996, "end_char_idx": 13236, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbce4d86-a647-4d02-89c4-5bd4b2727b95": {"__data__": {"id_": "dbce4d86-a647-4d02-89c4-5bd4b2727b95", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37589ad1-7249-4c95-919f-c59b67b41ace", "node_type": "4", "metadata": {}, "hash": "0012c781a1e0ca3bd3b19552bc3a6d6bca56431b3bdc9eb8037a669082f0493d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c993343f-0b07-4e8f-bc46-1c1d8066c7c1", "node_type": "1", "metadata": {}, "hash": "269037bd1738e03c66430608102b6e8657110cbf2ed136cc3c721009c60311e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In every grid all images belong to distinct classes, and for each of the corresponding class logits we measure the fraction of positive attribution an explanation method assigns to the correct location in the grid. To put it mathematically the GridPG localization score Li for the ith cell in the grid is defined as:\n\nLi =\nP\np\u2208celli A+(p)\n/ Pn2 P\nj=1 p\u2208cellj A+(p)\n\nwhere A+(p) denotes the amount of positive attributions given to the pth pixel in the n \u00d7 n grid image. Also note that in the case that an image has no positive attributions (all attributions are negative), we ignore that sample from the evaluation set as Li is undefined in such cases since the denominator is 0.\n\n# Energy Pointing Game\n\nFor both single-label and multi-label classification datasets we use the energy pointing game (EPG) to evaluate model explanations. Introduced in (Wang et al., 2020) the EPG measures the concentration of positive attributions inside the bounding box for each object class k in the image. Finally we report the mean EPG score on the entire validation (or test) set. We follow the formulation as in (Rao et al., 2023b) for a given image with height, width as H\u00d7W and kth class the EPGk score is given by:\n\nEPGk =\nPH\nw=1 Mk,hwA+\n/ PH PW\nh=1 w=1 A+ k,hw\n\nwhere Ak \u2208 RH\u00d7W denotes the attribution map for class k, and Ak+ denotes the positive part of the attributions; Mk \u2208 {0, 1}H\u00d7W denotes the binary mask for class k that is computed by taking the union of bounding boxes for all occurrences of class k in the given image. This evaluation setting might be used for any dataset that has available annotations in the form of bounding boxes or segmentation masks for the object classes of interest.\n\n# Pixel Deletion\n\nThis evaluation procedure has been used in prior works (Samek et al., 2017; Hedstr\u00f6m et al., 2024) as a reliable metric to evaluate the faithfulness of explanations derived from a given attribution method. In this setting, the least important pixels (or tokens) as given by an explanation are incrementally removed from the input. This should not affect the model\u2019s prediction and result in a slow decline of the model\u2019s prediction confidence.\n\n# Compactness\n\nIn their work work Chalasani et al. (2018) used the Gini index p.p. as a measure of quality of explanations generated for neural networks during adversarial training. The higher the compactness, the better is the explanation quality.\n# Under review as a conference paper at ICLR 2025\n\nComplexity The authors in (Tseng et al., 2020) used entropy as a measure of quality for explanations generated by attribution methods of deep learning models for genomics. Lower entropy of explanations is considered to indicate higher quality.\n\n# Attribution Implementation\n\nThe following attribution (or explanation) methods are used in our work: B-cos (B\u00f6hle et al., 2022), Layer-wise Relevance Propagation (LRP) (Bach et al., 2015), GradCAM (Selvaraju et al., 2017), Integrated Gradients (IntGrad) (Sundararajan et al., 2017), Input\u00d7Gradient (I\u00d7G) (Shrikumar et al., 2017), LIME (Ribeiro et al., 2016) and GuidedBackpropagation (GBP) (Springenberg et al., 2015).", "mimetype": "text/plain", "start_char_idx": 12400, "end_char_idx": 15527, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6cbbc5e1-0a41-4aef-908d-23a514a7e57a": {"__data__": {"id_": "6cbbc5e1-0a41-4aef-908d-23a514a7e57a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "532ba3a2-4f81-4db0-98b1-d600f70164e1", "node_type": "4", "metadata": {}, "hash": "1c2f4308a905c93454415114531d856c2d0928aa7b93c63c7c846f7ee3d0438d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For all methods except B-cos, LRP and LIME we use the implementations provided by the captum library (github.com/pytorch/captum).\n\nFor IntGrad, similar to (B\u00f6hle et al., 2022) we set n steps = 50 for integrating over the gradients and a batch size = 16 to accommodate for limited compute. For computing LRP attributions we rely on the zennit library (https://github.com/chr5tphr/zennit) and use the epsilon gamma box composite. For LIME attributions we use the official implementation available at https://github.com/marcotcr/lime. And for B-cos attributions we use the author provided implementation given at https://github.com/B-cos/B-cos-v2/.\n\nWe also evaluate on explanation methods developed specifically for Vision Transformers (Kolesnikov et al., 2021). In particular we use CGW1 (Chefer et al., 2020), Rollout (Abnar & Zuidema, 2020) and ViT-CX (CausalX, (Xie et al., 2022)). For CGW1 and Rollout we use the author provided implementation provided at https://github.com/hila-chefer/Transformer-Explainability. And for ViT-CX we use the official implementation available at https://github.com/vaynexie/CausalX-ViT.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1121, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d3061cab-32b1-4119-b4a7-ded32c90aaa6": {"__data__": {"id_": "d3061cab-32b1-4119-b4a7-ded32c90aaa6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The text discusses the impact of training details on the effectiveness of post-hoc explanation methods for Deep Neural Networks (DNNs), particularly focusing on the classification layer of pre-trained models. It highlights the importance of understanding how the training of the classification layer influences the quality of model explanations, even when the model backbone remains frozen. The text explores how using different loss functions (Binary Cross-Entropy vs. Cross-Entropy) for training linear probes can significantly affect the class-specificity and localization of explanations. It also delves into the benefits of using non-linear probes, such as multi-layer perceptrons, to extract class-specific features more effectively.\n\nSome questions that this text can answer include:\n- How does the training of the classification layer impact the quality of model explanations in DNNs?\n- What is the significance of using different loss functions for training linear probes in post-hoc explanation methods?\n- How do non-linear probes, like multi-layer perceptrons, enhance the localization ability of attribution methods in pre-trained models?\n- Why is it important to consider the interplay between model explanations and training details when using post-hoc attribution methods for DNNs?\n- What are some common interpretability metrics used to evaluate the quality of model explanations in the context of pre-trained backbones and probes?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d0bb36f-55bb-407c-8dcf-f3d27f01373c": {"__data__": {"id_": "8d0bb36f-55bb-407c-8dcf-f3d27f01373c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9434786b-aa69-40b3-b9cc-3ecf847eb7a1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The text discusses the impact of different optimization objectives on linear probes used for model explanations in the context of image classification. It compares the commonly used Cross Entropy (CE) loss with the Binary Cross Entropy (BCE) loss and evaluates their effects on the accuracy and quality of explanations provided by various attribution methods. The text explores how using BCE loss leads to better localization of class-specific features in the explanations, resulting in more interpretable and accurate model explanations. It also highlights the importance of probe optimization in enhancing the GridPG scores and the overall quality of explanations.\n\nSome questions that this text can answer include:\n- What are the differences between Cross Entropy (CE) loss and Binary Cross Entropy (BCE) loss in the context of linear probes for model explanations?\n- How does the choice of optimization objective impact the accuracy and localization ability of model explanations?\n- What are the implications of using BCE loss over CE loss on the interpretability and quality of explanations provided by various attribution methods?\n- How do different pre-training paradigms and explanation methods interact with the optimization objectives of linear probes in image classification tasks?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d5d4f5f7-3cad-4af9-8f3e-63a6369f48cc": {"__data__": {"id_": "d5d4f5f7-3cad-4af9-8f3e-63a6369f48cc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b038a19c-c71f-4121-bd5a-82e96b947813", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The text discusses the impact of different training methods on the quality of explanations derived from attribution methods for pre-trained models. It highlights that the quality of explanations is more dependent on how the classification layer is trained for a given downstream task rather than the choice of pre-training paradigm. The text also emphasizes the importance of training lightweight multi-layer B-cos probes on frozen features of SSL-trained models with BCE loss to obtain interpretable and high-performing classifiers on downstream tasks.\n\nSome questions that this text can answer include:\n- How does the training of the classification layer impact the quality of explanations derived from attribution methods?\n- What is the significance of using lightweight multi-layer B-cos probes on frozen features of SSL-trained models with BCE loss?\n- How do different pre-training paradigms affect the quality of explanations for popular attribution methods?\n- What are the implications of the findings on the interpretability of attribution methods for deep learning models?\n- How can the training details of underlying models affect the interpretability of model explanations?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a800d462-98fe-41c7-b4dd-51b151000173": {"__data__": {"id_": "a800d462-98fe-41c7-b4dd-51b151000173", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a885c675-2205-4e32-af60-be54d05f8a5c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The provided text consists of references to various research papers and conference proceedings related to topics such as deep learning, neural networks, image segmentation, model interpretability, self-supervised learning, and explainable AI. It covers a wide range of subjects within the field of artificial intelligence and machine learning, including methods for visual representation learning, contrastive learning, image saliency, model explanations, and the evaluation of deep neural networks.\n\nSome questions that this text can potentially answer include:\n- What are some recent advancements in self-supervised learning for visual representations?\n- How can deep neural networks be interpreted and explained for better understanding?\n- What methods are used for image segmentation and saliency detection in deep learning models?\n- How can model explanations be evaluated for fairness and quality?\n- What are some techniques for improving the interpretability and robustness of deep learning models?\n- How do different attribution methods work and what are their implications for model interpretability?\n- What are the challenges and considerations in training large-scale deep learning models efficiently?\n- How can visual explanations be generated from deep neural networks to understand their decision-making processes?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b0fcac38-35b2-469f-a995-3af2c379b04b": {"__data__": {"id_": "b0fcac38-35b2-469f-a995-3af2c379b04b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99b6828c-1bde-47f2-8d06-18aa053bc100", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The provided text discusses various aspects related to explainable artificial intelligence (XAI) and evaluation metrics for explanation methods. It delves into the importance of interpretability metrics, such as correctness, completeness, consistency, and other Co-12 properties, in assessing the quality of explanations generated by attribution methods. The text also explores the impact of training objectives, like BCE vs. CE probing, on the localization and quality of explanations for different models, including Vision Transformers (ViTs) and convolutional neural networks (CNNs).\n\nSome questions that this text can answer include:\n- What are the Co-12 properties proposed by Nauta et al. (2023) for evaluating explanation methods?\n- How does linear probing on frozen backbone features contribute to the interpretability of pre-trained models?\n- What is the significance of using BCE vs. CE probing in improving the localization of explanations for ViTs and CNNs?\n- How do metrics like GridPG and EPG help in evaluating the quality of explanations generated by attribution methods?\n- What are some key findings regarding the impact of training objectives on the explanation quality for different models and pre-training schemes?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e034a6e-3446-45ed-b977-45153ee631c5": {"__data__": {"id_": "2e034a6e-3446-45ed-b977-45153ee631c5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d8bdb5b-9c77-46f5-af13-5aa0e0147749", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The text discusses the impact of different probing strategies and methods on the accuracy and localization scores of models trained with various pre-training techniques. It compares the performance of probes trained with binary cross entropy (BCE) and cross entropy (CE) losses, as well as the use of more complex probes like B-cos MLPs. The text also highlights the improvements in localization ability when using BCE-trained probes over CE-trained probes, especially for smaller bounding boxes. Additionally, it mentions the consistent improvement in attribution localization for models trained with BCE loss across different SSL methods and supervised pre-training.\n\nSome questions that this text can answer include:\n1. How does the choice of probing strategy affect the accuracy and localization scores of models?\n2. What are the differences in performance between probes trained with BCE and CE losses?\n3. How do more complex probes like B-cos MLPs impact the localization ability of models?\n4. What are the benefits of using BCE-trained probes over CE-trained probes, particularly for smaller bounding boxes?\n5. How do the results for transformer-based architectures compare to those of convolutional backbones in terms of localization scores?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce9dd654-ca74-4398-b86f-b4b11368a82a": {"__data__": {"id_": "ce9dd654-ca74-4398-b86f-b4b11368a82a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3fabe9-d151-4818-9fc2-cd3511fbf510", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The provided text discusses the comparison of different explanation methods for SSL (Self-Supervised Learning) models such as DINO, BYOL, and MoCov2 with supervised training. It highlights the effectiveness of B-cos explanations in localizing objects of interest compared to conventional explanations like GradCAM, IntGrad, and LRP. The text also explores the impact of probe complexity, such as using MLP probes, on improving localization scores and downstream task performance.\n\nSome questions that this text can answer include:\n- How do B-cos explanations compare to conventional explanations in highlighting objects of interest for SSL models?\n- What is the impact of using different probe complexities, like MLP probes, on localization scores and performance in downstream tasks for SSL models?\n- How do SSL models like DINO, BYOL, and MoCov2 perform in terms of object localization compared to supervised models when using various explanation methods?\n- What are the differences in explanation quality between SSL models and supervised models when using different attribution methods like GradCAM, IntGrad, and LRP?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a75e04ee-de57-4192-8e52-39e206664801": {"__data__": {"id_": "a75e04ee-de57-4192-8e52-39e206664801", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "75f91c95-e67a-4a4f-836f-fe6f8a4e8f61", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The text provides detailed information about a research study focusing on the impact of different optimization objectives on probing in terms of accuracy and explanation quality using various metrics on ImageNet. It compares the performance of different SSL methods like DINO, BYOL, and MOCO when explained using different explanation methods such as GradCAM, Integrated Gradients, Input x Gradient, LIME, Guided Backpropagation, and Layer-wise Relevance Propagation. The study evaluates the impact of BCE versus CE probes and analyzes the effect of probe complexity on explanation localization. It also presents results on multi-label classification settings on COCO and VOC datasets.\n\nSome questions that this text can answer include:\n- How do different SSL methods perform when explained using various explanation methods on ImageNet?\n- What is the impact of optimization objectives like BCE versus CE on probing accuracy and explanation quality?\n- How does the probe complexity affect explanation localization in the study?\n- Which SSL method shows consistent improvement in localization score for BCE trained probes across different explanation methods?\n- What are the findings regarding the performance of different attribution methods and pre-trained backbones in terms of stability and explanation quality?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed47203f-b3de-4937-98e2-93d3db63322c": {"__data__": {"id_": "ed47203f-b3de-4937-98e2-93d3db63322c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5b5d091-a362-4b06-9ab4-a6257983cb23", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The provided text discusses the evaluation of different explainability methods for deep learning models trained on ImageNet datasets. It compares the performance of various attribution methods such as Layerwise Relevance Propagation, Integrated Gradients, Input x Gradient, GradCAM, and others for models pre-trained with different methods like MoCov2, BYOL, and DINO. The evaluation includes metrics like EPG scores, pixel deletion scores, compactness, and complexity. The text also mentions the impact of using BCE vs. CE loss functions on the performance of the attribution methods.\n\nSome questions that this text can answer include:\n- How do different explainability methods perform on models pre-trained with MoCov2, BYOL, and DINO?\n- What is the effect of using BCE vs. CE loss functions on the explainability of deep learning models?\n- How do the attribution methods compare in terms of EPG scores and pixel deletion scores for conventional and B-cos models on ImageNet?\n- What insights can be gained about the compactness and complexity of models probed with BCE vs. CE loss functions?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d80958f3-49b3-4626-bc3d-b0ad54aa697a": {"__data__": {"id_": "d80958f3-49b3-4626-bc3d-b0ad54aa697a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fa68b9a-1700-4b2f-9ff7-f97b0b162055", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The provided text discusses the evaluation of different probing methods and techniques used in supervised models, particularly focusing on the comparison between BCE and CE loss functions, as well as the impact of using complex MLP probes on downstream tasks. It also highlights the effectiveness of perturbation-based explanations like Score-CAM and the benefits of using B-cos probes for improved localization in pre-trained models.\n\nSome questions that this text can answer include:\n- What is the impact of using BCE versus CE loss functions on compactness and complexity metrics in supervised models?\n- How does probing fully-supervised backbones with linear probes affect comparability with self-supervised and vision-language approaches?\n- What are the results of using BCE probes over CE probes in Score-CAM for improved localization ability?\n- How does the use of complex MLP probes, specifically B-cos and conventional MLPs, affect performance and explanation localization ability in downstream tasks?\n- What are the findings regarding the effectiveness of B-cos probes in improving accuracy and GridPG localization scores compared to conventional MLP probes in supervised models?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f11f2f7-e4ff-46d4-a696-7ab96391c3f5": {"__data__": {"id_": "1f11f2f7-e4ff-46d4-a696-7ab96391c3f5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b571e5f2-5732-41c9-ab76-1583a8b95ea5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The provided text discusses a comparison of different models such as MoCov2, BYOL, and DINO using various techniques like Integrated Gradients, Input x Gradient, and GradCAM. The text presents a detailed analysis of these models and techniques in the context of ImageNet datasets. It can answer questions related to the performance of different models, the effectiveness of various interpretability techniques, and the comparison of results obtained using different methods on ImageNet data. Additionally, the text provides insights into the relevance and importance of each model and technique in the field of machine learning and image analysis.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "368f84f3-f914-42e7-a10c-f82df1b89ccc": {"__data__": {"id_": "368f84f3-f914-42e7-a10c-f82df1b89ccc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "773594f0-5d4a-49ab-9516-d152f7991587", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The provided text discusses the evaluation of SSL (Semi-Supervised Learning) models with B-cos and conventional backbones using various explanation methods under the multi-label classification setting. It explores the impact of using complex MLP probes on model performance, specifically focusing on f1-score and EPG (Explanation Performance Gain) score on COCO and VOC datasets. The text highlights that stronger MLP probes lead to improvements in both classification performance and explanation localization scores for most SSL models and explanation methods, except for GradCAM which shows inconsistent behavior in terms of localization. The text also mentions the limitations of GradCAM in terms of localization at earlier layers. \n\nSome questions that this text can answer include:\n- How are SSL models with B-cos and conventional backbones evaluated in the context of multi-label classification?\n- What is the impact of using complex MLP probes on model performance in terms of f1-score and EPG score?\n- Which explanation methods were used to evaluate the SSL models, and what were the observed trends in their performance?\n- What are the limitations of GradCAM in terms of localization, especially in comparison to other explanation methods?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87807c64-df88-46c8-886e-943e227afaea": {"__data__": {"id_": "87807c64-df88-46c8-886e-943e227afaea", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37589ad1-7249-4c95-919f-c59b67b41ace", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The provided text discusses a research paper under review at ICLR 2025 that focuses on evaluating different explanation methods for deep learning models across various pre-training frameworks and datasets. The paper explores the performance of these explanation methods on single-label and multi-label image classification datasets such as ImageNet, VOC 2007, and MS COCO 2014. It also delves into the evaluation metrics used, including the Grid Pointing Game (GridPG) and Energy Pointing Game (EPG) to assess model explanations.\n\nSome questions that this text can answer include:\n- What datasets are used for the experiments in the research paper?\n- What pre-training frameworks are evaluated in the study?\n- What evaluation metrics are employed to assess the model explanations?\n- How are the different explanation methods compared in terms of performance and consistency?\n- What are some of the key findings regarding the behavior of GradCAM in the context of the study?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0d7d5364-ed68-4b6d-9101-285a47c92db9": {"__data__": {"id_": "0d7d5364-ed68-4b6d-9101-285a47c92db9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "532ba3a2-4f81-4db0-98b1-d600f70164e1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The provided text discusses the various methods and libraries used for generating attributions and explanations in the context of model interpretability and explainability. It mentions the specific implementations utilized for methods like IntGrad, LRP, LIME, and B-cos, as well as explanation methods tailored for Vision Transformers such as CGW1, Rollout, and ViT-CX. The text provides links to the respective libraries and implementations for each method mentioned.\n\nSome questions that this text can answer include:\n- What libraries are used for generating attributions in the captum library?\n- How many steps are set for integrating over gradients in the IntGrad method?\n- Where can the official implementation for LIME attributions be found?\n- Which implementations are used for CGW1, Rollout, and ViT-CX explanation methods?\n- What batch size is used for computing LRP attributions?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"e9c876e7-f58f-4ebc-9ad1-5b9e8022d6c8": {"node_ids": ["509156ca-8903-422c-8d55-04d31f7a28c3", "39e8e174-2c07-43d1-840e-046161b01ea9", "2e3d8929-0c52-4cdc-9a13-6657d08243b2", "670cb8a2-96b6-44f3-a01b-2be22d814996", "2c90203a-1040-443c-b8eb-dcee414e230d", "d3061cab-32b1-4119-b4a7-ded32c90aaa6"], "metadata": {}}, "9434786b-aa69-40b3-b9cc-3ecf847eb7a1": {"node_ids": ["9f04337a-d2b9-4eb9-b530-25e4c7aecdbd", "f63766c7-afd7-45b9-a0b4-554f2cb3ec2b", "8407880d-4a1d-49b4-b361-f96b2da6c972", "6d2c42d6-8d07-4514-8e33-6741baffeeba", "5a6d0d3a-bc83-4e90-bf9b-4dc8b6ec4a5d", "8d0bb36f-55bb-407c-8dcf-f3d27f01373c"], "metadata": {}}, "b038a19c-c71f-4121-bd5a-82e96b947813": {"node_ids": ["69dc2a78-d7e0-4d04-af6f-70a28d6fca29", "c7e4035d-e71d-444b-bacf-08a674fad8fd", "bb5b8920-a3bf-4919-a56b-50da41b12714", "bb8ad3af-5a02-45d2-b4a8-949d3c9842a6", "25103751-f41d-4b4f-8652-8b0c942c054c", "d5d4f5f7-3cad-4af9-8f3e-63a6369f48cc"], "metadata": {}}, "a885c675-2205-4e32-af60-be54d05f8a5c": {"node_ids": ["921588a1-14f3-4360-97d8-c0e69e040e90", "93fd2242-059f-4a53-9399-7bbc25608b6c", "dcea69f3-ff89-4127-a2e8-f21e4e12b472", "be25a61e-f0ae-4f95-bfa8-a6e35c77247d", "253fdafe-73fa-44f7-b65b-68ed46fce9ab", "a800d462-98fe-41c7-b4dd-51b151000173"], "metadata": {}}, "99b6828c-1bde-47f2-8d06-18aa053bc100": {"node_ids": ["0e03c852-bbb7-4f8c-a6b6-f982e30aefa3", "a18f3843-a381-48ce-8c5a-1fb2d3577082", "11958e16-6d14-4071-a88b-8ce34e958501", "680b3d2c-9116-44c0-9ae9-d61c58297611", "b0fcac38-35b2-469f-a995-3af2c379b04b"], "metadata": {}}, "2d8bdb5b-9c77-46f5-af13-5aa0e0147749": {"node_ids": ["35df42b8-120a-4ff1-a1af-a8d31cb51229", "33538484-e919-4af5-927e-ff392b3cbc46", "e7a78683-ba39-495a-ad59-c6954f9200f1", "035ffeab-be1d-4d46-8d91-e280da11eec4", "34e2e113-2421-4f1c-ae2a-cf5cd5773d08", "2e034a6e-3446-45ed-b977-45153ee631c5"], "metadata": {}}, "6a3fabe9-d151-4818-9fc2-cd3511fbf510": {"node_ids": ["9b488bbe-e17e-4cb6-b6ad-fa33b2c20916", "c8952b2b-4672-4cdb-931d-ab5c4bce6da3", "599dddd6-22be-497e-b7e4-dc5cccb4ab9a", "7b3bbd18-3411-4a78-b092-bd5ec3a6b762", "002f2372-cf27-49ca-8e77-3a02d34c7da0", "e5ef7850-936f-4cd0-af9a-b59bc03aeb4e", "ce9dd654-ca74-4398-b86f-b4b11368a82a"], "metadata": {}}, "75f91c95-e67a-4a4f-836f-fe6f8a4e8f61": {"node_ids": ["83be867f-1739-4404-949a-62f15c183419", "8aff54e3-1eb4-438f-be7a-f35813354d28", "e692e3aa-bbc4-4c68-9f46-7e77c8320470", "8524edff-44a4-470f-be8a-0dab06c57d5d", "a75e04ee-de57-4192-8e52-39e206664801"], "metadata": {}}, "e5b5d091-a362-4b06-9ab4-a6257983cb23": {"node_ids": ["b0629401-432c-44a1-8bdb-51a55cef775e", "728752c2-7f9b-41e7-87a2-55523d10065c", "aafde6a4-5e10-47a0-a722-16f71fcef598", "36c66177-7ef8-48ac-a4c6-8bd2a1699c3f", "8f37fb67-79e6-45dc-b4b1-fd29556f2d38", "ed47203f-b3de-4937-98e2-93d3db63322c"], "metadata": {}}, "3fa68b9a-1700-4b2f-9ff7-f97b0b162055": {"node_ids": ["c71ddbaa-324e-447f-809e-b283a23f2f81", "ecf7a71b-9b4b-42ee-a83b-043585fedfdd", "366953ba-e3e6-4f02-8e23-356f687fa945", "6dbcc888-9064-42f3-89c5-11485572f074", "d80958f3-49b3-4626-bc3d-b0ad54aa697a"], "metadata": {}}, "b571e5f2-5732-41c9-ab76-1583a8b95ea5": {"node_ids": ["7b5dc589-333e-4822-8d64-e44356622efe", "3a3890db-eb00-484c-81cb-00c4753ccd73", "7472a06c-aa43-4420-8521-8e601d3ab4fc", "93199c91-5086-488e-91f4-f208a2030ee8", "291d4b2a-f2da-4d90-b1e8-6f43d4814703", "1f11f2f7-e4ff-46d4-a696-7ab96391c3f5"], "metadata": {}}, "773594f0-5d4a-49ab-9516-d152f7991587": {"node_ids": ["d89490a8-9cce-49fe-a837-a721281cca2e", "75668d34-9b3b-49a2-a6a2-2afe99aecacd", "c6022b8c-d37a-4c3f-82e7-1d1710f6f149", "0add8097-61e2-4b6a-9211-8efa93a89164", "3716fe1d-b48d-4ac3-9b5f-3fa4cebeae3c", "bbfcbde5-0704-4928-93c6-e0f9dc9e7700", "368f84f3-f914-42e7-a10c-f82df1b89ccc"], "metadata": {}}, "37589ad1-7249-4c95-919f-c59b67b41ace": {"node_ids": ["02c28097-932c-433a-9fd4-f4ffc87a4941", "4a3de1ee-dde0-473b-b211-a95aedda4abd", "895c1b36-0277-45e4-a7d4-8d16e9d06503", "c993343f-0b07-4e8f-bc46-1c1d8066c7c1", "dbce4d86-a647-4d02-89c4-5bd4b2727b95", "87807c64-df88-46c8-886e-943e227afaea"], "metadata": {}}, "532ba3a2-4f81-4db0-98b1-d600f70164e1": {"node_ids": ["6cbbc5e1-0a41-4aef-908d-23a514a7e57a", "0d7d5364-ed68-4b6d-9101-285a47c92db9"], "metadata": {}}}}